<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li class="is-active"><a class="tocitem" href>ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a><ul class="internal"><li><a class="tocitem" href="#Tensors-and-Operators"><span>Tensors and Operators</span></a></li><li><a class="tocitem" href="#Session"><span>Session</span></a></li><li><a class="tocitem" href="#Kernel"><span>Kernel</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li><li><a class="tocitem" href="#Tensor-Operations"><span>Tensor Operations</span></a></li></ul></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/tu_basic.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ADCME-Basics:-Tensor,-Type,-Operator,-Session-and-Kernel"><a class="docs-heading-anchor" href="#ADCME-Basics:-Tensor,-Type,-Operator,-Session-and-Kernel">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a><a id="ADCME-Basics:-Tensor,-Type,-Operator,-Session-and-Kernel-1"></a><a class="docs-heading-anchor-permalink" href="#ADCME-Basics:-Tensor,-Type,-Operator,-Session-and-Kernel" title="Permalink"></a></h1><h2 id="Tensors-and-Operators"><a class="docs-heading-anchor" href="#Tensors-and-Operators">Tensors and Operators</a><a id="Tensors-and-Operators-1"></a><a class="docs-heading-anchor-permalink" href="#Tensors-and-Operators" title="Permalink"></a></h2><p><code>Tensor</code> is a data structure for storing structured data, such as a scalar, a vector, a matrix or a high dimensional tensor. The name of the ADCME backend, <code>TensorFlow</code>, is also derived from its core framework, <code>Tensor</code>. Tensors can be viewed as symbolic versions of Julia&#39;s <code>Array</code>. </p><p>A tensor is a collection of <span>$n$</span>-dimensional arrays. ADCME represents tensors using a <code>PyObject</code> handle to the TensorFlow <code>Tensor</code> data structure. A tensor has three important properties</p><ul><li><code>name</code>: Each Tensor admits a unique name. </li><li><code>shape</code>: For scalars, the shape is always an empty tuple <code>()</code>; for <span>$n$</span>-dimensional vectors, the shape is <code>(n,)</code>; for matrices or higher order tensors, the shape has the form <code>(n1, n2, ...)</code></li><li><code>dtype</code>: The type of the tensors. There is a one-to-one correspondence between most TensorFlow types and Julia types (e.g., <code>Int64</code>, <code>Int32</code>, <code>Float64</code>, <code>Float32</code>, <code>String</code>, and <code>Bool</code>). Therefore, we have overloaded the type name so users have a unified interface. </li></ul><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/tensorspec.png?raw=true" alt/></p><p>An important difference is that <code>tensor</code> object stores data in the row-major while Julia&#39;s default for <code>Array</code> is column major. The difference may affect performance if not carefully dealt with, but more often than not, the difference is not relevant if you do not convert data between Julia and Python often. Here is a representation of ADCME <code>tensor</code></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/tensors.png?raw=true" alt/></p><p>There are 4 ways to create tensors. </p><ul><li><a href="../api/#ADCME.constant-Tuple{Any}"><code>constant</code></a>. As the name suggests, <code>constant</code> creates an immutable tensor from Julia Arrays. </li></ul><pre><code class="language-julia">constant(1.0)
constant(rand(10))
constant(rand(10,10))</code></pre><ul><li><a href="../api/#ADCME.Variable-Tuple{Any}"><code>Variable</code></a>. In contrast to <code>constant</code>, <code>Variable</code> creates tensors that are mutable. The mutability allows us to update the tensor values, e.g., in an optimization procedure. It is very important to understand the difference between <code>constant</code> and <code>Variable</code>: simply put, in inverse modeling, tensors that are defined as <code>Variable</code> should be the quantity you want to invert, while <code>constant</code> is a way to provide known data.</li></ul><pre><code class="language-julia">Variable(1.0)
Variable(rand(10))
Variable(rand(10,10))</code></pre><ul><li><a href="../api/#ADCME.placeholder-Tuple{Type}"><code>placeholder</code></a>. <code>placeholder</code> is a convenient way to specify a tensor whose values are to be provided in the runtime. One use case is that you want to try out different values for this tensor and scrutinize the simulation result. </li></ul><pre><code class="language-julia">placeholder(Float64, shape=[10,10])
placeholder(rand(10)) # default value is `rand(10)`</code></pre><ul><li><a href="../api/#ADCME.SparseTensor"><code>SparseTensor</code></a>. <code>SparseTensor</code> is a special data structure to store a sparse matrix. Although it is not very emphasized in machine learning, sparse linear algebra is one of the cores to scientific computing. Thus possessing a strong sparse linear algebra support is the key to success inverse modeling with physics based machine learning. </li></ul><pre><code class="language-julia">using SparseArrays
SparseTensor(sprand(10,10,0.3))
SparseTensor([1,2,3],[2,2,2],[0.1,0.3,0.5],3,3) # specify row, col, value, number of rows, number of columns</code></pre><p>Now we know how to create tensors, the next step is to perform mathematical operations on those tensors.</p><p><code>Operator</code> can be viewed as a function that takes multiple tensors and outputs multiple tensors. In the computational graph, operators are represented by nodes while tensors are represented by edges. Most mathematical operators, such as <code>+</code>, <code>-</code>, <code>*</code> and <code>/</code>, and matrix operators, such as matrix-matrix multiplication, indexing and linear system solve, also work on tensors. </p><pre><code class="language-julia">a = constant(rand(10,10))
b = constant(rand(10))
a + 1.0 # add 1 to every entry in `a`
a * b # matrix vector production
a * a # matrix matrix production
a .* a # element wise production
inv(a) # matrix inversion</code></pre><h2 id="Session"><a class="docs-heading-anchor" href="#Session">Session</a><a id="Session-1"></a><a class="docs-heading-anchor-permalink" href="#Session" title="Permalink"></a></h2><p>With the aforementioned syntax to create and transform tensors, we have created a computational graph. However, at this point, all the operations are symbolic, i.e., the operators have not been executed yet. </p><p>To trigger the actual computing, the TensorFlow mechanism is to create a session, which drives the graph based optimization (like detecting dependencies) and executes all the operations.  </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/session.gif?raw=true" alt/></p><pre><code class="language-julia">a = constant(rand(10,10))
b = constant(rand(10))
c = a * b
sess = Session()
run(sess, c) # syntax for triggering the execution of the graph</code></pre><p>If your computational graph contains <code>Variables</code>, which can be listed via <a href="../api/#ADCME.get_collection"><code>get_collection</code></a>, then you must initialize your graph before any <code>run</code> command, in which the Variables are populated with initial values</p><pre><code class="language-julia">init(sess)</code></pre><h2 id="Kernel"><a class="docs-heading-anchor" href="#Kernel">Kernel</a><a id="Kernel-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel" title="Permalink"></a></h2><p>The kernels provide the low level C++ implementation for the operators. ADCME augments users with missing features in TensorFlow that are crucial for scientific computing and tailors the syntax for numerical schemes. Those kernels, depending on their implementation, can be used in CPU, GPU, TPU or heterogenious computing environments. </p><p>All the intensive computations are  done either in Julia or C++, and therefore we can achieve very high performance if the logic is done appropriately. For performance critical part, users may resort to custom kernels using <a href="../api/#ADCME.customop-Tuple{}"><code>customop</code></a>, which allows you to incooperate custom designed C++ codes. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/kernel.png?raw=true" alt/></p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>ADCME performances operations on tensors. The actual computations are pushed back to low level C++ kernels via operators. A session is need to drive the executation of the computation. It will be easier for you to analyze computational cost and optimize your codes with this computation model in mind. </p><h2 id="Tensor-Operations"><a class="docs-heading-anchor" href="#Tensor-Operations">Tensor Operations</a><a id="Tensor-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-Operations" title="Permalink"></a></h2><p>Here we show a list of commonly used operators in ADCME. </p><table><tr><th style="text-align: right">Description</th><th style="text-align: right">API</th></tr><tr><td style="text-align: right">Constant creation</td><td style="text-align: right"><code>constant(rand(10))</code></td></tr><tr><td style="text-align: right">Variable creation</td><td style="text-align: right"><code>Variable(rand(10))</code></td></tr><tr><td style="text-align: right">Get size</td><td style="text-align: right"><code>size(x)</code></td></tr><tr><td style="text-align: right">Get size of dimension</td><td style="text-align: right"><code>size(x,i)</code></td></tr><tr><td style="text-align: right">Get length</td><td style="text-align: right"><code>length(x)</code></td></tr><tr><td style="text-align: right">Resize</td><td style="text-align: right"><code>reshape(x,5,3)</code></td></tr><tr><td style="text-align: right">Vector indexing</td><td style="text-align: right"><code>v[1:3]</code>,<code>v[[1;3;4]]</code>,<code>v[3:end]</code>,<code>v[:]</code></td></tr><tr><td style="text-align: right">Matrix indexing</td><td style="text-align: right"><code>m[3,:]</code>, <code>m[:,3]</code>, <code>m[1,3]</code>,<code>m[[1;2;5],[2;3]]</code></td></tr><tr><td style="text-align: right">3D Tensor indexing</td><td style="text-align: right"><code>m[1,:,:]</code>, <code>m[[1;2;3],:,3]</code>, <code>m[1:3:end, 1, 4]</code></td></tr><tr><td style="text-align: right">Index relative to end</td><td style="text-align: right"><code>v[end]</code>, <code>m[1,end]</code></td></tr><tr><td style="text-align: right">Extract row (most efficient)</td><td style="text-align: right"><code>m[2]</code>, <code>m[2,:]</code></td></tr><tr><td style="text-align: right">Extract column</td><td style="text-align: right"><code>m[:,3]</code></td></tr><tr><td style="text-align: right">Convert to dense diagonal matrix</td><td style="text-align: right"><code>diagm(v)</code></td></tr><tr><td style="text-align: right">Convert to sparse diagonal matrix</td><td style="text-align: right"><code>spdiag(v)</code></td></tr><tr><td style="text-align: right">Extract diagonals as vector</td><td style="text-align: right"><code>diag(m)</code></td></tr><tr><td style="text-align: right">Elementwise multiplication</td><td style="text-align: right"><code>a.*b</code></td></tr><tr><td style="text-align: right">Matrix (vector) multiplication</td><td style="text-align: right"><code>a*b</code></td></tr><tr><td style="text-align: right">Matrix transpose</td><td style="text-align: right"><code>m&#39;</code></td></tr><tr><td style="text-align: right">Dot product</td><td style="text-align: right"><code>sum(a*b)</code></td></tr><tr><td style="text-align: right">Solve</td><td style="text-align: right"><code>A\b</code></td></tr><tr><td style="text-align: right">Inversion</td><td style="text-align: right"><code>inv(m)</code></td></tr><tr><td style="text-align: right">Average all elements</td><td style="text-align: right"><code>mean(x)</code></td></tr><tr><td style="text-align: right">Average along dimension</td><td style="text-align: right"><code>mean(x, dims=1)</code></td></tr><tr><td style="text-align: right">Maximum/Minimum of all elements</td><td style="text-align: right"><code>maximum(x)</code>, <code>minimum(x)</code></td></tr><tr><td style="text-align: right">Squeeze all single dimensions</td><td style="text-align: right"><code>squeeze(x)</code></td></tr><tr><td style="text-align: right">Squeeze along dimension</td><td style="text-align: right"><code>squeeze(x, dims=1)</code>, <code>squeeze(x, dims=[1;2])</code></td></tr><tr><td style="text-align: right">Reduction (along dimension)</td><td style="text-align: right"><code>norm(a)</code>, <code>sum(a, dims=1)</code></td></tr><tr><td style="text-align: right">Elementwise Multiplication</td><td style="text-align: right"><code>a.*b</code></td></tr><tr><td style="text-align: right">Elementwise Power</td><td style="text-align: right"><code>a^2</code></td></tr><tr><td style="text-align: right">SVD</td><td style="text-align: right"><code>svd(a)</code></td></tr><tr><td style="text-align: right"><code>A[indices] = updates</code></td><td style="text-align: right"><code>A = scatter_update(A, indices, updates)</code></td></tr><tr><td style="text-align: right"><code>A[indices] += updates</code></td><td style="text-align: right"><code>A = scatter_add(A, indices, updates)</code></td></tr><tr><td style="text-align: right"><code>A[indices] -= updates</code></td><td style="text-align: right"><code>A = scatter_sub(A, indices, updates)</code></td></tr><tr><td style="text-align: right"><code>A[idx, idy] = updates</code></td><td style="text-align: right"><code>A = scatter_update(A, idx, idy, updates)</code></td></tr><tr><td style="text-align: right"><code>A[idx, idy] += updates</code></td><td style="text-align: right"><code>A = scatter_add(A, idx, idy, updates)</code></td></tr><tr><td style="text-align: right"><code>A[idx, idy] -= updates</code></td><td style="text-align: right"><code>A = scatter_sub(A, idx, idy, updates)</code></td></tr></table><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>In some cases you might find some features missing in ADCME but present in TensorFlow. You can always use <code>tf.&lt;function_name&gt;</code>. It&#39;s compatible.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tu_whatis/">« What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a><a class="docs-footer-nextpage" href="../tu_optimization/">PDE Constrained Optimization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 16 January 2021 10:10">Saturday 16 January 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
