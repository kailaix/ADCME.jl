<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Julia Custom Operators · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li class="is-active"><a class="tocitem" href>Julia Custom Operators</a><ul class="internal"><li><a class="tocitem" href="#Example-1"><span>Example</span></a></li><li><a class="tocitem" href="#Embedded-in-Modules-1"><span>Embedded in Modules</span></a></li><li><a class="tocitem" href="#Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1"><span>Quick Reference for Implementing C++ Custom Operators in ADCME</span></a></li></ul></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Julia Custom Operators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Julia Custom Operators</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/julia_customop.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Julia-Custom-Operators-1"><a class="docs-heading-anchor" href="#Julia-Custom-Operators-1">Julia Custom Operators</a><a class="docs-heading-anchor-permalink" href="#Julia-Custom-Operators-1" title="Permalink"></a></h1><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Currently, embedding Julia suffers from multithreading issues: calling Julia from a non-Julia thread is not supported in ADCME. When TensorFlow kernel codes are executed concurrently, it is difficult to invoke the Julia functions. See <a href="https://github.com/kailaix/ADCME.jl/issues/8">issue</a>.</p></div></div><p>In scientific and engineering applications, the operators provided by <code>TensorFlow</code> are not sufficient for high performance computing. In addition, constraining oneself to <code>TensorFlow</code> environment sacrifices the powerful scientific computing ecosystems provided by other languages such as <code>Julia</code> and <code>Python</code>. For example, one might want to code a finite volume method for a sophisticated fluid dynamics problem; it is hard to have the flexible syntax to achieve this goal, obtain performance boost from existing fast solvers such as AMG, and benefit from many other third-party packages within <code>TensorFlow</code>. This motivates us to find a way to &quot;plugin&quot; custom operators to <code>TensorFlow</code>.</p><p>We have already introduced how to incooperate <code>C++</code> custom operators.  For many researchers, they usually prototype the solvers in a high level language such as MATLAB, Julia or Python. To enjoy the parallelism and automatic differentiation feature of <code>TensorFlow</code>, they need to port them into <code>C/C++</code>. However, this is also cumbersome sometimes, espeically the original solvers depend on many packages in the high-level language. </p><p>We solve this problem by incorporating <code>Julia</code> functions directly into <code>TensorFlow</code>. That is, for any <code>Julia</code> functions, we can immediately convert it to a <code>TensorFlow</code> operator. At runtime, when this operator is executed, the corresponding <code>Julia</code> function is executed. That implies we have the <code>Julia</code> speed. Most importantly, the function is perfectly compitable with the native <code>Julia</code> environment; third-party packages, global variables, nested functions, etc. all work smoothly. Since <code>Julia</code> has the ability to call other languages in a quite elegant and simple manner, such as <code>C/C++</code>, <code>Python</code>, <code>R</code>, <code>Java</code>, this means it is possible to incoporate packages/codes from any supported languages into <code>TensorFlow</code> ecosystem. We need to point out that in <code>TensorFlow</code>, <code>tf.numpy_function</code> can be used to convert a <code>Python</code> function to a <code>TensorFlow</code> operator. However, in the runtime, the speed for this operator falls back to <code>Python</code> (or <code>numpy</code> operation for related parts). This is a drawback. </p><p>The key for implementing the mechanism is embedding <code>Julia</code> in <code>C++</code>. Still we need to create a <code>C++</code> dynamic library for <code>TensorFlow</code>. However, the library is only an interface for invoking <code>Julia</code> code. At runtime, <code>jl_get_function</code> is called to search for the related function in the main module. <code>C++</code> arrays, which include all the relavant data, are passed to this function through <code>jl_call</code>. It requires routine convertion from <code>C++</code> arrays to <code>Julia</code> array interfaces <code>jl_array_t*</code>. However, those bookkeeping tasks are programatic and possibly will be automated in the future. Afterwards,<code>Julia</code> returns the result to <code>C++</code> and thereafter the data are passed to the next operator. </p><p>There are two caveats in the implementation. The first is that due to GIL of Python, we must take care of the thread lock while interfacing with <code>Julia</code>. This was done by putting a guard around th e<code>Julia</code> interface</p><pre><code class="language-c">PyGILState_STATE py_threadstate;
py_threadstate = PyGILState_Ensure();
// code here 
PyGILState_Release(py_threadstate);</code></pre><p>The second is the memory mangement of <code>Julia</code> arrays. This was done by defining gabage collection markers explicitly</p><pre><code class="language-julia">jl_value_t **args;
JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects
args[0] = ...
args[1] = ...
# do something
JL_GC_POP();</code></pre><p>This technique is remarkable and puts together one of the best langages in scientific computing and that in machine learning. The work that can be built on <code>ADCME</code> is enormous and significantly reduce the development time. </p><h2 id="Example-1"><a class="docs-heading-anchor" href="#Example-1">Example</a><a class="docs-heading-anchor-permalink" href="#Example-1" title="Permalink"></a></h2><p>Here we present a simple example. Suppose we want to compute the Jacobian of a two layer neural network <span>$\frac{\partial y}{\partial x}$</span></p><div>\[y = W_2\tanh(W_1x+b_1)+b_2\]</div><p>where <span>$x, b_1, b_2, y\in \mathbb{R}^{10}$</span>, <span>$W_1, W_2\in \mathbb{R}^{100}$</span>. In <code>TensorFlow</code>, this can be done by computing the gradients <span>$\frac{\partial y_i}{\partial x}$</span> for each <span>$i$</span>. In <code>Julia</code>, we can use <code>ForwardDiff</code> to do it automatically. </p><pre><code class="language-julia">function twolayer(J, x, w1, w2, b1, b2)
    f = x -&gt; begin
        w1 = reshape(w1, 10, 10)
        w2 = reshape(w2, 10, 10)
        z = w2*tanh.(w1*x+b1)+b2
    end
    J[:] = ForwardDiff.jacobian(f, x)[:]
end</code></pre><p>To make a custom operator, we first generate a wrapper</p><pre><code class="language-julia">using ADCME
mkdir(&quot;TwoLayer&quot;)
cd(&quot;TwoLayer&quot;)
customop()</code></pre><p>We modify <code>custom_op.txt</code></p><pre><code class="language-none">TwoLayer
double x(?)
double w1(?)
double b1(?)
double w2(?)
double b2(?)
double y(?) -&gt; output</code></pre><p>and run </p><pre><code class="language-julia">customop()</code></pre><p>Three files are generated<code>CMakeLists.txt</code>, <code>TwoLayer.cpp</code> and <code>gradtest.jl</code>. Now create a new file <code>TwoLayer.h</code></p><pre><code class="language-c">#include &quot;julia.h&quot;
#include &quot;Python.h&quot;

void forward(double *y, const double *x, const double *w1, const double *w2, const double *b1, const double *b2, int n){
    PyGILState_STATE py_threadstate;
    py_threadstate = PyGILState_Ensure();
    jl_value_t* array_type = jl_apply_array_type((jl_value_t*)jl_float64_type, 1);
    jl_value_t **args;
    JL_GC_PUSHARGS(args, 6); // args can now hold 2 `jl_value_t*` objects
    args[0] = (jl_value_t*)jl_ptr_to_array_1d(array_type, y, n*n, 0);
    args[1] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast&lt;double*&gt;(x), n, 0);
    args[2] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast&lt;double*&gt;(w1), n*n, 0);
    args[3] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast&lt;double*&gt;(w2), n*n, 0);
    args[4] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast&lt;double*&gt;(b1), n, 0);
    args[5] = (jl_value_t*)jl_ptr_to_array_1d(array_type, const_cast&lt;double*&gt;(b2), n, 0);
    auto fun = jl_get_function(jl_main_module, &quot;twolayer&quot;);
  	if (fun==NULL) jl_errorf(&quot;Function not found in Main module.&quot;);
    else jl_call(fun, args, 6);
    JL_GC_POP();
    if (jl_exception_occurred())
        printf(&quot;%s \n&quot;, jl_typeof_str(jl_exception_occurred()));
    PyGILState_Release(py_threadstate);
}</code></pre><p>Most of the codes have been explanined except <code>jl_ptr_to_array_1d</code>. This function generates a <code>Julia</code> array wrapper from <code>C++</code> arrays. The last argument <code>0</code> indicates that <code>Julia</code> is not responsible for gabage collection. <code>TwoLayer.cpp</code> should also be modified according to <a href="https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp">https://github.com/kailaix/ADCME.jl/blob/master/examples/twolayer_jacobian/TwoLayer.cpp</a>.</p><p>Finally, we can test in <code>gradtest.jl</code> </p><pre><code class="language-julia">two_layer = load_op(&quot;build/libTwoLayer&quot;, &quot;two_layer&quot;)


w1 = rand(100)
w2 = rand(100)
b1 = rand(10)
b2 = rand(10)
x = rand(10)
J = rand(100)
twolayer(J, x, w1, w2, b1, b2)

y = two_layer(constant(x), constant(w1), constant(b1), constant(w2), constant(b2))
sess = Session(); init(sess)
J0 = run(sess, y)
@show norm(J-J0)</code></pre><h2 id="Embedded-in-Modules-1"><a class="docs-heading-anchor" href="#Embedded-in-Modules-1">Embedded in Modules</a><a class="docs-heading-anchor-permalink" href="#Embedded-in-Modules-1" title="Permalink"></a></h2><p>If the custom operator is intended to be used in a precompiled module, we can load the dynamic library at initialization</p><pre><code class="language-julia">global my_op 
function __init__()
	global my_op = load_op(&quot;$(@__DIR__)/path/to/libMyOp&quot;, &quot;my_op&quot;)
end</code></pre><p>The corresponding <code>Julia</code> function called by <code>my_op</code> must be exported in the module (such that it is in the Main module when invoked). One such example is given in <a href="https://github.com/kailaix/ADCME.jl/blob/master/examples/JuliaOpModule.jl">MyModule</a></p><h2 id="Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1"><a class="docs-heading-anchor" href="#Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1">Quick Reference for Implementing C++ Custom Operators in ADCME</a><a class="docs-heading-anchor-permalink" href="#Quick-Reference-for-Implementing-C-Custom-Operators-in-ADCME-1" title="Permalink"></a></h2><ol><li>Set output shape</li></ol><pre><code class="language-none">c-&gt;set_output(0, c-&gt;Vector(n));
c-&gt;set_output(0, c-&gt;Matrix(m, n));
c-&gt;set_output(0, c-&gt;Scalar());</code></pre><ol><li>Names</li></ol><p><code>.Input</code> and <code>.Ouput</code> : names must be in lower case, no <code>_</code>, only letters.</p><ol><li>TensorFlow Input/Output to TensorFlow Tensors</li></ol><pre><code class="language-none">grad.vec&lt;double&gt;();
grad.scalar&lt;double&gt;();
grad.matrix&lt;double&gt;();
grad.flat&lt;double&gt;();</code></pre><p>Obtain flat arrays</p><pre><code class="language-none">grad.flat&lt;double&gt;().data()</code></pre><ol><li>Scalars</li></ol><p>Allocate scalars using TensorShape()</p><ol><li>Allocate Shapes</li></ol><p>Although you can use -1 for shape reference, you must allocate exact shapes in <code>Compute</code></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../global/">« Shared Memory Across Kernels</a><a class="docs-footer-nextpage" href="../nn/">Neural Networks »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 15 April 2020 23:53">Wednesday 15 April 2020</span>. Using Julia version 1.4.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
