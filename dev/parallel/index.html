<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Parallel Computing · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../resources/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li class="is-active"><a class="tocitem" href>Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Parallel Computing</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Parallel Computing</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/parallel.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Parallel-Computing"><a class="docs-heading-anchor" href="#Parallel-Computing">Parallel Computing</a><a id="Parallel-Computing-1"></a><a class="docs-heading-anchor-permalink" href="#Parallel-Computing" title="Permalink"></a></h1><p>The ADCME backend, TensorFlow, treats each operator as the smallest computation unit. Users are allowed to manually assign each operator to a specific device (GPU, CPU, or TPU). This is usually done with the <code>@cpu device_id expr</code> or <code>@gpu device_id expr</code> syntax, where <code>device_id</code> is the index of devices you want to place all operators and variables in <code>expr</code>. For example, if we want to create a variable <code>a</code> and compute <span>$\sin(a)$</span> on <code>GPU:0</code> we can write</p><pre><code class="language-julia">@cpu 0 begin
    global a = Variable(1.0)
    global b = sin(a)
end</code></pre><p>If the <code>device_id</code> is missing, 0 is treated as default. </p><pre><code class="language-julia">@cpu begin
    global a = Variable(1.0)
    global b = sin(a)
end</code></pre><p><strong>Custom Device Placement Functions</strong></p><p>The above placement function is useful and simple for placing operators on certain GPU devices without changing original codes. However, sometimes we want to place certain operators on certain devices. This can be done by implementing a custom <code>assign_to_device</code> function. As an example, we want to place all <code>Variables</code> on CPU:0 while placing all other operators on GPU:0, the code has the following form</p><pre><code class="language-julia">PS_OPS = [&quot;Variable&quot;, &quot;VariableV2&quot;, &quot;AutoReloadVariable&quot;]
function assign_to_device(device, ps_device=&quot;/device:CPU:0&quot;)
    function _assign(op)
        node_def = pybuiltin(&quot;isinstance&quot;)(op, tf.NodeDef) ? op : op.node_def
        if node_def.op in PS_OPS
            return ps_device
        else
            return device
        end
    end

    return _assign
end</code></pre><p>Then we can write something like</p><pre><code class="language-julia">@pywith tf.device(assign_to_device(&quot;/device:GPU:0&quot;)) begin
    global a = Variable(1.0)
    global b = sin(a)
end</code></pre><p>We can check the location of <code>a</code> and <code>b</code> by inspecting their <code>device</code> attributes</p><pre><code class="language-julia-repl">julia&gt; a.device
&quot;/device:CPU:0&quot;

julia&gt; b.device
&quot;/device:GPU:0&quot;</code></pre><p><strong>Collocate Gradient Operators</strong></p><p>When we call <code>gradients</code>, TensorFlow actually creates a set of new operators, one for each operator in the forward computation. By default, those operators are placed on the default device (<code>GPU:0</code> if GPU is available; otherwise it&#39;s <code>CPU:0</code>). Sometimes we want to place the operators created by gradients on the same devices as the corresponding forward computation operators. For example, if the operator <code>b</code> (<code>sin</code>) in the last example is on <code>GPU:0</code>, we hope the corresponding gradient computation (<code>cos</code>) is also on <code>GPU:0</code>. This can be done by specifying <code>colocate</code> <sup class="footnote-reference"><a id="citeref-colocate" href="#footnote-colocate">[colocate]</a></sup> keyword arguments in <code>gradients</code>:</p><pre><code class="language-julia">@pywith tf.device(assign_to_device(&quot;/device:GPU:0&quot;)) begin
    global a = Variable(1.0)
    global b = sin(a)
end

@pywith tf.device(&quot;/CPU:0&quot;) begin
    global c = cos(b)
end

g = gradients(c, a, colocate=true)</code></pre><p>In the following figure, we show the effects of <code>colocate</code> of the above codes. The test code snippet is</p><pre><code class="language-julia">g = gradients(c, a, colocate=true)
sess = Session(); init(sess)
run_profile(sess, g+c)
save_profile(&quot;true.json&quot;)

g = gradients(c, a, colocate=false)
sess = Session(); init(sess)
run_profile(sess, g+c)
save_profile(&quot;false.json&quot;)</code></pre><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/colocate.png?raw=true" alt/></p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If you use <code>bn</code> (batch normalization) on multi-GPUs, you must be careful to update the parameters in batch normalization on CPUs. This can be done by explicitly specify </p><pre><code class="language-julia">@pywith tf.device(&quot;/cpu:0&quot;) begin
global update_ops = get_collection(tf.GraphKeys.UPDATE_OPS)
end</code></pre><p>and bind <code>update_ops</code> to an active operator (or explictly execute it in <code>run(sess,...)</code>).</p></div></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-colocate"><a class="tag is-link" href="#citeref-colocate">colocate</a>Unfortunately, in the TensorFlow APIs, &quot;collocate&quot; is spelt as &quot;colocate&quot;. </li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../newton_raphson/">« Newton Raphson</a><a class="docs-footer-nextpage" href="../optimizers/">Optimizers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 22 October 2020 05:43">Thursday 22 October 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
