<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reinforcement Learning Basics: Q-learning and SARSA · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li><li><a class="tocitem" href="../sqlite3/">Introducing ADCME Database and SQL Integration: an Efficient Approach to Simulation Data Management</a></li><li><a class="tocitem" href="../hessian/">The Mathematical Structure of DNN Hessians</a></li><li><a class="tocitem" href="../plotly/">Visualization with Plotly</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li><li class="is-active"><a class="tocitem" href>Reinforcement Learning Basics: Q-learning and SARSA</a><ul class="internal"><li><a class="tocitem" href="#Reinforcement-Learning"><span>Reinforcement Learning</span></a></li><li><a class="tocitem" href="#Q-learning-and-SARSA"><span>Q-learning and SARSA</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li></ul></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li><li><a class="tocitem" href="../windows_installation/">Install ADCME on Windows</a></li><li><a class="tocitem" href="../docker/">Install ADCME Docker Image</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Deep Learning Schemes</a></li><li class="is-active"><a href>Reinforcement Learning Basics: Q-learning and SARSA</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reinforcement Learning Basics: Q-learning and SARSA</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/reinforcement_learning.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reinforcement-Learning-Basics:-Q-learning-and-SARSA"><a class="docs-heading-anchor" href="#Reinforcement-Learning-Basics:-Q-learning-and-SARSA">Reinforcement Learning Basics: Q-learning and SARSA</a><a id="Reinforcement-Learning-Basics:-Q-learning-and-SARSA-1"></a><a class="docs-heading-anchor-permalink" href="#Reinforcement-Learning-Basics:-Q-learning-and-SARSA" title="Permalink"></a></h1><p>This note gives a short introduction to Q-learning and SARSA in reinforcement learning. </p><h2 id="Reinforcement-Learning"><a class="docs-heading-anchor" href="#Reinforcement-Learning">Reinforcement Learning</a><a id="Reinforcement-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Reinforcement-Learning" title="Permalink"></a></h2><p>Reinforcement learning aims at making optimal decisions using experiences. In reinforcement learning, an <strong>agent</strong> interacts with an <strong>environment</strong>. There are three important concepts in reinforcement learning: <strong>states</strong>, <strong>actions</strong>, and <strong>rewards</strong>. An agent in a certain state takes an action, which results in a reward from the environment and a change of states. In this note, we assume that the states and actions are discrete, and let <span>$\mathcal{S}$</span> and <span>$\mathcal{A}$</span> denotes the set of states and actions, respectively. </p><p>We first define some important concepts in reinforcement learning:</p><ul><li>Policy. A policy is a function <span>$\pi: \mathcal{S}\rightarrow \mathcal{A}$</span> that defines the agent&#39;s action at a given state. </li><li>Reward. A reward is a function <span>$R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$</span> whose value is rewarded to an agent for performing certain actions at a given state. Particularly, the goal of an agent is to maximum the discounted reward</li></ul><p class="math-container">\[\min_\pi\ R = \sum_{t=0}^\infty \gamma^t R(s_{t+1}, a_{t+1}) \tag{1}\]</p><p>where <span>$\gamma\in (0,1)$</span> is a discounted factor, and <span>$(s_t, a_t)$</span> are a sequence of states and actions following a policy <span>$\pi$</span>. </p><p>Specifically, in this note we assume that both the policy and the reward function are time-independent and deterministic.  </p><p>We can now define the quality function (Q-function) for a given policy </p><p class="math-container">\[Q^\pi(s_t, a_t) = \mathbf{E}(R(s_t, a_t) + \gamma R(s_{t+1}, a_{t+1}) + \gamma^2 R(s_{t+2}, a_{t+2})  + \ldots | s_t, a_t)\]</p><p>here <span>$\pi$</span> is a given policy</p><p>It can be shown that the solution to Equation 1 is given by the policy <span>$\pi$</span> that satisfies</p><p class="math-container">\[\pi(s) = \max_a\ Q^\pi(s, a)\]</p><h2 id="Q-learning-and-SARSA"><a class="docs-heading-anchor" href="#Q-learning-and-SARSA">Q-learning and SARSA</a><a id="Q-learning-and-SARSA-1"></a><a class="docs-heading-anchor-permalink" href="#Q-learning-and-SARSA" title="Permalink"></a></h2><p>Both Q-learning and SARSA learn the Q-function iteratively. We denote the everchanging Q-function in the iterative process as <span>$Q(s,a)$</span> (no superscript referring to any policy). When <span>$|\mathcal{S}|&lt;\infty, |\mathcal{A}|&lt;\infty$</span>, <span>$Q(s,a)$</span> can be tabulated as a <span>$|\mathcal{S}|\times |\mathcal{A}|$</span> table. </p><p>A powerful technique in reinforcment learning is the epsilon-greedy algorithm, which strikes a balance between exploration and exploitation of the state space. To describe the espilon-greedy algorithm, we introduce the <strong>stochastic policy</strong> <span>$\pi_\epsilon$</span> given a Q-function:</p><p class="math-container">\[\pi_\epsilon(s) = \begin{cases}a&#39; &amp; \text{w.p.}\ \epsilon \\ \arg\max_a Q(s, a) &amp;\text{w.p.}\ 1-\epsilon\end{cases}\]</p><p>Here <span>$a&#39;$</span> is a random variable whose values are in <span>$\mathcal{A}$</span> (e.g., a uniform random variable over <span>$\mathcal{A}$</span>). </p><p>Then the Q-learning update formula can be expressed as </p><p class="math-container">\[Q(s,a) = (1-\alpha)Q(s,a) + \alpha \left(R(s,a) + \gamma\max_{a&#39;} Q(s&#39;, a&#39;)\right)\]</p><p>The SARSA update formula can be expressed as </p><p class="math-container">\[Q(s,a) \gets (1-\alpha)Q(s,a) + \alpha \left(R(s,a) + \gamma Q
(s&#39;, a&#39;)\right),\ a&#39; = \pi_\epsilon(s&#39;)\]</p><p>In both cases, <span>$s&#39;$</span> is the subsequent state given the last action <span>$a$</span> at state <span>$s$</span>, and <span>$a = \pi_\epsilon(s)$</span>. </p><p>The subtle difference between Q-learning and SARSA is <strong>how you select your next best action</strong>, either max or mean.</p><p>To extract the optimal deterministic policy <span>$\pi$</span> from <span>$\pi_\epsilon$</span>, we only need to define</p><p class="math-container">\[\pi(s) := \arg\max_a Q(s,a)\]</p><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>We use OpenAI gym to perform numerical experiments. We reimplemented the Q-learning algorithm from <a href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/">this post</a> in Julia. </p><ul><li>Q-learning: <a href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/assets/Codes/ML/qlearning.jl">code</a></li><li>SARSA: <a href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/assets/Codes/ML/salsa.jl">code</a></li></ul><p>To run the scripts, you need to install the dependencies via </p><pre><code class="language-julia">using ADCME
PIP = get_pip()
run(`$PIP install cmake &#39;gym[atari]&#39;`)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../bnn/">« Bayesian Neural Networks</a><a class="docs-footer-nextpage" href="../designpattern/">Design Pattern »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 21 May 2021 03:06">Friday 21 May 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
