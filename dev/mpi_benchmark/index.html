<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MPI Benchmarks · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li class="is-active"><a class="tocitem" href>MPI Benchmarks</a><ul class="internal"><li><a class="tocitem" href="#Transposition"><span>Transposition</span></a></li><li><a class="tocitem" href="#Poisson&#39;s-Equation"><span>Poisson&#39;s Equation</span></a></li><li><a class="tocitem" href="#Acoustic-Seismic-Inversion"><span>Acoustic Seismic Inversion</span></a></li><li><a class="tocitem" href="#Elastic-Seismic-Inversion"><span>Elastic Seismic Inversion</span></a></li></ul></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>MPI Benchmarks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>MPI Benchmarks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/mpi_benchmark.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="MPI-Benchmarks"><a class="docs-heading-anchor" href="#MPI-Benchmarks">MPI Benchmarks</a><a id="MPI-Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Benchmarks" title="Permalink"></a></h1><p>The purpose of this section is to present the distributed computing capability of ADCME via MPI. With the MPI operators, ADCME is well suited to parallel implementations on clusters with very large numbers of cores. We benchmark individual operators as well as the gradient calculation as a whole. Particularly, we use two metrics for measuring the scaling of the implementation:</p><ol><li><strong>weak scaling</strong>, i.e., how the solution time varies with the number of processors for a fixed problem size per processor. </li><li><strong>strong scaling</strong>, i.e., the speedup for a fixed problem size with respect to the number of processors, and is governed by Amdahl&#39;s law.</li></ol><p>For most operators, ADCME is just a wrapper of existing third-party parallel computing software (e.g., Hypre). However, for gradient back-propagation, some functions may be missing and are implemented at our own discretion. For example, in Hypre, distributed sparse matrices split into multiple stripes, where each MPI rank owns a stripe with continuous row indices. In gradient back-propagation, the transpose of the original matrix is usually needed and such functionalities are missing in Hypre as of September 2020. </p><p>The MPI programs are verified with serial programs. Note that ADCME uses hybrid parallel computing models, i.e., a mixture of multithreading programs and MPI communication; therefore, one MPI processor may be allocated multiple cores. </p><h2 id="Transposition"><a class="docs-heading-anchor" href="#Transposition">Transposition</a><a id="Transposition-1"></a><a class="docs-heading-anchor-permalink" href="#Transposition" title="Permalink"></a></h2><p>Matrix transposition is an operator that are common in gradient back-propagation. For example, assume the forward computation is (<span>$x$</span> is the input, <span>$y$</span> is the output, and <span>$A$</span> is a matrix) </p><p class="math-container">\[y(\theta) = Ax(\theta) \tag{1}\]</p><p>Given a loss function <span>$L(y)$</span>, we have</p><p class="math-container">\[\frac{\partial L(y(x))}{\partial x} = \frac{\partial L(y)}{\partial y}\frac{\partial y(x)}{\partial x} = \frac{\partial L(y)}{\partial y} A\]</p><p>Note that <span>$\frac{\partial L(y)}{\partial y}$</span> is a row vector, and therefore, </p><p class="math-container">\[\left(\frac{\partial L(y(x))}{\partial x}\right)^T = A^T \left(\frac{\partial L(y)}{\partial y} \right)^T\]</p><p>requires a matrix vector multiplication, where the matrix is <span>$A^T$</span>. </p><p>Given that Equation 1 is ubiquitous in numerical PDE schemes, a distributed implementation of parallel transposition is very important. </p><p>ADCME uses the same distributed sparse matrix representation as Hypre. In Hypre, each MPI processor own a set of rows of the whole sparse matrix. The rows have continuous indices. To transpose the sparse matrix in a parallel environment, we first split the matrices in each MPI processor into blocks and then use <code>MPI_Isend</code>/<code>MPI_Irecv</code> to exchange data. Finally, we transpose the matrices in place for each block. Using this method, we obtained a CSR representation of the transposed matrix. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/mpi_transpose.png?raw=true" alt/></p><p>The following results shows the scalability of the transposition operator of <a href="../api/#ADCME.mpi_SparseTensor"><code>mpi_SparseTensor</code></a>. In the plots, we show the strong scaling for a fixed matrix size of <span>$25\text{M} \times 25\text{M}$</span> as well as the weak scaling, where each MPI processor owns <span>$300^2=90000$</span> rows. </p><table><tr><th style="text-align: right">Strong Scaling</th><th style="text-align: right">Weak Scaling</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/transpose_strong.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/transpose_weak.png?raw=true" alt/></td></tr></table><h2 id="Poisson&#39;s-Equation"><a class="docs-heading-anchor" href="#Poisson&#39;s-Equation">Poisson&#39;s Equation</a><a id="Poisson&#39;s-Equation-1"></a><a class="docs-heading-anchor-permalink" href="#Poisson&#39;s-Equation" title="Permalink"></a></h2><p>This example presents the overhead of ADCME MPI operators when the main computation is carried out through a third-party library (Hypre). We solve the Poisson&#39;s equation </p><p class="math-container">\[\nabla \cdot (\kappa(x, y) \nabla u(x,y)) = f(x, y), (x,y) \in \Omega \quad u(x,y) = 0, (x,y) \in \partial \Omega\]</p><p>Here <span>$\kappa(x, y)$</span> is approximated by a neural network <span>$\kappa_\theta(x,y) = \mathcal{NN}_\theta(x,y)$</span>, and the weights and biases <span>$\theta$</span> are broadcasted from the root processor. We express the numerical scheme as a computational graph is:</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/poisson_cg.png?raw=true" alt/> </p><p>The domain decomposition is as follows:</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/grid.png?raw=true" alt/> </p><p>The domain <span>$[0,1]^2$</span> is divided into <span>$N\times N$</span> blocks, and each block contains <span>$n\times n$</span> degrees of freedom. The domain is padded with boundary nodes, which are eliminated from the discretized equation. The grid size is </p><p class="math-container">\[h = \frac{1}{Nn+1}\]</p><p>We use a finite difference method for discretizing the Poisson&#39;s equation, which has the following form</p><p class="math-container">\[\begin{aligned}(\kappa_{i+1, j}+\kappa_{ij})u_{i+1,j} + (\kappa_{i-1, j}+\kappa_{ij})u_{i-1,j} &amp;\\ 
+ (\kappa_{i,j+1}+\kappa_{ij})u_{i,j+1} + (\kappa_{i, j-1}+\kappa_{ij})u_{i,j-1} &amp;\\ 
-(\kappa_{i+1, j}+\kappa_{i-1, j}+\kappa_{i,j+1}+\kappa_{i, j-1}+4\kappa_{ij})u_{ij} &amp;\\ 
= 2h^2f_{ij}
\end{aligned}\]</p><p>We show the strong scaling with a fixed problem size <span>$1800 \times 1800$</span> (mesh size, which implies the matrix size is around 32 million). We also show the weak scaling where each MPI processor owns a <span>$300\times 300$</span> block. For example, a problem with 3600 processors has the problem size <span>$90000\times 3600 \approx 0.3$</span> billion.</p><h3 id="Weak-Scaling"><a class="docs-heading-anchor" href="#Weak-Scaling">Weak Scaling</a><a id="Weak-Scaling-1"></a><a class="docs-heading-anchor-permalink" href="#Weak-Scaling" title="Permalink"></a></h3><p>We first consider the weak scaling. The following plots shows the runtime for forward computation as well as gradient back-propagation. </p><table><tr><th style="text-align: right">Speedup</th><th style="text-align: right">Efficiency</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/speedup_core14.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/efficiency_core14.png?raw=true" alt/></td></tr></table><p>There are several important observations:</p><ol><li>By using more cores per processor, the runtime is reduced significantly. For example, the runtime for the backward is reduced to around 10 seconds from 30 seconds by switching from 1 core to 4 cores per processor. </li><li>The runtime for the backward is typically less than twice the forward computation. Although the backward requires solve two linear systems (one of them is in the forward computation), the AMG linear solver in the back-propagation may converge faster, and therefore costs less than the forward. </li></ol><p>Additionally, we show the overhead, which is defined as the difference between total runtime and Hypre linear solver time, of both the forward and backward calculation. </p><table><tr><th style="text-align: right">1 Core</th><th style="text-align: right">4 Cores</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/overhead_core1.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/overhead_core4.png?raw=true" alt/></td></tr></table><p>We see that the overhead is quite small compared to the total time, especially when the problem size is large. This indicates that the ADCME MPI implementation is very effective. </p><h3 id="Strong-Scaling"><a class="docs-heading-anchor" href="#Strong-Scaling">Strong Scaling</a><a id="Strong-Scaling-1"></a><a class="docs-heading-anchor-permalink" href="#Strong-Scaling" title="Permalink"></a></h3><p>Now we consider the strong scaling. In this case, we fixed the whole problem size and split the mesh onto different MPI processors. The following plots show the runtime for the forward computation and the gradient back-propagation</p><table><tr><th style="text-align: right">Forward</th><th style="text-align: right">Bckward</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/serial_forward.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/serial_backward.png?raw=true" alt/></td></tr></table><p>The following plots show the speedup and efficiency </p><table><tr><th style="text-align: right">1 Core</th><th style="text-align: right">4 Cores</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/time_core1.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/time_core4.png?raw=true" alt/></td></tr></table><p>We can see that the 4 cores have smaller runtime compared to 1 core. </p><p>Interested readers can go to <a href="https://github.com/kailaix/ADCME.jl/tree/master/docs/src/assets/Codes/MPI">here</a> for implementations. To compile the codes, make sure that MPI and Hypre is available (see <a href="@ref"><code>install_openmpi</code></a> and <a href="@ref"><code>install_hypre</code></a>) and run the following script:</p><pre><code class="language-julia">using ADCME 
change_directory(&quot;ccode/build&quot;)
ADCME.cmake()
ADCME.make()</code></pre><h2 id="Acoustic-Seismic-Inversion"><a class="docs-heading-anchor" href="#Acoustic-Seismic-Inversion">Acoustic Seismic Inversion</a><a id="Acoustic-Seismic-Inversion-1"></a><a class="docs-heading-anchor-permalink" href="#Acoustic-Seismic-Inversion" title="Permalink"></a></h2><p>In this example, we consider the acoustic wave equation with perfectly matched layer (PML). The governing equation for the acoustic equation is</p><p class="math-container">\[\frac{\partial^2 u}{\partial t^2} = \nabla \cdot (c^2 \nabla u)\]</p><p>where <span>$u$</span> is the displacement, <span>$f$</span> is the source term, and <span>$c$</span> is the spatially varying acoustic velocity. </p><p>In the inverse problem, only the wavefield <span>$u$</span> on the surface is observable, and we want to use this information to estimate <span>$c$</span>. The problem is usually ill-posed, so regularization techniques are usually used to constrain <span>$c$</span>. One approach is to represent <span>$c$</span> by a deep neural network</p><p class="math-container">\[c(x,y) = \mathcal{NN}_\theta(x,y)\]</p><p>where <span>$\theta$</span> is the neural network weights and biases. The loss function is formulated by the square loss for the wavefield on the surface. </p><table><tr><th style="text-align: right">Model <span>$c$</span></th><th style="text-align: right">Wavefield</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/acoustic_model.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/acoustic_wavefield.gif?raw=true" alt/></td></tr></table><p>To implement an MPI version of the acoustic wave equation propagator, we use <a href="../api/#ADCME.mpi_halo_exchange-Tuple{Union{Array{Float64,2}, PyCall.PyObject},Int64,Int64}"><code>mpi_halo_exchange</code></a>, which is implemented using MPI and performs the halo exchange mentioned in the last example for both wavefields and axilliary fields. This function communicates the boundary information for each block of the mesh. The following plot shows the computational graph for the numerical simulation of the acoustic wave equation</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/wave.png?raw=true" alt/></p><p>The following plot shows the strong scaling and weak scaling of our implementation. Each processor consists of 32 processors, which are used at the discretion of ADCME&#39;s backend, i.e., TensorFlow. The strong scaling result is obtained by using a <span>$1000\times 1000$</span> grid and 100 times steps. For the weak scaling result, each MPI processor owns a <span>$100\times 100$</span> grid, and the total number of steps is 2000. It is remarkable that even though we increase the number of processors from 1 to 100, the total time only increases 2 times in the weak scaling. </p><table><tr><th style="text-align: right">Strong Scaling</th><th style="text-align: right">Weak Scaling</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/acoustic_time_forward_and_backward.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/acoustic_weak.png?raw=true" alt/></td></tr></table><p>We also show the speedup and efficiency for the strong scaling case. We can achieve more than 20 times acceleration by using 100 processors (3200 cores in total) and the trend is not slowing down at this scale. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/acoustic_speedup_and_efficiency.png?raw=true" alt/> </p><h2 id="Elastic-Seismic-Inversion"><a class="docs-heading-anchor" href="#Elastic-Seismic-Inversion">Elastic Seismic Inversion</a><a id="Elastic-Seismic-Inversion-1"></a><a class="docs-heading-anchor-permalink" href="#Elastic-Seismic-Inversion" title="Permalink"></a></h2><p>In the last example, we consider the elastic wave equation</p><p class="math-container">\[\begin{aligned}
\rho \frac{\partial v_i}{\partial t} &amp;= \sigma_{ij,j} + \rho f_i \\ 
\frac{\partial \sigma_{ij}}{\partial t} &amp;= \lambda v_{k, k} + \mu (v_{i,j}+v_{j,i})
\end{aligned}\tag{2}\]</p><p>where <span>$v$</span> is the velocity, <span>$\sigma$</span> is the stress tensor, <span>$\rho$</span> is the density, and <span>$\lambda$</span> and <span>$\mu$</span> are the Lamé constants. Similar to the acoustic equation, we use the PML boundary conditions and have observations on the surface. However, the inversion parameters are now spatially varying <span>$\rho$</span>, <span>$\lambda$</span> and <span>$\mu$</span>. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/elastic_wavefield.gif?raw=true" alt/></p><p>As an example, we approximate <span>$\lambda$</span> by a deep neural network</p><p class="math-container">\[\lambda(x,y) = \mathcal{NN}_\theta(x,y)\]</p><p>and the other two parameters are kept fixed. </p><p>We use the same geometry settings as the acoustic wave equation case. Note that elastic wave equation has more state variables as well as auxilliary fields, and thus is more memory demanding. The huge memory cost calls for a  distributed framework, especially for large-scale problems. </p><p>Additionally, we use fourth-order finite difference scheme for discretizing Equation 2. This scheme requires us to exchange two layers on the boundaries for each block in the mesh. This data communication is implemented using MPI, i.e., <a href="../api/#ADCME.mpi_halo_exchange2-Tuple{Union{Array{Float64,2}, PyCall.PyObject},Int64,Int64}"><code>mpi_halo_exchange2</code></a>.</p><p>The following plots show the strong and weak scaling. Again, we see that the weak scaling of the implementation is quite effective because the runtime increases mildly even if we increase the number of processors from 1 to 100. </p><table><tr><th style="text-align: right">Strong Scaling</th><th style="text-align: right">Weak Scaling</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/elastic_time_forward_and_backward.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/elastic_weak.png?raw=true" alt/></td></tr></table><p>The following plots show the speedup and efficiency for the strong scaling. We can achieve more than 20 times speedup when using 100 processors. <img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/elastic_speedup_and_efficiency.png?raw=true" alt/> </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mpi/">« Distributed Scientific Machine Learning using MPI</a><a class="docs-footer-nextpage" href="../multithreading/">Understand the Multi-threading Model »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 31 January 2021 22:57">Sunday 31 January 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
