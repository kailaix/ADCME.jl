<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training Deep Neural Networks with Trust-Region Methods · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Training Deep Neural Networks with Trust-Region Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training Deep Neural Networks with Trust-Region Methods</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/second_order_optimizer.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-Deep-Neural-Networks-with-Trust-Region-Methods"><a class="docs-heading-anchor" href="#Training-Deep-Neural-Networks-with-Trust-Region-Methods">Training Deep Neural Networks with Trust-Region Methods</a><a id="Training-Deep-Neural-Networks-with-Trust-Region-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Training-Deep-Neural-Networks-with-Trust-Region-Methods" title="Permalink"></a></h1><p>Trust-region methods are a class of global optimization methods. The basic idea is to successively solve an approximated optimization problem in a small neighborhood of the current state. For example, we can approximate the local landscape of the objective function using a quadratic function, and thus can solve efficiently and accurately. The biggest advantage of trust-region methods in the context of deep neural network is that they can escape saddle points, which are demonstrated to be the dominant causes for slow convergence, with proper algorithm design. </p><p>However, the most challenging problem with trust-region methods is that we need to calculate the Hessian (curvature information) to leverage the local curvature information. Computing the Hessian can be quite expensive and challenging, especially if the forward computation involves complex procedures and has a large number of optimizable variables. Fortunately, for many deep neural network based inverse problems, the DNNs do not need to be huge for good accuracy. Therefore, calculating the Hessian is plausible. This does not mean that efficient computation is easy, and we introduce the technique is another post. In this post, we compare the trust-region method with other competing methods (L-BFGS-B, BFGS, and ADAM optimizer) for training deep neural networks that are coupled with a numerical solver. We also shed lights on why the other optimizers slow down. </p><h2 id="Trust-region-Methods"><a class="docs-heading-anchor" href="#Trust-region-Methods">Trust-region Methods</a><a id="Trust-region-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Trust-region-Methods" title="Permalink"></a></h2><p>We consider an unconstrained optimization problem </p><p class="math-container">\[\min_x f(x) \tag{1}\]</p><p>The trust-region method solves the optimization problem Eq. 1 by iteratively solving many simpler subproblems, which are good approximation to <span>$f(x_k)$</span> at the neighborhood of <span>$x_k$</span>. We model <span>$f(x_k+s)$</span> using a quadratic model </p><p class="math-container">\[m(s) = f_k + s^T g_k + \frac{1}{2}s^T H_k s \tag{2}\]</p><p>Here <span>$f_k = f(x_k)$</span>, <span>$g_k = \nabla f(x_k)$</span>, <span>$H_k = \nabla^2 f(x_k)$</span>. </p><p>Eq. 2 is essentially the Taylor expansion of <span>$f(x)$</span> at <span>$x_k$</span>. This approximation is only accurate within the neighborhood of <span>$x_k$</span>. Therefore, we constrain our subproblem to a <strong>trust region</strong> </p><p class="math-container">\[||s||\leq \Delta_k\]</p><p>The subproblem has the following form </p><p class="math-container">\[\begin{aligned}\min_{s} &amp; \; m(s) \\ \text{s.t.} &amp; \; \|s\|\leq \Delta_k\end{aligned} \tag{3}\]</p><p>In this work, we use the method proposed in [^trust-region] to solve Eq. 3 nearly exactly. </p><p>[^trust-region]: A.R. Conn, N.I. Gould, and P.L. Toint, &quot;Trust region methods&quot;, Siam, pp. 169-200, 2000.</p><h2 id="Model-Problem:-Static-Poisson&#39;s-Equation"><a class="docs-heading-anchor" href="#Model-Problem:-Static-Poisson&#39;s-Equation">Model Problem: Static Poisson&#39;s Equation</a><a id="Model-Problem:-Static-Poisson&#39;s-Equation-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Problem:-Static-Poisson&#39;s-Equation" title="Permalink"></a></h2><p>In this example, we consider the Poisson&#39;s equation </p><p class="math-container">\[\nabla \cdot (\kappa_\theta(x) \nabla u)) = f(x), \;x\in \Omega, \; x\in \partial\Omega\]</p><p>Here <span>$\kappa_\theta(x)$</span> is a deep neural network and <span>$\theta$</span> is the weights and biases. We discretize <span>$\Omega$</span> using a uniform grid. Assume we can observe the full field data <span>$u_{obs}$</span> on the grid points. We can then train the deep neural network using the residual minimization method [^residual-minimization]</p><p class="math-container">\[\min_\theta \sum_{i,j} (F_{i,j}(\theta) - f_{i,j})^2 \tag{4}\]</p><p>Here <span>$F_{i,j}(\theta)$</span> is the finite difference discretization of <span>$\nabla \cdot (\kappa_\theta(x) \nabla u_{obs}))$</span> at the grid points. In our benchmark, we add 10% uniform random noise to <span>$f$</span> and <span>$u_{obs}$</span> to make the problem more challenging.  </p><p>[^residual-minimization]: Huang, Daniel Z., et al. &quot;Learning constitutive relations from indirect observations using deep neural networks.&quot; Journal of Computational Physics (2020): 109491.</p><p>We apply 4 optimizers to solve Eq. 4. Because the optimization results depend on the initialization of the deep neural network, we use 5 different initial guess for DNNs. The result is shown below</p><table><tr><th style="text-align: right">Case</th><th style="text-align: right">Convergence Plots</th></tr><tr><td style="text-align: right">1</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses2_static.png?raw=true" alt/></td></tr><tr><td style="text-align: right">2</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses23_static.png?raw=true" alt/></td></tr><tr><td style="text-align: right">3</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses233_static.png?raw=true" alt/></td></tr><tr><td style="text-align: right">4</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses2333_static.png?raw=true" alt/></td></tr><tr><td style="text-align: right">5</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses23333_static.png?raw=true" alt/></td></tr></table><p>We can see for all cases, the trust-region method provides a much more accurate result, and in general converges faster. The ADAM optimizer is the least competent, partially because it&#39;s a first-order optimizer and is not able to fully leverage the curvature information. The BFGS optimizers constructs an approximate Hessian that is SPD. The L-BFGS-B optimizer is an approximation to BFGS, where it uses only a limited number of previous iterates to construct the Hessian matrix. As mentioned, in the optimization problem involving deep neural networks, the slow down is mainly due to the saddle point, where the descent direction corresponds to the negative eigenvalues of the Hessian matrix. Because BFGS and L-BFGS-B ensure that the Hessian matrix is SPD, they cannot provide approximate guidance to escape the saddle point. This hypothesis is demonstrated in the following plot, where we show the distribution of Hessian eigenvalues at the last step for Case 2</p><table><tr><th style="text-align: right">Optimizer</th><th style="text-align: right">L-BFGS-B</th><th style="text-align: right">BFGS</th><th style="text-align: right">Trust Region</th></tr><tr><td style="text-align: right">Eigenvalue Distribution</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/static_lbfgs_eig.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/static_bfgs_eig.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/static_tr_eig.png?raw=true" alt/></td></tr></table><p>We also show the number of <strong>negative</strong> eigenvalues for the BFGS optimizer</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/static_positive_eigvals.png?raw=true" alt/></p><p>We can see that the number of negative eigenvalues stays at around 150 after 1300 iterations. That&#39;s also where the BFGS optimizer starts to stagnate. This means that the Hessian matrix of the optimization problem is <strong>never SPD</strong> during the iteration of the BFGS optimizer. The non-SPD property is actually inconsistent with the local convexity assumption at stationary points  in the convergence theory of the BFGS optimizer!</p><p>We also analyze the direction of the search direction <span>$p_k$</span> in the BFGS optimizer. We consider two values</p><p class="math-container">\[\begin{aligned}\cos(\theta_1) &amp;= \frac{-p_k^T g_k}{\|p_k\|\|g_k\|} \\ \cos(\theta_2) &amp;= \frac{p_k^T q_k}{\|p_k\|\|q_k\|}\end{aligned}\]</p><p>Here <span>$q_k$</span> is the direction for the <strong>Cauchy point</strong>, which is the descent direction in the trust-region method</p><p class="math-container">\[q_k = -H_k^{-1}g_k\]</p><p>The two quantities are shown in the following plots (since the trust-region method converges in around 270 iterations, <span>$\cos(\theta2)$</span> only has limited data points)</p><table><tr><th style="text-align: right"><span>$\cos(\theta_1)$</span></th><th style="text-align: right"><span>$\cos(\theta_2)$</span></th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/static_angles.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/static_angles2.png?raw=true" alt/></td></tr></table><p>There are two conclusions to draw from the plots</p><ol><li>The search direction of the BFGS optimizer deviates from the gradient descent method. </li><li>The search direction of the BFGS optimizer is not very correlated with the Cauchy point direction; this indicates the search direction poorly recognizes the negative curvature directions. </li></ol><h2 id="Another-Example:-Heat-Equation"><a class="docs-heading-anchor" href="#Another-Example:-Heat-Equation">Another Example: Heat Equation</a><a id="Another-Example:-Heat-Equation-1"></a><a class="docs-heading-anchor-permalink" href="#Another-Example:-Heat-Equation" title="Permalink"></a></h2><p>We consider a time-dependent PDE problem: the heat equation</p><p class="math-container">\[\frac{\partial u}{\partial t} = \nabla \cdot (\kappa_\theta(x) \nabla u)) + f(x), \;x\in \Omega, \; x\in \partial\Omega\]</p><p>We assume that we can observe the full field data of <span>$u$</span> as snapshots. We again apply the residual minimization method to train the deep neural network. The following shows the convergence plots for different initial guesses of the DNNs. </p><table><tr><th style="text-align: right">Case</th><th style="text-align: right">Convergence Plots</th></tr><tr><td style="text-align: right">1</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses2_dynamic.png?raw=true" alt/></td></tr><tr><td style="text-align: right">2</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses23_dynamic.png?raw=true" alt/></td></tr><tr><td style="text-align: right">3</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses233_dynamic.png?raw=true" alt/></td></tr><tr><td style="text-align: right">4</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses2333_dynamic.png?raw=true" alt/></td></tr><tr><td style="text-align: right">5</td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/second_order_optimizer/losses23333_dynamic.png?raw=true" alt/></td></tr></table><p>We see that the trust-region is more competitive than the other methods. </p><h2 id="Limitations"><a class="docs-heading-anchor" href="#Limitations">Limitations</a><a id="Limitations-1"></a><a class="docs-heading-anchor-permalink" href="#Limitations" title="Permalink"></a></h2><p>Despite many promising features of the trust region method, it is not without limitations, which we want to discuss here. The current trust-region method requires calculating the Hessian matrix. Firstly, computing the Hessian matrix can be technically difficult, especially when DNNs are coupled with a sophisticated numerical PDE solver. There are many existing techniques for computing the Hessian. The TensorFlow backend supports Hessian computation concurrently, but it requires users to implement rules for calculating &quot;gradients of gradients&quot;. Additionally, TensorFlow uses forward propagation to evaluate the Hessian. This means that TensorFlow loops over each gradient component and calculating a row of Hessian at a time. This does not leverage the symmetry of Hessians and can be quite inefficient if the number of unknowns is large. Another approach, edge pushing algorithms, uses one backward pass to evaluate the Hessian. This approach takes advantage of the symmetry of Hessians. However, the implementation can be quite convolved and computations can be expensive in some scenarios. We will discuss0 in more details in another post. </p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>Trust-region methods are a class of global optimization techniques. They are less popular in the deep learning approach because the DNNs tend to be huge and the computation of Hessians is expensive. However, they are very suitable for many computational engineering problems, where DNNs are typically small, and convergence as well as accuracy is a critical concern. Our point of view is that although the Hessian computations are expensive, they are quite rewarding. Future researches will focus on efficient computation and automation of Hessian computations. </p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 15 December 2020 04:31">Tuesday 15 December 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
