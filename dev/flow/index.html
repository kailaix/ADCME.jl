<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Normalizing Flows · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li><li><a class="tocitem" href="../sqlite3/">Introducing ADCME Database and SQL Integration: an Efficient Approach to Simulation Data Management</a></li><li><a class="tocitem" href="../hessian/">The Mathematical Structure of DNN Hessians</a></li><li><a class="tocitem" href="../plotly/">Visualization with Plotly</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li class="is-active"><a class="tocitem" href>Normalizing Flows</a><ul class="internal"><li><a class="tocitem" href="#Type-Hierarchy"><span>Type Hierarchy</span></a></li><li><a class="tocitem" href="#A-Simple-Example"><span>A Simple Example</span></a></li><li><a class="tocitem" href="#Models"><span>Models</span></a></li></ul></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li><li><a class="tocitem" href="../reinforcement_learning/">Reinforcement Learning Basics: Q-learning and SARSA</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li><li><a class="tocitem" href="../windows_installation/">Install ADCME on Windows</a></li><li><a class="tocitem" href="../docker/">Install ADCME Docker Image</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Deep Learning Schemes</a></li><li class="is-active"><a href>Normalizing Flows</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Normalizing Flows</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/flow.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Normalizing-Flows"><a class="docs-heading-anchor" href="#Normalizing-Flows">Normalizing Flows</a><a id="Normalizing-Flows-1"></a><a class="docs-heading-anchor-permalink" href="#Normalizing-Flows" title="Permalink"></a></h1><p>In this article we introduce the ADCME module for flow-based generative models. The flow-based generative models can be used to model the joint distribution of high-dimensional random variables. It constructs a sequence of invertible transformation of distributions</p><p class="math-container">\[x = f(u) \quad u \sim \pi(u)\]</p><p>based on the change of variable equation</p><p class="math-container">\[p(x) = \pi(f^{-1}(x)) \left|\det\left(\frac{\partial f^{-1}(x)}{\partial x}\right)\right|\]</p><div class="admonition is-category-example"><header class="admonition-header">Example</header><div class="admonition-body"><p>Consider the transformation <span>$f: \mathbb{R}\rightarrow [0,1]$</span>, s.t. <span>$f(x) = \mathrm{sigmoid}(x) = \frac{1}{1+e^{-x}}$</span>. Consider the random variable </p><p class="math-container">\[u \sim \mathcal{N}(0,1)\]</p><p>We want to find out the probability density function of <span>$p(x)$</span>, where <span>$x=f(u)$</span>. To this end, we have </p><p class="math-container">\[f^{-1}(x)=\log(x) - \log(1-x) \Rightarrow \frac{\partial f^{-1}(x)}{\partial x} = \frac{1}{x} + \frac{1}{1-x}\]</p><p>Therefore, we have</p><p class="math-container">\[\begin{aligned}
p(x) &amp;= \pi(f^{-1}(x))\left( \frac{1}{x} + \frac{1}{1-x}\right) \\ 
&amp;= \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(\log(x)-\log(1-x))^2}{2}\right)\left( \frac{1}{x} + \frac{1}{1-x}\right)
\end{aligned}\]</p></div></div><p>Compared to other generative models such as variational autoencoder (VAE) and generative neural networks (GAN), the flow-based generative models give us explicit formuas of density functions. For model training, we can directly minimizes the posterier log likelihood in the flow-based generative models, while use approximate likelihood functions in VAE and adversarial training in GAN. In general, the flow-based generative model is easier to train than VAE and GAN. In the following, we give some examples of using flow-based generatives models in ADCME. </p><h2 id="Type-Hierarchy"><a class="docs-heading-anchor" href="#Type-Hierarchy">Type Hierarchy</a><a id="Type-Hierarchy-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Hierarchy" title="Permalink"></a></h2><p>The flow-based generative model is organized as follows, from botton level to top level:</p><ul><li><code>FlowOp</code>. This consists of unit invertible transformations, such as <a href="@ref"><code>AffineConstantFlow</code></a> and <a href="@ref"><code>Invertible1x1Conv</code></a>.</li><li><code>NormalizingFlow</code>. This is basically a sequence of <code>FlowOp</code>. It is not exposed to users. </li><li><code>NormalizingFlowModel</code>. This is a container of the sequence of <code>FlowOp</code>s and a prior distribution. <code>NormalizingFlowModel</code> is callable and can &quot;normalize&quot; the data distribution. We can also sample from <code>NormalizingFlowModel</code>, where the prior distribution is transformed to data distribution. </li></ul><h2 id="A-Simple-Example"><a class="docs-heading-anchor" href="#A-Simple-Example">A Simple Example</a><a id="A-Simple-Example-1"></a><a class="docs-heading-anchor-permalink" href="#A-Simple-Example" title="Permalink"></a></h2><p>Let&#39;s consider a simple example for transforming the two moons dataset to a univariate Gaussian distribution. First, we adapt a function from <a href="https://github.com/wildart/nmoons">here</a> and use it to generate the dataset</p><pre><code class="language-julia">using Revise
using ADCME
using PyCall
using PyPlot
using Random

# `nmoons` is adapted from https://github.com/wildart/nmoons
function nmoons(::Type{T}, n::Int=100, c::Int=2;
    shuffle::Bool=false, ε::Real=0.1, d::Int = 2,
    translation::Vector{T}=zeros(T, d),
    rotations::Dict{Pair{Int,Int},T} = Dict{Pair{Int,Int},T}(),
    seed::Union{Int,Nothing}=nothing) where {T &lt;: Real}
    rng = seed === nothing ? Random.GLOBAL_RNG : MersenneTwister(Int(seed))
    ssize = floor(Int, n/c)
    ssizes = fill(ssize, c)
    ssizes[end] += n - ssize*c
    @assert sum(ssizes) == n &quot;Incorrect partitioning&quot;
    pi = convert(T, π)
    R(θ) = [cos(θ) -sin(θ); sin(θ) cos(θ)]
    X = zeros(d,0)
    for (i, s) in enumerate(ssizes)
    circ_x = cos.(range(zero(T), pi, length=s)).-1.0
    circ_y = sin.(range(zero(T), pi, length=s))
    C = R(-(i-1)*(2*pi/c)) * hcat(circ_x, circ_y)&#39;
    C = vcat(C, zeros(d-2, s))
    dir = zeros(d)-C[:,end] # translation direction
    X = hcat(X, C .+ dir.*translation)
    end
    y = vcat([fill(i,s) for (i,s) in enumerate(ssizes)]...)
    if shuffle
        idx = randperm(rng, n)
        X, y = X[:, idx], y[idx]
    end
    # Add noise to the dataset
    if ε &gt; 0.0
        X += randn(rng, size(X)).*convert(T,ε/d)
    end
    # Rotate dataset
    for ((i,j),θ) in rotations
        X[[i,j],:] .= R(θ)*view(X,[i,j],:)
    end
    return X, y
end

function sample_moons(n)
    X, _ = nmoons(Float64, n, 2, ε=0.05, d=2, translation=[0.25, -0.25])
    return Array(X&#39;)
end</code></pre><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/moons.png?raw=true" alt/></p><p>Next we construct a flow-based generative model, as follows:</p><pre><code class="language-julia">prior = ADCME.MultivariateNormalDiag(loc=zeros(2))
model = NormalizingFlowModel(prior, flows)</code></pre><p>We can print the model by just type <code>model</code> in REPL:</p><pre><code class="language-none">( MultivariateNormalDiag )
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow
        ↓
AffineHalfFlow</code></pre><p>Finally, we maximize the log llikelihood function using <a href="../api/#ADCME.AdamOptimizer"><code>AdamOptimizer</code></a></p><pre><code class="language-julia">

x = placeholder(rand(128,2))
zs, prior_logpdf, logdet = model(x)
log_pdf = prior_logpdf + logdet
loss = -sum(log_pdf)

model_samples = rand(model, 128*8)
sess = Session(); init(sess)
opt = AdamOptimizer(1e-4).minimize(loss)
sess = Session(); init(sess)
for i = 1:10000
    _, l = run(sess, [opt, loss], x=&gt;sample_moons(128))
    if mod(i,100)==0
        @info i, l 
    end
end</code></pre><h2 id="Models"><a class="docs-heading-anchor" href="#Models">Models</a><a id="Models-1"></a><a class="docs-heading-anchor-permalink" href="#Models" title="Permalink"></a></h2><p>ADCME has implemeted some popular flow-based generative models. For example, <a href="@ref"><code>AffineConstantFlow</code></a>, <a href="@ref"><code>AffineHalfFlow</code></a>, <a href="@ref"><code>SlowMAF</code></a>, <a href="@ref"><code>MAF</code></a>, <a href="@ref"><code>IAF</code></a>, <a href="@ref"><code>ActNorm</code></a>,  <a href="@ref"><code>Invertible1x1Conv</code></a>, <a href="@ref"><code>NormalizingFlow</code></a>, <a href="@ref"><code>NormalizingFlowModel</code></a>, and <a href="@ref"><code>NeuralCouplingFlow</code></a>. </p><p>The following script shows how to usee those functions:</p><pre><code class="language-julia"># Adapted from https://github.com/karpathy/pytorch-normalizing-flows
using Revise
using ADCME
using PyCall
using PyPlot
using Random

# `nmoons` is adapted from https://github.com/wildart/nmoons
function nmoons(::Type{T}, n::Int=100, c::Int=2;
    shuffle::Bool=false, ε::Real=0.1, d::Int = 2,
    translation::Vector{T}=zeros(T, d),
    rotations::Dict{Pair{Int,Int},T} = Dict{Pair{Int,Int},T}(),
    seed::Union{Int,Nothing}=nothing) where {T &lt;: Real}
    rng = seed === nothing ? Random.GLOBAL_RNG : MersenneTwister(Int(seed))
    ssize = floor(Int, n/c)
    ssizes = fill(ssize, c)
    ssizes[end] += n - ssize*c
    @assert sum(ssizes) == n &quot;Incorrect partitioning&quot;
    pi = convert(T, π)
    R(θ) = [cos(θ) -sin(θ); sin(θ) cos(θ)]
    X = zeros(d,0)
    for (i, s) in enumerate(ssizes)
    circ_x = cos.(range(zero(T), pi, length=s)).-1.0
    circ_y = sin.(range(zero(T), pi, length=s))
    C = R(-(i-1)*(2*pi/c)) * hcat(circ_x, circ_y)&#39;
    C = vcat(C, zeros(d-2, s))
    dir = zeros(d)-C[:,end] # translation direction
    X = hcat(X, C .+ dir.*translation)
    end
    y = vcat([fill(i,s) for (i,s) in enumerate(ssizes)]...)
    if shuffle
        idx = randperm(rng, n)
        X, y = X[:, idx], y[idx]
    end
    # Add noise to the dataset
    if ε &gt; 0.0
        X += randn(rng, size(X)).*convert(T,ε/d)
    end
    # Rotate dataset
    for ((i,j),θ) in rotations
        X[[i,j],:] .= R(θ)*view(X,[i,j],:)
    end
    return X, y
end

function sample_moons(n)
    X, _ = nmoons(Float64, n, 2, ε=0.05, d=2, translation=[0.25, -0.25])
    return Array(X&#39;)
end


#------------------------------------------------------------------------------------------
# RealNVP
function mlp(x, k, id)
    x = constant(x)
    variable_scope(&quot;layer$k$id&quot;) do
        x = dense(x, 24, activation=&quot;leaky_relu&quot;)
        x = dense(x, 24, activation=&quot;leaky_relu&quot;)
        x = dense(x, 24, activation=&quot;leaky_relu&quot;)
        x = dense(x, 1)
    end
    return x
end
flows = [AffineHalfFlow(2, mod(i,2)==1, x-&gt;mlp(x, i, 0), x-&gt;mlp(x, i, 1)) for i = 0:8]


#------------------------------------------------------------------------------------------
# RealNVP
function mlp(x, k, id)
    x = constant(x)
    variable_scope(&quot;layer$k$id&quot;) do
        x = dense(x, 24, activation=&quot;leaky_relu&quot;)
        x = dense(x, 24, activation=&quot;leaky_relu&quot;)
        x = dense(x, 24, activation=&quot;leaky_relu&quot;)
        x = dense(x, 1)
    end
    return x
end
flows = [AffineHalfFlow(2, mod(i,2)==1, x-&gt;mlp(x, i, 0), x-&gt;mlp(x, i, 1)) for i = 0:8]


#------------------------------------------------------------------------------------------
# NICE
# function mlp(x, k, id)
#     x = constant(x)
#     variable_scope(&quot;layer$k$id&quot;) do
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 1)
#     end
#     return x
# end
# flow1 = [AffineHalfFlow(2, mod(i,2)==1, missing, x-&gt;mlp(x, i, 1)) for i = 0:4]
# flow2 = [AffineConstantFlow(2, shift=false)]
# flows = [flow1;flow2]


# SlowMAF
#------------------------------------------------------------------------------------------
# function mlp(x, k, id)
#     x = constant(x)
#     variable_scope(&quot;layer$k$id&quot;) do
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 2)
#     end
#     return x
# end
# flows = [SlowMAF(2, mod(i,2)==1, [x-&gt;mlp(x, i, 0)]) for i = 0:3]

# MAF
#------------------------------------------------------------------------------------------ 
# flows = [MAF(2, mod(i,2)==1, [24, 24, 24], name=&quot;layer$i&quot;) for i = 0:3]



# IAF 
#------------------------------------------------------------------------------------------ 
# flows = [IAF(2, mod(i,2)==1, [24, 24, 24], name=&quot;layer$i&quot;) for i = 0:3]
# prior = ADCME.MultivariateNormalDiag(loc=zeros(2))
# model = NormalizingFlowModel(prior, flows)

# Insert ActNorm to any of the flows 
#------------------------------------------------------------------------------------------ 
# flow2 = [ActNorm(2, &quot;ActNorm$i&quot;) for i = 1:length(flows)]
# flows = permutedims(hcat(flow2, flows))[:]
# # error()
# # msample = rand(model,1)
# # zs, prior_logprob, log_det = model([0.0040 0.4426])
# # sess = Session(); init(sess)
# # run(sess, msample)
# # run(sess,zs)


# GLOW
#------------------------------------------------------------------------------------------ 
# function mlp(x, k, id)
#     x = constant(x)
#     variable_scope(&quot;layer$k$id&quot;) do
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 24, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 1)
#     end
#     return x
# end
# flows = [Invertible1x1Conv(2, &quot;conv$i&quot;) for i = 0:2]
# norms = [ActNorm(2, &quot;ActNorm$i&quot;) for i = 0:2]
# couplings = [AffineHalfFlow(2, mod(i, 2)==1, x-&gt;mlp(x, i, 0), x-&gt;mlp(x, i, 1)) for i = 0:length(flows)-1]
# flows = permutedims(hcat(norms, flows, couplings))[:]

#------------------------------------------------------------------------------------------ 
# Neural Splines Coupling
# function mlp(x, k, id)
#     x = constant(x)
#     variable_scope(&quot;fc$k$id&quot;) do
#         x = dense(x, 16, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 16, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 16, activation=&quot;leaky_relu&quot;)
#         x = dense(x, 3K-1)
#     end
#     return x
# end
# K = 8
# flows = [NeuralCouplingFlow(2, x-&gt;mlp(x, i, 0), x-&gt;mlp(x, i, 1), K) for i = 0:2]
# convs = [Invertible1x1Conv(2, &quot;conv$i&quot;) for i = 0:2]
# norms = [ActNorm(2, &quot;ActNorm$i&quot;) for i = 0:2]
# flows = permutedims(hcat(norms, convs, flows))[:]

#------------------------------------------------------------------------------------------ 


prior = ADCME.MultivariateNormalDiag(loc=zeros(2))
model = NormalizingFlowModel(prior, flows)


x = placeholder(rand(128,2))
zs, prior_logpdf, logdet = model(x)
log_pdf = prior_logpdf + logdet
loss = -sum(log_pdf)

model_samples = rand(model, 128*8)
sess = Session(); init(sess)
opt = AdamOptimizer(1e-4).minimize(loss)
sess = Session(); init(sess)
for i = 1:10000
    _, l = run(sess, [opt, loss], x=&gt;sample_moons(128))
    if mod(i,100)==0
        @info i, l 
    end
end

z = run(sess, model_samples[end]) 
x = sample_moons(128*8)
scatter(x[:,1], x[:,2], c=&quot;b&quot;, s=5, label=&quot;data&quot;)
scatter(z[:,1], z[:,2], c=&quot;r&quot;, s=5, label=&quot;prior --&gt; posterior&quot;)
axis(&quot;scaled&quot;); xlabel(&quot;x&quot;); ylabel(&quot;y&quot;)#</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../vae/">« Variational Autoencoder</a><a class="docs-footer-nextpage" href="../convnet/">Convolutional Neural Network »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 21 May 2021 03:06">Friday 21 May 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
