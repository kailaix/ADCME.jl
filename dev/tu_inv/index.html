<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Inverse Modeling with ADCME · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li class="is-active"><a class="tocitem" href>Inverse Modeling with ADCME</a><ul class="internal"><li><a class="tocitem" href="#Parameter-Inverse-Problem"><span>Parameter Inverse Problem</span></a></li><li><a class="tocitem" href="#Function-Inverse-Problem"><span>Function Inverse Problem</span></a></li><li><a class="tocitem" href="#Functional-Inverse-Problem"><span>Functional Inverse Problem</span></a></li><li><a class="tocitem" href="#Stochastic-Inverse-Problem"><span>Stochastic Inverse Problem</span></a></li></ul></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>Inverse Modeling with ADCME</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Inverse Modeling with ADCME</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/tu_inv.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Inverse-Modeling-with-ADCME"><a class="docs-heading-anchor" href="#Inverse-Modeling-with-ADCME">Inverse Modeling with ADCME</a><a id="Inverse-Modeling-with-ADCME-1"></a><a class="docs-heading-anchor-permalink" href="#Inverse-Modeling-with-ADCME" title="Permalink"></a></h1><p>Roughly speaking, there are four types of inverse modeling in partial differential equations. We have developed numerical methods that takes advantage of deep neural networks and automatic differentiation. To be more concrete, let the forward model be a 1D Poisson equation</p><p class="math-container">\[\begin{aligned}-\nabla (X\nabla u(x)) &amp;= \varphi(x) &amp; x\in (0,1)\\ u(0)=u(1) &amp;= 0\end{aligned}\]</p><p>Here <span>$X$</span> is the unknown  which may be one of the four forms: parameter, function, functional or random variable. </p><table><tr><th style="text-align: right"><strong>Inverse problem</strong></th><th style="text-align: right"><strong>Problem type</strong></th><th style="text-align: right"><strong>ADCME Approach</strong></th><th style="text-align: center"><strong>Reference</strong></th></tr><tr><td style="text-align: right"><span>$\nabla\cdot(c\nabla u) = \varphi(x)$</span></td><td style="text-align: right">Parameter</td><td style="text-align: right">Adjoint-State Method</td><td style="text-align: center"><a href="http://arxiv.org/abs/1912.07552">1</a> <a href="http://arxiv.org/abs/1912.07547">2</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(f(x)\nabla u) = \varphi(x)$</span></td><td style="text-align: right">Function</td><td style="text-align: right">DNN as a Function Approximator</td><td style="text-align: center"><a href="https://arxiv.org/abs/1901.07758">3</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(f(u)\nabla u) = \varphi(x)$</span></td><td style="text-align: right">Functional</td><td style="text-align: right">Residual Learning or Physics Constrained Learning</td><td style="text-align: center"><a href="https://arxiv.org/abs/1905.12530">4</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(\varpi\nabla u) = \varphi(x)$</span></td><td style="text-align: right">Stochastic Inversion</td><td style="text-align: right">Generative Neural Networks</td><td style="text-align: center"><a href="https://arxiv.org/abs/1910.06936">5</a></td></tr></table><h2 id="Parameter-Inverse-Problem"><a class="docs-heading-anchor" href="#Parameter-Inverse-Problem">Parameter Inverse Problem</a><a id="Parameter-Inverse-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Inverse-Problem" title="Permalink"></a></h2><p>When <span>$X$</span> is just a scalar/vector, we call this type of problem <strong>parameter inverse problem</strong>. We consider a manufactured solution: the exact <span>$X=1$</span> and <span>$u(x)=x(1-x)$</span>, so we have</p><p class="math-container">\[\varphi(x) = 2\]</p><p>Assume we can observe <span>$u(0.5)=0.25$</span> and the initial guess for <span>$X_0=10$</span>. We use finite difference method to discretize the PDE and the interval <span>$[0,1]$</span> is divided uniformly to <span>$0=x_0&lt;x_1&lt;\ldots&lt;x_n=1$</span>, with <span>$n=100$</span>, <span>$x_{i+1}-x_i = h=\frac{1}{n}$</span>.</p><p>we can solve the problem with the following code snippet</p><pre><code class="language-julia">using ADCME
n = 100
h = 1/n
X0 = Variable(10.0)
A = X0 * diagm(0=&gt;2/h^2*ones(n-1), 1=&gt;-1/h^2*ones(n-2), -1=&gt;-1/h^2*ones(n-2)) # coefficient matrix for the finite difference
φ = 2.0*ones(n-1) # right hand side
u = A\φ
loss = (u[50] - 0.25)^2

sess = Session(); init(sess)
BFGS!(sess, loss)</code></pre><p>After around 7 iterations, the estimated <span>$X_0$</span> converges to 1.0000000016917243. </p><h2 id="Function-Inverse-Problem"><a class="docs-heading-anchor" href="#Function-Inverse-Problem">Function Inverse Problem</a><a id="Function-Inverse-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Function-Inverse-Problem" title="Permalink"></a></h2><p>When <span>$X$</span> is a function that does not depend on <span>$u$</span>, i.e., a function of location <span>$x$</span>, we call this type of problem <strong>function inverse problem</strong>. A common approach to this type of problem is to approximate the unknown function <span>$X$</span> with a parametrized form, such as piecewise linear functions, radial basis functions or Chebyshev polynomials; sometimes we can also discretize <span>$X$</span> and substitute <span>$X$</span> by a vector of its values at the discrete grid nodes. </p><p>This tutorial is not aimed at the comparison of different methods. Instead, we show how we can use neural networks to represent <span>$X$</span> and train the neural network by coupling it with numerical schemes. The gradient calculation can be laborious with the traditional adjoint state methods but is trivial with automatic differentiation. </p><p>Let&#39;s assume the true <span>$X$</span> has the following form</p><p class="math-container">\[X(x) = \frac{1}{1+x^2}\]</p><p>The exact <span>$\varphi$</span> is given by </p><p class="math-container">\[\varphi(x) = \frac{2 \left(x^{2} - x \left(2 x - 1\right) + 1\right)}{\left(x^{2} + 1\right)^{2}}\]</p><p>The idea is to use a neural network <span>$\mathcal{N}(x;w)$</span> with weights and biases <span>$w$</span> that maps the location <span>$x\in \mathbb{R}$</span> to a scalar value such that</p><p class="math-container">\[\mathcal{N}(x; w)\approx X(x)\]</p><p>To find the optional <span>$w$</span>, we solve the Poisson equation with <span>$X(x)=\mathcal{N}(x;w)$</span>, where the numerical scheme is </p><p class="math-container">\[\left( -\frac{X_i+X_{i+1}}{2} \right) u_{i+1} + \frac{X_{i-1}+2X_i+X_{i+1}}{2} u_i + \left( -\frac{X_i+X_{i-1}}{2} \right) = \varphi(x_i) h^2\]</p><p>Here <span>$X_i = \mathcal{N}(x_i; w)$</span>. </p><p>Assume we can observe the full solution <span>$u(x)$</span>, we can compare it with the solution <span>$u(x;w)$</span>, and minimize the loss function </p><p class="math-container">\[L(w) = \sum_{i=2}^{n-1} (u(x_i;w)-u(x_i))^2\]</p><pre><code class="language-julia">using ADCME
n = 100
h = 1/n
x = collect(LinRange(0, 1.0, n+1))
X = ae(x, [20,20,20,1])^2  # to ensure that X is positive, we use NN^2 instead of NN
A = spdiag(
  n-1,
  1=&gt;-(X[2:end-2] + X[3:end-1])/2,
  -1=&gt;-(X[3:end-1] + X[2:end-2])/2,
  0=&gt;(2*X[2:end-1]+X[3:end]+X[1:end-2])/2
)/h^2
φ = @. 2*x*(1 - 2*x)/(x^2 + 1)^2 + 2 /(x^2 + 1)
u = A\φ[2:end-1] # for efficiency, we can use A\φ[2:end-1] (sparse solver)
u_obs = (@. x * (1-x))[2:end-1]
loss = sum((u - u_obs)^2)

sess = Session(); init(sess)
BFGS!(sess, loss)</code></pre><p>We show the exact <span>$X(x)$</span> and the pointwise error in the following plots</p><p class="math-container">\[\left|\mathcal{N}(x_i;w)-X(x_i)\right|\]</p><table><tr><th style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/errorX.png?raw=true" alt="errorX"/></th><th style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/exactX.png?raw=true" alt="exactX"/></th></tr><tr><td style="text-align: right">Pointwise Absolute Error</td><td style="text-align: right">Exact <span>$X(u)$</span></td></tr></table><h2 id="Functional-Inverse-Problem"><a class="docs-heading-anchor" href="#Functional-Inverse-Problem">Functional Inverse Problem</a><a id="Functional-Inverse-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Functional-Inverse-Problem" title="Permalink"></a></h2><p>In the <strong>functional inverse problem</strong>, <span>$X$</span> is a function that <em>depends</em> on <span>$u$</span> (or both <span>$x$</span> and <span>$u$</span>); it must not be confused with the functional inverse problem and it is much harder to solve (since the equation is nonlinear). </p><p>As an example, assume </p><p class="math-container">\[\begin{aligned}\nabla (X(u)\nabla u(x)) &amp;= \varphi(x) &amp; x\in (0,1)\\ u(0)=u(1) &amp;= 0\end{aligned}\]</p><p>where the quantity of interest is </p><p class="math-container">\[X(u) = \frac{1}{1+100u^2}\]</p><p>The corresponding <span>$\varphi$</span> can be analytically evaluated (e.g., using SymPy).</p><p>To solve the Poisson equation, we use the standard Newton-Raphson scheme, in which case, we need to compute the residual</p><p class="math-container">\[R_i = X&#39;(u_i)\left(\frac{u_{i+1}-u_{i-1}}{2h}\right)^2 + X(u_i)\frac{u_{i+1}+u_{i-1}-2u_i}{h^2} - \varphi(x_i)\]</p><p>and the corresponing Jacobian</p><p class="math-container">\[\frac{\partial R_i}{\partial u_j} = \left\{ \begin{matrix}  -\frac{X&#39;(u_i)}{h}\frac{u_{i+1}-u_{i-1}}{2h} + \frac{X(u_i)}{h^2} &amp; j=i-1\\ X&#39;&#39;(u_i)\frac{u_{i+1}-u_{i-1}}{2h} + X&#39;(u_i)\frac{u_{i+1}+u_{i-1}-2u_i}{h^2} - \frac{2}{h^2}X(u_i) &amp; j=i \\ \frac{X&#39;(u_i)}{2h}\frac{u_{i+1}-u_{i-1}}{2h} + \frac{X(u_i)}{h^2} &amp; j=i+1\\ 0 &amp; |j-i|&gt;1  \end{matrix} \right.\]</p><p>Just like the function inverse problem, we also use a neural network to approximate <span>$X(u)$</span>; the difference is that the input of the neural network is <span>$u$</span> instead of <span>$x$</span>. It is convenient to compute <span>$X&#39;(u)$</span> with automatic differentiation.</p><p>Solving the forward problem (given <span>$X(u)$</span>, solve for <span>$u$</span>) requires conducting Newton-Raphson iterations. One challenge here is that the Newton-Raphson operator is a nonlinear implicit operator that does not fall into the types of operators where automatic differentiation applies. The relevant technique is <strong>physics constrained learning</strong>. The basic idea is to extract the gradients by the implicit function theorem. The limitation is that we need to provide the Jacobian matrix for the residual term in the Newton-Raphson algorithm. In ADCME, the complex algorithm is wrapped in the API <a href="../api/#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyCall.PyObject},Union{PyCall.PyObject, Array{Float64,N} where N}}} where T&lt;:Real"><code>NonlinearConstrainedProblem</code></a> and users only need to focus on constructing the residual and the gradient term</p><pre><code class="language-julia">using ADCME 
using PyPlot

function residual_and_jacobian(θ, u)
    X = ae(u, config, θ) + 1.0     # (1)
    Xp = tf.gradients(X, u)[1]
    Xpp = tf.gradients(Xp, u)[1]
    up = [u[2:end];constant(zeros(1))]
    un = [constant(zeros(1)); u[1:end-1]]
    R = Xp .* ((up-un)/2h)^2 + X .* (up+un-2u)/h^2 - φ
    dRdu = Xpp .* ((up-un)/2h)^2 + Xp.*(up+un-2u)/h^2 - 2/h^2*X 
    dRdun = -Xp[2:end]/h .* (up-un)[2:end]/2h + X[2:end]/h^2
    dRdup = Xp[1:end-1]/h .* (up-un)[1:end-1]/2h + X[1:end-1]/h^2
    J = spdiag(n-1, 
        -1=&gt;dRdun,
        0=&gt;dRdu,
        1=&gt;dRdup)     # (2)
    return R, J
end


config = [20,20,20,1]
n = 100
h = 1/n
x = collect(LinRange(0, 1.0, n+1))

φ = @. (1 - 2*x)*(-100*x^2*(2*x - 2) - 200*x*(1 - x)^2)/(100*x^2*(1 - x)^2 + 1)^2 - 2 - 2/(100*x^2*(1 - x)^2 + 1)
φ = φ[2:end-1]
θ = Variable(ae_init([1,config...]))
u0 = constant(zeros(n-1)) 
function L(u)    # (3)
  u_obs = (@. x * (1-x))[2:end-1]
  loss = mean((u - u_obs)^2) 
end
loss, solution, grad = NonlinearConstrainedProblem(residual_and_jacobian, L, θ, u0)
X_pred = ae(collect(LinRange(0.0,0.25,100)), config, θ) + 1.0

sess = Session(); init(sess)
BFGS!(sess, loss, grad, θ)
x_pred, sol = run(sess, [X_pred, solution])

figure(figsize=(10,4))
subplot(121)
s = LinRange(0.0,0.25,100)
x_exact = @. 1/(1+100*s^2) + 1
plot(s, x_exact, &quot;-&quot;, linewidth=3, label=&quot;Exact&quot;)
plot(s, x_pred, &quot;o&quot;, markersize=2, label=&quot;Estimated&quot;)
legend()
xlabel(&quot;u&quot;)
ylabel(&quot;X(u)&quot;)

subplot(122)
s = LinRange(0.0,1.0,101)[2:end-1]
plot(s, (@. s * (1-s)), &quot;-&quot;, linewidth=3, label=&quot;Exact&quot;)
plot(s, sol, &quot;o&quot;, markersize=2, label=&quot;Estimated&quot;)
legend()
xlabel(&quot;x&quot;)
ylabel(&quot;u&quot;)
savefig(&quot;nn.png&quot;)</code></pre><p>Detailed explaination: (1) This is the neural network we constructed. Note that with default initialization, the neural network output values are close to 0, and thus poses numerical stability issue for the solver. We can shift the neural network value by <span>$+1$</span> (equivalently, we use 1 for the initial guess of the last bias term); (2) The jacobian matrix is sparse, and thus we use <a href="../api/#ADCME.spdiag-Tuple{Int64}"><code>spdiag</code></a> to create a sparse matrix; (3) A loss function is formulated and minimized in the physics constrained learning. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/nn.png?raw=true" alt="nn"/></p><h2 id="Stochastic-Inverse-Problem"><a class="docs-heading-anchor" href="#Stochastic-Inverse-Problem">Stochastic Inverse Problem</a><a id="Stochastic-Inverse-Problem-1"></a><a class="docs-heading-anchor-permalink" href="#Stochastic-Inverse-Problem" title="Permalink"></a></h2><p>The final type of inverse problem is called <strong>stochastic inverse problem</strong>. In this problem, <span>$X$</span> is a random variable with unknown distribution. Consequently, the solution <span>$u$</span> will also be a random variable. For example, we may have the following settings in practice</p><ul><li>The measurement of <span>$u(0.5)$</span> may not be accurate. We might assume that <span>$u(0.5) \sim \mathcal{N}(\hat u(0.5), \sigma^2)$</span> where <span>$\hat u(0.5)$</span> is one observation and <span>$\sigma$</span> is the prescribed standard deviation of the measurement. Thus, we want to estimate the distribution of <span>$X$</span> which will produce the same distribution for <span>$u(0.5)$</span>. This type of problem falls under the umbrella of <strong>uncertainty quantification</strong>. </li><li>The quantity <span>$X$</span> itself is subject to randomness in nature, but its distribution may be positively/negatively skewed (e.g., the stock price returns). We can measure several samples of <span>$u(0.5)$</span> and want to estimate the distribution of <span>$X$</span> based on the samples. This problem is also called the <strong>probabilistic inverse problem</strong>. </li></ul><p>We cannot simply minimize the distance between <span>$u(0.5)$</span> and <code>u</code>   (which are random variables) as usual; instead, we need a metric to measure the discrepancy between two distributions–<code>u</code> and <span>$u(0.5)$</span>. The observables <span>$u(0.5)$</span> may be given in multiple forms</p><ul><li>The probability density function. </li><li>The unnormalized log-likelihood function. </li><li>Discrete samples. </li></ul><p>We consider the third type in this tutorial. The idea is to construct a sampler for <span>$X$</span> with a neural network and find the optimal weights and biases by minimizing the discrepancy between actually observed samples  and produced ones. Here is how we train the neural network:</p><p>We first propose a candidate neural network that transforms a sample from <span>$\mathcal{N}(0, I_d)$</span> to a sample from <span>$X$</span>. Then we randomly generate <span>$K$</span> samples <span>$\{z_i\}_{i=1}^K$</span> from <span>$\mathcal{N}(0, I_d)$</span> and transform them to <span>$\{X_i; w\}_{i=1}^K$</span>. We solve the Poisson equation <span>$K$</span> times to obtain <span>$\{u(0.5;z_i, w)\}_{i=1}^K$</span>. Meanwhile, we sample <span>$K$</span> items from the observations (e.g., with the bootstrap method) <span>$\{u_i(0.5)\}_{i=1}^K$</span>. We can use a probability metric <span>$D$</span> to measure the discrepancy between <span>$\{u(0.5;z_i, w)\}_{i=1}^K$</span> and <span>$\{u_i(0.5)\}_{i=1}^K$</span>. There are many choices for <span>$D$</span>, such as (they are not necessarily non-overlapped)</p><ul><li>Wasserstein distance (from optimal transport)</li><li>KL-divergence, JS-divergence, etc. </li><li>Discriminator neural networks (from generative adversarial nets)</li></ul><p>For example, we can consider the first approach, and invoke <a href="../api/#ADCME.sinkhorn-Tuple{Union{PyCall.PyObject, Array{Float64,N} where N},Union{PyCall.PyObject, Array{Float64,N} where N},Union{Array{Float64,2}, PyCall.PyObject}}"><code>sinkhorn</code></a> provided by ADCME</p><pre><code class="language-julia">using ADCME
using Distributions

# we add a mixture Gaussian noise to the observation
m = MixtureModel(Normal[
   Normal(0.3, 0.1),
   Normal(0.0, 0.1)], [0.5, 0.5])

function solver(a)
  n = 100
  h = 1/n
  A = a[1] * diagm(0=&gt;2/h^2*ones(n-1), 1=&gt;-1/h^2*ones(n-2), -1=&gt;-1/h^2*ones(n-2)) 
  φ = 2.0*ones(n-1) # right hand side
  u = A\φ
  u[50]
end

batch_size = 64
x = placeholder(Float64, shape=[batch_size,10])
z = placeholder(Float64, shape=[batch_size,1])
dat = z + 0.25
fdat  = reshape(map(solver, ae(x, [20,20,20,1])+1.0), batch_size, 1)
loss = empirical_sinkhorn(fdat, dat, dist=(x,y)-&gt;dist(x,y,2), method=&quot;lp&quot;)
opt = AdamOptimizer(0.01, beta1=0.5).minimize(loss)

sess = Session(); init(sess)
for i = 1:100000
  run(sess, opt, feed_dict=Dict(
        x=&gt;randn(batch_size, 10),
        z=&gt;rand(m, batch_size,1)
      ))
end</code></pre><table><tr><th style="text-align: right">Loss Function</th><th style="text-align: right">Iteration 5000</th><th style="text-align: right">Iteration 15000</th><th style="text-align: right">Iteration 25000</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/loss.png?raw=true" alt="loss"/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/test5000.png?raw=true" alt="test5000"/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/test15000.png?raw=true" alt="test15000"/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/test25000.png?raw=true" alt="test25000"/></td></tr></table></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tu_fem/">« Numerical Scheme in ADCME: Finite Element Example</a><a class="docs-footer-nextpage" href="../tu_recipe/">Inverse Modeling Recipe »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 23 November 2020 21:23">Monday 23 November 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
