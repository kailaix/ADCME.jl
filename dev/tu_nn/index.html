<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Combining Neural Networks with Numerical Schemes · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li class="is-active"><a class="tocitem" href>Combining Neural Networks with Numerical Schemes</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Residual-Minimization-for-Full-Field-Data"><span>Residual Minimization for Full Field Data</span></a></li><li><a class="tocitem" href="#Penalty-Method-for-Sparse-Observations"><span>Penalty Method for Sparse Observations</span></a></li><li><a class="tocitem" href="#Physics-Constrained-Learning"><span>Physics Constrained Learning</span></a></li><li><a class="tocitem" href="#Summary"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>Combining Neural Networks with Numerical Schemes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Combining Neural Networks with Numerical Schemes</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/tu_nn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Combining-Neural-Networks-with-Numerical-Schemes"><a class="docs-heading-anchor" href="#Combining-Neural-Networks-with-Numerical-Schemes">Combining Neural Networks with Numerical Schemes</a><a id="Combining-Neural-Networks-with-Numerical-Schemes-1"></a><a class="docs-heading-anchor-permalink" href="#Combining-Neural-Networks-with-Numerical-Schemes" title="Permalink"></a></h1><p>Modeling unknown componenets in a physical models using a neural networks is an important method for function inverse problem. This approach includes a wide variety of applications, including</p><ul><li>Koopman operator in dynamical systems</li><li>Constitutive relations in solid mechanics.</li><li>Turbulent closure relations in fluid mechanics.</li><li>...... </li></ul><p>To use the known physics to the largest extent, we couple the neural networks and numerical schemes (e.g., finite difference, finite element, or finite volumn method) for partial differential equations. Based on the nature of the observation data, we present three methods to train the neural networks using gradient-based optimization method: the residual minimization method, the penalty method, and the physics constrained learning. We discuss the pros and cons for each method and show how the gradients can be computed using automatic differentiation in ADCME. </p><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>Many science and engineering problems use models to describe the physical processes. These physical processes are usually derived from first principles or based on empirical relations. Neural networks have shown to be effective to model complex and high dimensional functions, and can be used to model the unknown mapping within a physical model. The known part of the model, such as conservation laws and boundary conditions, are preserved to obey the physics. Basically, we substutite unknown part of the model using neural networks in a physical system. As an example, consider a simple 1D heat equation</p><p class="math-container">\[\begin{aligned}
\frac{\partial}{\partial x}\left(\kappa(u)\frac{\partial u}{\partial x}\right) &amp;= f(x)&amp; x\in \Omega\\
u(0) = u(1) &amp;= 0
\end{aligned}\]</p><p>The diffusivity coefficient <span>$\kappa(u)$</span> is a function of the temperature <span>$u$</span>, and is an unknown function to be calibrated. We consider a data-driven approach to discover <span>$\kappa(u)$</span>  using only the temperature data set <span>$\mathcal{T}$</span>. In the case where we do not know the constitutive relation, it can be modeled using a neural network  <span>$\kappa_\theta (u)$</span>, where <span>$\theta$</span> is the weights and biases of the neural network. </p><p>In the following, we will discuss three methods based on the nature of the observation data set <span>$\mathcal{T}$</span> to train the neural network. </p><h2 id="Residual-Minimization-for-Full-Field-Data"><a class="docs-heading-anchor" href="#Residual-Minimization-for-Full-Field-Data">Residual Minimization for Full Field Data</a><a id="Residual-Minimization-for-Full-Field-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Residual-Minimization-for-Full-Field-Data" title="Permalink"></a></h2><p>In the case <span>$\mathcal{T}$</span> consist of full-field data, i.e., the values of <span>$u(x)$</span> on a very fine grid, we can use the <strong>residual minimization</strong> to learn the neural network. Specifically, we can discretize the PDE using a numerical scheme, such as finite element method (FEM), and obtain the residual term </p><p class="math-container">\[R_j(\theta) =\sum_i u(x_i)\int_0^1  \kappa_\theta(\sum_i c_i \phi_i) \phi_i&#39;(x) \phi_j&#39;(x) dx - \int_0^1 f(x)\phi_j(x)dx\]</p><p>where <span>$\phi_i$</span> are the basis functions in FEM. To find the optimal values for <span>$\theta$</span>, we can perform a residual minimization </p><p class="math-container">\[\min_\theta \sum_j R_j(\theta)^2\]</p><p>The residual minimization method avoids solving the PDE system, which can be expensive. The implication is straightforward using ADCME: all we need is to evaluate <span>$R_j(\theta)$</span> and <span>$\frac{\partial R_j(\theta)}{\partial \theta}$</span> (using automatic differention). However, the limitation is that this method is only applicable when full-field data are available. </p><p>In the following two references, we explore the applications of the residual minimization method to constitutive modeling</p><p>DZ Huang, K Xu, C Farhat, E Darve. <a href="https://arxiv.org/pdf/1905.12530.pdf">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></p><p>K Xu, DZ Huang, E Darve. <a href="https://arxiv.org/pdf/2004.00265.pdf">Learning Constitutive Relations using Symmetric Positive Definite Neural Networks</a></p><h2 id="Penalty-Method-for-Sparse-Observations"><a class="docs-heading-anchor" href="#Penalty-Method-for-Sparse-Observations">Penalty Method for Sparse Observations</a><a id="Penalty-Method-for-Sparse-Observations-1"></a><a class="docs-heading-anchor-permalink" href="#Penalty-Method-for-Sparse-Observations" title="Permalink"></a></h2><p>In the case where <span>$\mathcal{T}$</span> only consists of sparse observations, i.e., <span>$u_o(x_i)$</span> (observation of <span>$u(x_i)$</span>) at only a few locations <span>$\{x_i\}$</span>, we can formulate the problem as a PDE-constrained optimization problem</p><p class="math-container">\[\begin{aligned}
\min_\theta&amp; \;\sum_i (u(x_i) - u_o(x_i))^2\\
\text{s.t.} &amp;\; \frac{\partial}{\partial x}\left(\kappa_\theta(u)\frac{\partial u}{\partial x}\right) = f(x)&amp; x\in \Omega\\
&amp; u(0) = u(1) = 0
\end{aligned}\tag{1}\]</p><p>As pointed in <a href="https://kailaix.github.io/ADCME.jl/dev/tu_optimization/">this article</a>, we can apply the <strong>penalty method</strong> to solve the contrained optimization problem, </p><p class="math-container">\[\min_{\theta,u} \;\sum_i (u(x_i) - u_o(x_i))^2 + \rho \|F(u, \theta)\|^2_2\]</p><p>Here <span>$\rho$</span> is the penalty parameter and <span>$F(u, \theta)$</span> is the discretized form of the PDE. For example, <span>$F(u,\theta)_i$</span> can be <span>$R_i(\theta)$</span> in the residual minimization problem. </p><p>The penalty method is conceptually simple and easy to implement. Like the residual minimization method, the penalty method requires limited insights into the numerical simulator to evaluate the gradients with respect to <span>$u$</span> and <span>$\theta$</span>. The method does not require solving the PDE.</p><p>However, the penalty method treats <span>$u$</span> as optimization variable and therefore typically has much more degrees of freedom than the original constrained optimization problem. Mathematically, the penalty method suffers from worse conditioning than the constrained one, making it unfavorable in many scenarios. </p><h2 id="Physics-Constrained-Learning"><a class="docs-heading-anchor" href="#Physics-Constrained-Learning">Physics Constrained Learning</a><a id="Physics-Constrained-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Physics-Constrained-Learning" title="Permalink"></a></h2><p>An alternative approach to the penalty method in the context of sparse observations is the <strong>physics constrained learning</strong> (PCL). The physics constrained learning reduces Equation 1 to an unconstrained optimization problem by two steps:</p><ol><li>Solve for <span>$u$</span> from the PDE constraint, given a candidate neural network;</li><li>Plug the solution <span>$u(\theta)$</span> into the objective function and obtain a reduced loss function</li></ol><p>The reduced loss function only depends on <span>$\theta$</span> and therefore we can perform the unconstrained optimization, in which case a wide variety of off-the-shelf optimizers are available. The advantage of this approach is that PCL enforces the physics constraints, which can be crucial for some applications and are essential for numerical solvers. Additionally, PCL typically exhibits high efficiency and fast convergence compared to the other two methods. However, PCL requires deep insights into the numerical simulator since an analytical form of <span>$u(\theta)$</span> is not always tractable. It requires automatic differentiation through implict numerical solvers as well as iterative algorithms, which are usually not supported in AD frameworks. In the references below, we provide the key techniques for solving these problems. A final limitation of PCL is that it consumes much more memory and runtime per iteration than the other two methods. </p><p>For more details on PCL, and comparison between the penalty method and PCL, see the following reference</p><p>K Xu, E Darve. <a href="https://arxiv.org/pdf/2002.10521.pdf">Physics Constrained Learning for Data-driven Inverse Modeling from Sparse Observations</a></p><h2 id="Summary"><a class="docs-heading-anchor" href="#Summary">Summary</a><a id="Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Summary" title="Permalink"></a></h2><p>Using physics based machine learning  to solve inverse problems requires techniques from different areas, deep learning, automatic differentiation, numerical PDEs, and physical modeling. By a combination of the best from all worlds, we can leverage the power of modern high performance computing environments to solve long standing inverse problems in physical modeling. Specially, in this article we review three methods for training a neural network in a physical model using automatic differentiation. These methods can be readily implemented in ADCME. We conclude this article by a direct comparison of the three methods</p><table><tr><th style="text-align: right">Method</th><th style="text-align: right">Residual Minimization</th><th style="text-align: right">Penalty Method</th><th style="text-align: right">Physics Constrained Learning</th></tr><tr><td style="text-align: right">Sparse Observations</td><td style="text-align: right">❌</td><td style="text-align: right">✔️</td><td style="text-align: right">✔️</td></tr><tr><td style="text-align: right">Easy-to-implement</td><td style="text-align: right">✔️</td><td style="text-align: right">✔️</td><td style="text-align: right">❌</td></tr><tr><td style="text-align: right">Enforcing Physical Constraints</td><td style="text-align: right">❌</td><td style="text-align: right">❌</td><td style="text-align: right">✔️</td></tr><tr><td style="text-align: right">Fast Convergence</td><td style="text-align: right">❌</td><td style="text-align: right">❌</td><td style="text-align: right">✔️</td></tr><tr><td style="text-align: right">Minimal Optimization Variables</td><td style="text-align: right">✔️</td><td style="text-align: right">❌</td><td style="text-align: right">✔️</td></tr></table></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tu_recipe/">« Inverse Modeling Recipe</a><a class="docs-footer-nextpage" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 29 January 2021 02:10">Friday 29 January 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
