<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Custom Operators · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li class="is-active"><a class="tocitem" href>Custom Operators</a><ul class="internal"><li><a class="tocitem" href="#CPU-Operators-1"><span>CPU Operators</span></a></li><li><a class="tocitem" href="#GPU-Operators-1"><span>GPU Operators</span></a></li><li><a class="tocitem" href="#Miscellany-1"><span>Miscellany</span></a></li></ul></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../extra/">Miscellaneous Tools</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Custom Operators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Custom Operators</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/customop.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Custom-Operators-1"><a class="docs-heading-anchor" href="#Custom-Operators-1">Custom Operators</a><a class="docs-heading-anchor-permalink" href="#Custom-Operators-1" title="Permalink"></a></h1><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>As a reminder, there are many built-in custom operators in <code>deps/CustomOps</code> and they are good resources for understanding custom operators. The following is a step-by-step instruction on how custom operators are implemented. </p></div></div><h2 id="CPU-Operators-1"><a class="docs-heading-anchor" href="#CPU-Operators-1">CPU Operators</a><a class="docs-heading-anchor-permalink" href="#CPU-Operators-1" title="Permalink"></a></h2><p>Custom operators are ways to add missing features in ADCME. Typically users do not have to worry about custom operators. However, in the following situation custom opreators might be very useful</p><ul><li>Direct implementation in ADCME is inefficient (bottleneck). </li><li>There are legacy codes users want to reuse, such as GPU-accelerated codes. </li><li>Special acceleration techniques such as checkpointing scheme. </li></ul><p>In the following, we present an example of implementing a sparse solver for <span>$Au=b$</span> as a custom operator.</p><p><strong>Input</strong>: row vector <code>ii</code>, column vector<code>jj</code> and value vector <code>vv</code> for the sparse coefficient matrix <span>$A$</span>; row vector <code>kk</code> and value vector <code>ff</code> for the right hand side <span>$b$</span>; the coefficient matrix dimension is <span>$d\times d$</span></p><p><strong>Output</strong>: solution vector <span>$u\in \mathbb{R}^d$</span></p><p><strong>Step 1: Create and modify the template file</strong></p><p>The following command helps create the wrapper</p><pre><code class="language-julia">customop()</code></pre><p>There will be a <code>custom_op.txt</code> in the current directory. Modify the template file </p><pre><code class="language-txt">MySparseSolver
int32 ii(?)
int32 jj(?)
double vv(?)
int32 kk(?)
double ff(?)
int32 d()
double u(?) -&gt; output</code></pre><p>The first line is the name of the operator. It should always be in the camel case. </p><p>The 2nd to the 7th lines specify the input arguments, the signature is <code>type</code>+<code>variable name</code>+<code>shape</code>. For the shape, <code>()</code> corresponds to a scalar, <code>(?)</code> to a vector and <code>(?,?)</code> to a matrix. The variable names must be in <em>lower cases</em>. Additionally, the supported types are: <code>int32</code>, <code>int64</code>, <code>float</code>, <code>double</code>, <code>bool</code> and <code>string</code>. </p><p>The last line is the output, denoted by <code>-&gt; output</code> (do not forget the whitespace before and after <code>-&gt;</code>).  </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"></div></div><pre><code class="language-none">If there are non-real type outputs, the corresponding top gradients input to the gradient kernel should be removed.</code></pre><p><strong>Step 2: Implement the kernels</strong></p><p>Run <code>customop()</code> again and there will be <code>CMakeLists.txt</code>, <code>gradtest.jl</code>, <code>MySparseSolver.cpp</code> appearing in the current directory. <code>MySparseSolver.cpp</code> is the main wrapper for the codes and <code>gradtest.jl</code> is used for testing the operator and its gradients. <code>CMakeLists.txt</code> is the file for compilation. </p><p>Create a new file <code>MySparseSolver.h</code> and implement both the forward simulation and backward simulation (gradients)</p><pre><code class="language-cpp">#include &lt;eigen3/Eigen/Sparse&gt;
#include &lt;eigen3/Eigen/SparseLU&gt;
#include &lt;vector&gt;
#include &lt;iostream&gt;
using namespace std;
typedef Eigen::SparseMatrix&lt;double&gt; SpMat; // declares a column-major sparse matrix type of double
typedef Eigen::Triplet&lt;double&gt; T;

SpMat A;

void forward(double *u, const int *ii, const int *jj, const double *vv, int nv, const int *kk, const double *ff,int nf,  int d){
    vector&lt;T&gt; triplets;
    Eigen::VectorXd rhs(d); rhs.setZero();
    for(int i=0;i&lt;nv;i++){
      triplets.push_back(T(ii[i]-1,jj[i]-1,vv[i]));
    }
    for(int i=0;i&lt;nf;i++){
      rhs[kk[i]-1] += ff[i];
    }
    A.resize(d, d);
    A.setFromTriplets(triplets.begin(), triplets.end());
    auto C = Eigen::MatrixXd(A);
    Eigen::SparseLU&lt;SpMat&gt; solver;
    solver.analyzePattern(A);
    solver.factorize(A);
    auto x = solver.solve(rhs);
    for(int i=0;i&lt;d;i++) u[i] = x[i];
}

void backward(double *grad_vv, const double *grad_u, const int *ii, const int *jj, const double *u, int nv, int d){
    Eigen::VectorXd g(d);
    for(int i=0;i&lt;d;i++) g[i] = grad_u[i];
    auto B = A.transpose();
    Eigen::SparseLU&lt;SpMat&gt; solver;
    solver.analyzePattern(B);
    solver.factorize(B);
    auto x = solver.solve(g);
    // cout &lt;&lt; x &lt;&lt; endl;
    for(int i=0;i&lt;nv;i++) grad_vv[i] = 0.0;
    for(int i=0;i&lt;nv;i++){
      grad_vv[i] -= x[ii[i]-1]*u[jj[i]-1];
    }
}</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>In this implementation we have used <code>Eigen</code> library for solving sparse matrix. Other choices are also possible, such as algebraic multigrid methods. Note here for convenience we have created a global variable <code>SpMat A;</code>. This is not recommend if you want to run the code concurrently, since the variable <code>A</code> must be overwritten by another concurrent thread. </p></div></div><p><strong>Step 3: Compile</strong></p><p>It is recommended that you use the <code>cmake</code>, <code>make</code> and <code>gcc</code> provided by <code>ADCME</code>. The binary locations can be found via</p><table><tr><th style="text-align: right">Variable</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>ADCME.CXX</code></td><td style="text-align: right">C++ Compiler</td></tr><tr><td style="text-align: right"><code>ADCME.CC</code></td><td style="text-align: right">C Compiler</td></tr><tr><td style="text-align: right"><code>ADCME.TFLIB</code></td><td style="text-align: right"><code>libtensorflow_framework.so</code> location</td></tr><tr><td style="text-align: right"><code>ADCME.CMAKE</code></td><td style="text-align: right">Cmake binary location</td></tr><tr><td style="text-align: right"><code>ADCME.MAKE</code></td><td style="text-align: right">Make binary location</td></tr></table><ul><li>Make a <code>build</code> directory in bash.</li></ul><pre><code class="language-bash">mkdir build
cd build</code></pre><ul><li>Configure <code>CMakeLists.txt</code> files.</li></ul><pre><code class="language-bash">cmake ..</code></pre><p>or use a safer way </p><pre><code class="language-julia-repl">julia&gt; using ADCME
julia&gt; ADCME.cmake()</code></pre><p>This step requires <code>Conda</code> and <code>PyCall</code> be properly installed. Try the following if necessary</p><pre><code class="language-julia">julia&gt; pkg
pkg&gt; add Conda
pkg&gt; add PyCall</code></pre><ul><li>Build. </li></ul><pre><code class="language-bash">make -j</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>If the system <code>make</code> or <code>cmake</code> command is not compatible, try the pre-installed ADCME <code>make</code> located at <code>ADCME.MAKE</code> or <code>cmake</code> located at <code>ADCME.CMAKE</code>. </p></div></div><p>Based on your operation system, you will create <code>libMySparseSolver.{so,dylib,dll}</code>. This will be the dynamic library to link in <code>TensorFlow</code>. </p><p><strong>Step 4: Test</strong></p><p>Finally, you could use <code>gradtest.jl</code> to test the operator and its gradients (specify appropriate data in <code>gradtest.jl</code> first). If you implement the gradients correctly, you will be able to obtain first order convergence for finite difference and second order convergence for automatic differentiation. Note you need to modify this file first, e.g., creating data and modifying the function <code>scalar_function</code>. </p><p><img src="../assets/custom_op.png" alt="custom_op"/></p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>If the process fails, it is most probable the GCC compiler is not compatible with which was used to compile <code>libtensorflow_framework.{so,dylib}</code>. Using the built-in <code>cmake</code> and <code>make</code> will alleviate this problem in most cases. In the Linux system, you can check the compiler using </p></div></div><pre><code class="language-bash">readelf -p .comment libtensorflow_framework.so</code></pre><p>Compatibility issues are frustrating. We hope you can submit an issue to ADCME developers; we are happy to resolve the compatibility issue and improve the robustness of ADCME.</p><h2 id="GPU-Operators-1"><a class="docs-heading-anchor" href="#GPU-Operators-1">GPU Operators</a><a class="docs-heading-anchor-permalink" href="#GPU-Operators-1" title="Permalink"></a></h2><h3 id="Dependencies-1"><a class="docs-heading-anchor" href="#Dependencies-1">Dependencies</a><a class="docs-heading-anchor-permalink" href="#Dependencies-1" title="Permalink"></a></h3><p>To create a GPU custom operator, you must have an NVCC compiler and a CUDA toolkit installed on your system. To install NVCC, see <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">the installation guide</a>. To check you have successfully installed NVCC, type</p><pre><code class="language-bash">which nvcc</code></pre><p>It should gives you the location of <code>nvcc</code> compiler. </p><p>For quick installation of other dependencies, you can try</p><pre><code class="language-julia">ENV[&quot;GPU&quot;] = 1
Pkg.build(&quot;ADCME&quot;)</code></pre><h4 id="Manual-Installation-1"><a class="docs-heading-anchor" href="#Manual-Installation-1">Manual Installation</a><a class="docs-heading-anchor-permalink" href="#Manual-Installation-1" title="Permalink"></a></h4><p>In case </p><ul><li>To install CUDA toolkit (if you do not have one), you can install via conda</li></ul><pre><code class="language-julia">using Conda
Conda.add(&quot;cudatoolkit&quot;, channel=&quot;anaconda&quot;)</code></pre><ul><li>The next step is to cp the CUDA include file to tensorflow include directory. This could be done with </li></ul><pre><code class="language-julia">using ADCME
gpus = joinpath(splitdir(tf.__file__)[1], &quot;include/third_party/gpus&quot;)
if !isdir(gpus)
  mkdir(gpus)
end
gpus = joinpath(gpus, &quot;cuda&quot;)
if !isdir(gpus)
  mkdir(gpus)
end
incpath = joinpath(splitdir(strip(read(`which nvcc`, String)))[1], &quot;../include/&quot;)
if !isdir(joinpath(gpus, &quot;include&quot;))
    mv(incpath, gpus)
end</code></pre><ul><li>Finally, add the CUDA library path to <code>LD_LIBRARY_PATH</code>. This can be done by adding the following line to <code>.bashrc</code></li></ul><pre><code class="language-bash">export LD_LIBRARY_PATH=&lt;path&gt;:$LD_LIBRARY_PATH</code></pre><p>where <code>&lt;path&gt;</code> is </p><pre><code class="language-julia">joinpath(Conda.ROOTENV, &quot;pkgs/cudatoolkit-10.1.168-0/lib/&quot;)</code></pre><h3 id="File-Organization-1"><a class="docs-heading-anchor" href="#File-Organization-1">File Organization</a><a class="docs-heading-anchor-permalink" href="#File-Organization-1" title="Permalink"></a></h3><p>There should be three files in your source directories</p><ul><li><code>MyOp.cpp</code>: driver file</li><li><code>MyOp.cu</code>: GPU implementation</li><li><code>MyOp.h</code>: CPU implementation</li></ul><p>The first two files have been generated for you by <code>customop()</code>. The following are two important notes on the implementation.</p><ul><li>In <code>MyOp.cu</code>, the implementation usually has the structure</li></ul><pre><code class="language-c">namespace tensorflow{
  typedef Eigen::GpuDevice GPUDevice;

    __global__ void forward_(const int nthreads, double *out, const double *y, const double *H0, int n){
      for(int i : CudaGridRangeX(nthreads)) {
          // do something here
      }
    }

    void forwardGPU(double *out, const double *y, const double *H0, int n, const GPUDevice&amp; d){
      // forward_&lt;&lt;&lt;(n+255)/256, 256&gt;&gt;&gt;(out, y, H0, n);
      GpuLaunchConfig config = GetGpuLaunchConfig(n, d);
      TF_CHECK_OK(GpuLaunchKernel(
          forward_, config.block_count, config.thread_per_block, 0,
          d.stream(), config.virtual_thread_count, out, y, H0, n));
      }
}</code></pre><ul><li>In <code>MyOp.cpp</code>, the device information (<code>const GPUDevice&amp; d</code> above) is obtained with </li></ul><pre><code class="language-c">context-&gt;eigen_device&lt;GPUDevice&gt;()</code></pre><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p><code>for(int i : CudaGridRangeX(nthreads))</code> is interpreted as </p><pre><code class="language-c">for (int index = blockIdx.x * blockDim.x + threadIdx.x; index &lt; nthreads; index += blockDim.x * gridDim.x)</code></pre><p>and the kernel launch semantic is equivalent to </p><pre><code class="language-cuda">forward_&lt;&lt;&lt;config.block_count, config.thread_per_block, 0,
                            d.stream()&gt;&gt;&gt;(config.virtual_thread_count,
                                          			out, y, H0, n);</code></pre></div></div><h2 id="Miscellany-1"><a class="docs-heading-anchor" href="#Miscellany-1">Miscellany</a><a class="docs-heading-anchor-permalink" href="#Miscellany-1" title="Permalink"></a></h2><h3 id="Mutable-Inputs-1"><a class="docs-heading-anchor" href="#Mutable-Inputs-1">Mutable Inputs</a><a class="docs-heading-anchor-permalink" href="#Mutable-Inputs-1" title="Permalink"></a></h3><p>Sometimes we want to modify tensors in place. In this case we can use mutable inputs. Mutable inputs must be <a href="../api/#ADCME.Variable-Tuple{Any}"><code>Variable</code></a> and it must be forwarded to one of the output. We consider implement a <code>my_assign</code> operator, with signature</p><pre><code class="language-none">my_assign(u::PyObject, v::PyObject)::PyObject</code></pre><p>Here <code>u</code> is a <code>Variable</code> and we copy the data from <code>v</code> to <code>u</code>. In the <code>MyAssign.cpp</code> file, we modify the input and output specifications to </p><pre><code class="language-c">.Input(&quot;u : Ref(double)&quot;)
.Input(&quot;v : double&quot;)
.Output(&quot;w : Ref(double)&quot;)</code></pre><p>In addition, the input tensor is obtained through</p><pre><code class="language-c">Tensor u = context-&gt;mutable_input(0, true);</code></pre><p>The second argument <code>lock_held</code> specifies whether the input mutex is acquired (false) before the operation. Note the output must be a <code>Tensor</code> instead of a reference. </p><p>To forward the input, use</p><pre><code class="language-c">context-&gt;forward_ref_input_to_ref_output(0,0);</code></pre><p>We use the following code snippet to test the program</p><pre><code class="language-julia">my_assign = load_op(&quot;./build/libMyAssign&quot;,&quot;my_assign&quot;)
u = Variable([0.1,0.2,0.3])
v = constant(Array{Float64}(1:3))
u2 = u^2
w = my_assign(u,v)
sess = tf.Session()
init(sess)
@show run(sess, u)
@show run(sess, u2)
@show run(sess, w)
@show run(sess, u2)</code></pre><p>The output is </p><pre><code class="language-none">[0.1,0.2,0.3]
[0.1,0.04,0.09]
[1.0,2.0,3.0]
[1.0,4.0,9.0]</code></pre><p>We can see that the tensors depending on <code>u</code> are also aware of the assign operator. The complete programs can be downloaded here: <a href="https://kailaix.github.io/ADCME.jl/dev/codes/mutable/CMakeLists.txt">CMakeLists.txt</a>, <a href="https://kailaix.github.io/ADCME.jl/dev/codes/mutable/MyAssign.cpp">MyAssign.cpp</a>, <a href="https://kailaix.github.io/ADCME.jl/dev/codes/mutable/gradtest.jl">gradtest.jl</a>.</p><h3 id="Third-party-Plugins-1"><a class="docs-heading-anchor" href="#Third-party-Plugins-1">Third-party Plugins</a><a class="docs-heading-anchor-permalink" href="#Third-party-Plugins-1" title="Permalink"></a></h3><p>ADCME also allows third-party custom operators hosted on Github. To build your own custom operators, implement your own custom operators in a Github repository. The root directory of the repository should have the following files</p><ul><li><p><code>formula.txt</code>, which tells how ADCME should interact with the custom operator. It is a Julia Pair, which has the format</p><pre><code class="language-none">signature =&gt; (source_directory, library_name, signature, has_gradient)</code></pre><p>For example</p><pre><code class="language-julia">&quot;ot_network&quot;=&gt;(&quot;OTNetwork&quot;, &quot;libOTNetwork&quot;, &quot;ot_network&quot;, true)</code></pre></li><li><p><code>CMakeLists.txt</code>, which is used for compiling the library. </p></li></ul><p>Users are free to arrange other source files or other third-party libraries. </p><p>Upon using those libraries in ADCME, users first download those libraries to <code>deps</code> directory via</p><pre><code class="language-julia">install(&quot;https://github.com/ADCMEMarket/OTNetwork&quot;)</code></pre><p>The official plugins are hosted on <code>https://github.com/ADCMEMarket</code>. To get access to the custom operators in ADCME, use</p><pre><code class="language-julia">op = load_system_op(&quot;OTNetwork&quot;)</code></pre><ol><li>https://on-demand.gputechconf.com/ai-conference-2019/T1-3<em>Minseok%20Lee</em>Adding%20custom%20CUDA%20C++%20Operations%20in%20Tensorflow%20for%20boosting%20BERT%20Inference.pdf)</li></ol></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ode/">« PDE/ODE Solvers</a><a class="docs-footer-nextpage" href="../global/">Shared Memory Across Kernels »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 1 April 2020 08:29">Wednesday 1 April 2020</span>. Using Julia version 1.4.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
