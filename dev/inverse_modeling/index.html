<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Inverse Modeling · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li class="is-active"><a class="tocitem" href>Inverse Modeling</a><ul class="internal"><li><a class="tocitem" href="#Automatic-Differentiation-1"><span>Automatic Differentiation</span></a></li><li><a class="tocitem" href="#AD-Implementation-in-ADCME-1"><span>AD Implementation in ADCME</span></a></li><li><a class="tocitem" href="#Forward-Operator-Types-1"><span>Forward Operator Types</span></a></li><li><a class="tocitem" href="#Related-Algorithms-1"><span>Related Algorithms</span></a></li></ul></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../inverse_impl/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../array/">Tensor Operations</a></li><li><a class="tocitem" href="../sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../customop/">Custom Operators</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../while_loop/">While Loops</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../pytorchnn/">Neural Network in C++</a></li><li><a class="tocitem" href="../extra/">Miscellaneous Tools</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Inverse Modeling</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Inverse Modeling</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/inverse_modeling.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Inverse-Modeling-1"><a class="docs-heading-anchor" href="#Inverse-Modeling-1">Inverse Modeling</a><a class="docs-heading-anchor-permalink" href="#Inverse-Modeling-1" title="Permalink"></a></h1><p><strong>Inverse modeling</strong> (IM) identifies a certain set of parameters or functions with which the outputs of the <em>forward model</em> matches the desired result or measurement. IM can usually be solved by formulating it as an optimization problem. But the major difference is that IM aims at getting information not accessible to forward model, instead of obtaining an optimal value of a fixed objective function and set of constraints. In practice, the objective function and constraints can be adjusted and prior information of the unknown parameters or functions can be imposed in the form of regularizers to better reflect the physical laws. </p><p>The inverse modeling problem can be mathematically formulated as finding an unknown parameter <span>$X$</span> given input <span>$\theta = \hat \theta$</span> and output <span>$u = \hat u$</span> of a forward model</p><div>\[u = F(\theta, X)\]</div><p>Here <span>$\theta$</span> and <span>$u$</span> can be a sample from a stochastic process. The scope of inverse problems that can be tackled with ADCME is</p><ol><li>The forward model must be <em>differentiable</em>,  i.e., <span>$\frac{\partial F}{\partial X}$</span> and <span>$\frac{\partial F}{\partial \theta}$</span> exist. However, we do not require those gradients to be implemented by users; they can be computed with automatic differentiation in ADCME.</li><li>The forward model must be a <em>white-box</em> and implemented with ADCME. ADCME is not for inverse modeling of black-box models. </li></ol><p>One iterative process for estimating <span>$X$</span> works as follows: we start from an initial guess <span>$X = \hat X_0$</span>, assuming it is correct, and compute the predicted output <span>$\hat u_0$</span> with the forward modeling codes implemented in ADCME. Then, we measure the discrepancy between the predicted output <span>$\hat u_0$</span> and the actual observation <span>$\hat u$</span> and apply the regular gradient-based optimization method to find the optimal <span>$X$</span> that minimizes this discrepancy. The gradients are computed with automatic differentiation, adjoint state methods or both. </p><p>This conceptually simple approach can solve various types of inverse problems: either <span>$\theta$</span>, <span>$u$</span> are stochastic or deterministic and the unknown <span>$X$</span> can  be a value, function and even functionals. As an example, assume the forward model is Poisson equation <span>$\nabla \cdot (X\nabla u(\mathbf{x})) = 0$</span> with appropriate boundary condition, <span>$u(\mathbf{x})$</span> is the output (<span>$\mathbf{x}$</span> is the coordinate) , the following is four kinds of potential classes of problems solvable with ADCME</p><table><tr><th style="text-align: right"><strong>Inverse problem</strong></th><th style="text-align: right"><strong>Problem type</strong></th><th style="text-align: right"><strong>Approach</strong></th><th style="text-align: center"><strong>Reference</strong></th></tr><tr><td style="text-align: right"><span>$\nabla\cdot(c\nabla u) = 0$</span></td><td style="text-align: right">Parameter</td><td style="text-align: right">Adjoint State Method</td><td style="text-align: center"><a href="http://arxiv.org/abs/1912.07552">1</a> <a href="http://arxiv.org/abs/1912.07547">2</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(f(\mathbf{x})\nabla u) = 0$</span></td><td style="text-align: right">Function*</td><td style="text-align: right">DNN</td><td style="text-align: center"><a href="https://arxiv.org/abs/1901.07758">3</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(f(u)\nabla u) = 0$</span></td><td style="text-align: right">Functional**</td><td style="text-align: right">DNN Learning from indirect data</td><td style="text-align: center"><a href="https://arxiv.org/abs/1905.12530">4</a></td></tr><tr><td style="text-align: right"><span>$\nabla\cdot(\varpi\nabla u) = 0$</span></td><td style="text-align: right">Stochastic Inversion</td><td style="text-align: right">Adversarial Learning with GAN</td><td style="text-align: center"><a href="https://arxiv.org/abs/1910.06936">5</a></td></tr></table><p>To see how those problems can be solved with ADCME in practice, see this <a href="../inverse_impl/">tutorial</a>. </p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>(*) Arguments of <span>$f$</span> are independent of <span>$u$</span></p><p>(**) At least one arguments of <span>$f$</span> is dependent on <span>$u$</span>. </p></div></div><p><img src="../assets/im.png" alt/></p><h2 id="Automatic-Differentiation-1"><a class="docs-heading-anchor" href="#Automatic-Differentiation-1">Automatic Differentiation</a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation-1" title="Permalink"></a></h2><p>One powerful tool in inverse modeling is automatic differentiation (AD). Automatic differentiation is a general way to compute gradients based on the chain rule. By tracing the forward-pass computation, the gradient at the final step can propagate back to every operator and every parameter in a computational graph. </p><p>As an example, a neural network model mainly consists of a sequence of linear transforms and non-linear activation functions. The goal of the training process is to minimize the error between its prediction and the label of ground truth. Automatic differentiation is used to calculate the gradients of every variable by back-propagating the gradients from the loss function to the trainable parameters, i.e., the weights and biases of neural networks. The gradients are then used in a gradient-based optimizer such as gradient descent methods to update the parameters. </p><p>For another example, the physical forward simulation is similar to the neural network model in that they are both sequences of linear/non-linear transforms. One popular method in physical simulation, the FDTD (Finite-Difference Time-Domain) method, applies a finite difference operator to a consecutive time steps to solve time-dependent partial differential equations (PDEs). In seismic problems, we can specify parameters such as earthquake source functions and earth media properties to simulate the received seismic signals. In seismic inversion problems, those parameters are unknown and we can invert the underlining source characteristic and media property by minimizing the difference between the simulated seismic signals and the observed ones. In the framework of automatic differentiation, the gradients of the difference can be computed automatically and thus used in a gradient-based optimizer. </p><p><img src="../assets/compare-NN-PDE.png" alt/></p><h2 id="AD-Implementation-in-ADCME-1"><a class="docs-heading-anchor" href="#AD-Implementation-in-ADCME-1">AD Implementation in ADCME</a><a class="docs-heading-anchor-permalink" href="#AD-Implementation-in-ADCME-1" title="Permalink"></a></h2><p>ADCME uses TensorFlow as the backend for automatic differentiation. However, one major difference of ADCME compared with TensorFlow is that it provides a friendly syntax for scientific computing (essentially the same syntax as native Julia). This substantially reduces development time. In addition, ADCME augments TensorFlow libraries by adding missing features that are useful for scientific computing, such as sparse matrix solve, sparse least square, sparse assembling, etc. Additionally, Julia interfaces make it possible for directly implementing efficient numerical computation parts of the simulation (requires no automatic differentiation), for interacting with other languages (MATLAB, C/C++, R, etc.) and for built-in Julia parallelism. </p><p>As an example, we show how a convoluted acoustic wave equation simulation with PML boundary condition can be translated to Julia codes with AD feature very neatly. </p><p><img src="../assets/Julia.png" alt/></p><h2 id="Forward-Operator-Types-1"><a class="docs-heading-anchor" href="#Forward-Operator-Types-1">Forward Operator Types</a><a class="docs-heading-anchor-permalink" href="#Forward-Operator-Types-1" title="Permalink"></a></h2><p>All numerical simulations can be decomposed into operators that are chained together. These operators range from a simple arithmetic operation such as addition or multiplication, to more sophisticated computation such as solving a linear system. Automatic differentiation relies on the differentiation of those operators and integrates them with chain rules. Therefore, it is very important for us to study the basic types of existing operators. </p><p><img src="../assets/sim.png" alt="Operators"/></p><p>In this tutorial, a operator is defined as a numerical procedure that accepts a parameter called <strong>input</strong>, <span>$x$</span>, and turns out a parameter called <strong>ouput</strong>, <span>$y=f(x)$</span>. For reverse mode automatic differentiation, besides evaluating <span>$f(x)$</span>, we need also to compute <span>$\frac{\partial J}{\partial x}$</span> given <span>$\frac{\partial J}{\partial y}$</span> where <span>$J$</span> is a functional of <span>$y$</span>. </p><p>Note  the operator <span>$y=f(x)$</span> may be implicit in the sense that <span>$f$</span> is not given directly. In general, we can write the relationship between <span>$x$</span> and <span>$y$</span> as <span>$F(x,y)=0$</span>. The operator is <strong>well-defined</strong> if for given <span>$x$</span>, there exists one and only one <span>$y$</span> such that <span>$F(x,y)=0$</span>. </p><p>For automatic differentiation, besides the well-definedness of <span>$F$</span>, we also require that we can compute <span>$\frac{\partial J}{\partial x}$</span> given <span>$\frac{\partial J}{\partial y}$</span>. It is easy to see that</p><div>\[\frac{\partial J}{\partial x} = -\frac{\partial J}{\partial y}F_y^{-1}F_x\]</div><p>Therefore, we call an operator <span>$F$</span> is <strong>well-posed</strong> if <span>$F_y^{-1}$</span> exists. </p><p>All operators can be classified into four types based on the linearity and explicitness.</p><p><strong>Linear and explicit</strong></p><p>This type of operators has the form </p><div>\[y = Ax\]</div><p>where <span>$A$</span> is a matrix. In this case, </p><div>\[F(x,y) = Ax-y\]</div><p>and therefore </p><div>\[\frac{\partial J}{\partial x} = \frac{\partial J}{\partial y}A\]</div><p>In Tensorflow, such an operator can be implemented as (assuming <code>A</code> is )</p><pre><code class="language-python">import tensorflow as tf
@tf.custom_gradient
def F(x):
      u = tf.linalg.matvec(A, x)
      def grad(dy):
          return tf.linalg.matvec(tf.transpose(A), dy)
      return u, grad</code></pre><p><strong>Nonlinear and explicit</strong></p><p>In this case, we have </p><div>\[y = F(x)\]</div><p>where <span>$F$</span> is explicitly given. We have</p><div>\[F(x,y) = F(x)-y\Rightarrow \frac{\partial J}{\partial x} = \frac{\partial J}{\partial y} F_x(x)\]</div><p>One challenge here is we need to implement the matrix vector production <span>$\frac{\partial J}{\partial y} F_x(x)$</span> for <code>grad</code>. </p><p><strong>Linear and implicit</strong></p><p>In this case </p><div>\[Ay = x\]</div><p>We have <span>$F(x,y) = x-Ay$</span> and </p><div>\[\frac{\partial J}{\partial x} = \frac{\partial J}{\partial y}A^{-1}\]</div><p><strong>Nonlinear and implicit</strong></p><p>In this case <span>$F(x,y)=0$</span> and the corresponding gradient is </p><div>\[\frac{\partial J}{\partial x} = -\frac{\partial J}{\partial y}F_y^{-1}F_x\]</div><p>This case is the most challenging of the four but widely seen in scientific computing code. In many numerical simulation code, <span>$F_y$</span> is usually sparse and therefore it is rewarding to exploit the sparse structure for computation acceleration in practice.</p><h2 id="Related-Algorithms-1"><a class="docs-heading-anchor" href="#Related-Algorithms-1">Related Algorithms</a><a class="docs-heading-anchor-permalink" href="#Related-Algorithms-1" title="Permalink"></a></h2><h3 id="EM-Algorithm-1"><a class="docs-heading-anchor" href="#EM-Algorithm-1">EM Algorithm</a><a class="docs-heading-anchor-permalink" href="#EM-Algorithm-1" title="Permalink"></a></h3><p>We now split the parameter <span>$\theta=(\theta_1, \theta_2)$</span>, i.e., the inverse model is</p><div>\[y = F(x, (\theta_1, \theta_2))\]</div><p>The expectation maximization algorithm alternates between the steps of guessing a probability distribution of a certain unknown <span>$\theta_1$</span> (E-Step) given current <span>$\theta_2$</span>  and then re-estimating the other unknown <span>$\theta_2$</span> (M-Step) assuming the guess <span>$\theta_1$</span> is true. The name &quot;expectation&quot; comes from the fact that usually the optimal guess is the expectation over a probability distribution. The algorithm is as follows</p><div>\[\begin{aligned}
\theta_2^{(0)} &amp;= \arg\min_{\theta_2} D_1(\hat y, F(\hat x; \theta_1^{(0)}, \theta_2))\\
\theta_1^{(1)} &amp;= \arg\min_{\theta_1} D_2(\hat y, F(\hat x; \theta_1, \theta_2^{(0)}))\\
\theta_2^{(1)} &amp;= \arg\min_{\theta_2} D_1(\hat y, F(\hat x; \theta_1^{(1)}, \theta_2))\\
\theta_1^{(2)} &amp;= \arg\min_{\theta_1} D_1(\hat y, F(\hat x; \theta_1, \theta_2^{(1)}))\\
\ldots
\end{aligned}\]</div><p>Recall that <span>$\hat x$</span> and <span>$\hat y$</span> represent observations (can be stochastic). <span>$D_1$</span> and <span>$D_2$</span> are discrepancy metrics, such as KL divergence. We immediately recognize EM algorithm as a coordinate descent method when <span>$D_1=D_2$</span>. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Overview</a><a class="docs-footer-nextpage" href="../inverse_impl/">Inverse Modeling with ADCME »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 9 February 2020 01:57">Sunday 9 February 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
