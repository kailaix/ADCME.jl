<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Inverse Modeling Recipe · ADCME</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">ADCME</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li class="is-active"><a class="tocitem" href>Inverse Modeling Recipe</a><ul class="internal"><li><a class="tocitem" href="#Forward-Modeling"><span>Forward Modeling</span></a></li><li><a class="tocitem" href="#Inverse-Modeling"><span>Inverse Modeling</span></a></li><li><a class="tocitem" href="#Debugging"><span>Debugging</span></a></li></ul></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li><li><a class="tocitem" href="../sqlite3/">Introducing ADCME Database and SQL Integration: an Efficient Approach to Simulation Data Management</a></li><li><a class="tocitem" href="../hessian/">The Mathematical Structure of DNN Hessians</a></li><li><a class="tocitem" href="../plotly/">Visualization with Plotly</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li><li><a class="tocitem" href="../reinforcement_learning/">Reinforcement Learning Basics: Q-learning and SARSA</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li><li><a class="tocitem" href="../windows_installation/">Install ADCME on Windows</a></li><li><a class="tocitem" href="../docker/">Install ADCME Docker Image</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>Inverse Modeling Recipe</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Inverse Modeling Recipe</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/tu_recipe.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Inverse-Modeling-Recipe"><a class="docs-heading-anchor" href="#Inverse-Modeling-Recipe">Inverse Modeling Recipe</a><a id="Inverse-Modeling-Recipe-1"></a><a class="docs-heading-anchor-permalink" href="#Inverse-Modeling-Recipe" title="Permalink"></a></h1><p>Here is a tip for inverse modeling using ADCME. </p><h2 id="Forward-Modeling"><a class="docs-heading-anchor" href="#Forward-Modeling">Forward Modeling</a><a id="Forward-Modeling-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-Modeling" title="Permalink"></a></h2><p>The first step is to implement your forward computation in ADCME. Let&#39;s consider a simple example. Assume that we want to compute a transformation from <span>$\{x_1,x_2, \ldots, x_n\}$</span> to <span>$\{f_\theta(x_1), f_\theta(x_2), \ldots, f_\theta(x_n)\}$</span>, where </p><p class="math-container">\[f_\theta(x) = a_2\sigma(a_1x+b_1)+b_2\quad \theta=(a_1,b_2,a_2,b_2)\]</p><p>The value <span>$\theta=(1,2,3,4)$</span>. We can code the forward computation as follows</p><pre><code class="language-julia">using ADCME
θ = constant([1.;2.;3.;4.])
x = collect(LinRange(0.0,1.0,10))
f = θ[3]*sigmoid(θ[1]*x+θ[2])+θ[4]

sess = Session(); init(sess)
f0 = run(sess, f)</code></pre><p>We obtained</p><pre><code class="language-text">10-element Array{Float64,1}:
 6.6423912339336475
 6.675935315969742
 6.706682200447601
 6.734800968378825
 6.7604627001561575
 6.783837569144308
 6.805092492614008
 6.824389291376896
 6.841883301751329
 6.8577223804673</code></pre><h2 id="Inverse-Modeling"><a class="docs-heading-anchor" href="#Inverse-Modeling">Inverse Modeling</a><a id="Inverse-Modeling-1"></a><a class="docs-heading-anchor-permalink" href="#Inverse-Modeling" title="Permalink"></a></h2><p>Assume that we want to estimate the target variable <span>$\theta$</span> from observations <span>$\{f_\theta(x_1), f_\theta(x_2), \ldots, f_\theta(x_n)\}$</span>. The inverse modeling is split into 6 steps. Follow the steps one by one</p><ul><li><p><strong>Step 1: Mark the target variable as <code>placeholder</code></strong>. That is, we replace <code>θ = constant([1.;2.;3.;4.])</code> by <code>θ = placeholder([1.;2.;3.;4.])</code>.</p></li><li><p><strong>Step 2: Check that the loss is zero given true values.</strong> The loss function is usually formulated so that it equals zero when we plug the true value to the target variable. </p><p>You should expect <code>0.0</code> using the following codes. </p></li></ul><pre><code class="language-julia">using ADCME
θ = placeholder([1.;2.;3.;4.])
x = collect(LinRange(0.0,1.0,10))
f = θ[3]*sigmoid(θ[1]*x+θ[2])+θ[4]
loss = sum((f - f0)^2)
sess = Session(); init(sess)
@show run(sess, loss)</code></pre><ul><li><strong>Step 3: Use <code>lineview</code> to visualize the landscape</strong>. Assume the initial guess is <span>$\theta_0$</span>, we can use the <code>lineview</code> function from <a href="https://github.com/kailaix/ADCMEKit.jl"><code>ADCMEKit.jl</code></a> package to visualize the landscape from <span>$\theta_0=[0,0,0,0]$</span> to <span>$\theta^*$</span> (true value). This gives us  early confidence  on the correctness of the implementation as well as the difficulty of the optimization problem. You can also use <code>meshview</code>, which shows a 2D landscape but is more expensive to evaluate. </li></ul><pre><code class="language-julia">using ADCME
using ADCMEKit
θ = placeholder([1.;2.;3.;4.])
x = collect(LinRange(0.0,1.0,10))
f = θ[3]*sigmoid(θ[1]*x+θ[2])+θ[4]
loss = sum((f - f0)^2)
sess = Session(); init(sess)
@show run(sess, loss)
lineview(sess, θ, loss, [1.;2.;3.;4.], zeros(4)) # or meshview(sess, θ, loss, [1.;2.;3.;4.])</code></pre><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/landscape.png?raw=true" alt="image-20200227233902747"/></p><p>The landscape is very nice (convex and smooth)! That means the optimization should be very easy. </p><ul><li><strong>Step 4: Use <code>gradview</code> to check the gradients.</strong> <code>ADCMEKit.jl</code> also provides <code>gradview</code> which visualizes the gradients at arbitrary points. This helps us to check whether the gradient is implemented correctly. </li></ul><pre><code class="language-julia">using ADCME
using ADCMEKit
θ = placeholder([1.;2.;3.;4.])
x = collect(LinRange(0.0,1.0,10))
f = θ[3]*sigmoid(θ[1]*x+θ[2])+θ[4]
loss = sum((f - f0)^2)
sess = Session(); init(sess)
@show run(sess, loss)
lineview(sess, θ, loss, [1.;2.;3.;4.], zeros(4)) # or meshview(sess, θ, loss, [1.;2.;3.;4.])
gradview(sess, θ, loss, zeros(4))</code></pre><p>​		You should get something like this:</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/custom_op.png?raw=true" alt/></p><ul><li><strong>Step 5: Change <code>placeholder</code> to <code>Variable</code> and perform optimization!</strong> We use L-BFGS-B optimizer to solve the minimization problem. A useful trick is to multiply the loss function by a large scalar so that the optimizer does not stop early (or reduce the tolerance). </li></ul><pre><code class="language-julia">using ADCME
using ADCMEKit
θ = Variable(zeros(4))
x = collect(LinRange(0.0,1.0,10))
f = θ[3]*sigmoid(θ[1]*x+θ[2])+θ[4]
loss = 1e10*sum((f - f0)^2)
sess = Session(); init(sess)
BFGS!(sess, loss)
run(sess, θ)</code></pre><p>You should get </p><pre><code class="language-bash">4-element Array{Float64,1}:
 1.0000000000008975
 2.0000000000028235
 3.0000000000056493
 3.999999999994123</code></pre><p>That&#39;s exact what we want. </p><ul><li><strong>Step 6: Last but not least, repeat step 3 and step 4 if you get stuck in a local minimum.</strong> Scrutinizing the landscape at the local minimum will give you useful information so you can make educated next step!</li></ul><h2 id="Debugging"><a class="docs-heading-anchor" href="#Debugging">Debugging</a><a id="Debugging-1"></a><a class="docs-heading-anchor-permalink" href="#Debugging" title="Permalink"></a></h2><h3 id="Sensitivity-Analysis"><a class="docs-heading-anchor" href="#Sensitivity-Analysis">Sensitivity Analysis</a><a id="Sensitivity-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Sensitivity-Analysis" title="Permalink"></a></h3><p>When the gradient test fails, we can perform <em>unit sensitivity analysis</em>. The idea is that given a function <span>$y = f(x_1, x_2, \ldots, x_n)$</span>, if we want to confirm that the gradients <span>$\frac{\partial f}{\partial x_i}$</span> is correctly implemented, we can perform 1D gradient test with respect to a small perturbation <span>$\varepsilon_i$</span>: </p><p class="math-container">\[y(\varepsilon_i) = f(x_1, x_2, \ldots, x_i + \varepsilon_i, \ldots, x_n)\]</p><p>or in the case you are not sure about the scale of <span>$x_i$</span>, </p><p class="math-container">\[y(\varepsilon_i) = f(x_1, x_2, \ldots, x_i (1 + \varepsilon_i), \ldots, x_n)\]</p><p>As an example, if we want to check whether the gradients for <code>sigmoid</code> is correctly backpropagated in the above code, we have </p><pre><code class="language-julia">using ADCME
using ADCMEKit
ε = placeholder(1.0)
θ = constant([1.;2.;3.;4.])
x = collect(LinRange(0.0,1.0,10))
f = θ[3]*sigmoid(θ[1]*x+θ[2] + ε)+θ[4]
loss = sum((f - f0)^2)
sess = Session(); init(sess)
gradview(sess, ε, loss, 0.01)</code></pre><p>We will see a second order convergence for the automatic differentiation method while a first order convergence for the finite difference method. The principle for identifying problematic operator is to go from downstream operators to top stream operators in the computational graph. For example, given the computational graph</p><p class="math-container">\[f_1\rightarrow f_2 \rightarrow \cdots \rightarrow f_i \rightarrow f_{i+1} \rightarrow \ldots \rightarrow f_n\]</p><p>If we conduct sensitivity analysis for <span>$f_i:o_i \mapsto o_{i+1}$</span>, and find that the gradient is wrong, then we can infer that at least one of the operators in the downstream <span>$f_i \rightarrow f_{i+1} \rightarrow \ldots \rightarrow f_n$</span> has problematic gradients. </p><h3 id="Check-Your-Training-Data"><a class="docs-heading-anchor" href="#Check-Your-Training-Data">Check Your Training Data</a><a id="Check-Your-Training-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Check-Your-Training-Data" title="Permalink"></a></h3><p>Sometimes it is also useful to check your training data. For example, if you are working with numerical schemes, check whether your training data are generated from reasonable physical parameters, and whether or not the numerical schemes are stable. </p><h3 id="Local-Minimum"><a class="docs-heading-anchor" href="#Local-Minimum">Local Minimum</a><a id="Local-Minimum-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Minimum" title="Permalink"></a></h3><p>To check whether or not the optimization converged to a local minimum, you can either check <code>meshview</code> or <code>lineview</code>. However, these functions only give you some hints and you should only rely solely on their results. A more reliable check is to consider <code>gradview</code>. In principle, if you have a local minimum, the gradient at the local minimum should be zero, and therefore the finite difference curve should also have second order convergence. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tu_inv/">« Inverse Modeling with ADCME</a><a class="docs-footer-nextpage" href="../tu_nn/">Combining Neural Networks with Numerical Schemes »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 19 June 2021 00:10">Saturday 19 June 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
