<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced: Automatic Differentiation for Implicit Operators · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../resources/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li class="is-active"><a class="tocitem" href>Advanced: Automatic Differentiation for Implicit Operators</a><ul class="internal"><li><a class="tocitem" href="#Implicit-Function-Theorem"><span>Implicit Function Theorem</span></a></li><li><a class="tocitem" href="#Special-Case:-Linear-Implicit-Operator"><span>Special Case: Linear Implicit Operator</span></a></li><li><a class="tocitem" href="#Implementation-in-ADCME"><span>Implementation in ADCME</span></a></li></ul></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>Advanced: Automatic Differentiation for Implicit Operators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Advanced: Automatic Differentiation for Implicit Operators</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/tu_implicit.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Advanced:-Automatic-Differentiation-for-Implicit-Operators"><a class="docs-heading-anchor" href="#Advanced:-Automatic-Differentiation-for-Implicit-Operators">Advanced: Automatic Differentiation for Implicit Operators</a><a id="Advanced:-Automatic-Differentiation-for-Implicit-Operators-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced:-Automatic-Differentiation-for-Implicit-Operators" title="Permalink"></a></h1><p>An explicit operator is an operator directly supplied by the AD library while an implicit operator is an operator whose outputs must be computed using compositions of functions that may not be differentiable, or involving iterative algorithms. For example, <span>$y = \texttt{sigmoid}(x)$</span> is an implicit operator while <span>$x = \texttt{sigmoid}(y)$</span> is an implicit operator if the library does not provide <span>$\texttt{sigmoid}^{-1}$</span>, where <span>$x$</span> is the input and <span>$y$</span> is the output. </p><p>Implicit operators are everywhere in scientific computing, from implicit numerical schemes to iterative algorithms. How to incooperate implicit operators into a differentiable programming framework is the true challenge in AD. AD is not the panacea to all inverse modeling problems; it must be augmented with abilities to tackle implicit operators to be real useful for a large variety of real-world applications. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/sim.png?raw=true" alt="Operators"/> </p><p>Roughly speaking, there are four types of operators in the computational graph, depending on whether it is linear or nonlinear and whether it is explicit or implicit. Let <span>$A$</span> be a matrix, <span>$f$</span> be a nonlinear function, <span>$F$</span> be a bivariate nonlinear function, and it is hard to express <span>$y$</span> analytically as a function of <span>$x$</span> in <span>$F(x,y)=0$</span>. </p><table><tr><th style="text-align: right">Operator</th><th style="text-align: right">Linear</th><th style="text-align: right">Nonlinear</th></tr><tr><td style="text-align: right">Explicit</td><td style="text-align: right"><span>$y = Ax$</span></td><td style="text-align: right"><span>$y = f(x)$</span></td></tr><tr><td style="text-align: right">Implicit</td><td style="text-align: right"><span>$Ay = x$</span></td><td style="text-align: right"><span>$F(x, y)=0$</span></td></tr></table><p>It is straightforward to apply AD to explicit operators, provided that the AD library supports the corresponding operators <span>$A$</span> and <span>$f$</span> (which usually do). In this tutorial, we focus on the implicit operators. </p><h2 id="Implicit-Function-Theorem"><a class="docs-heading-anchor" href="#Implicit-Function-Theorem">Implicit Function Theorem</a><a id="Implicit-Function-Theorem-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-Function-Theorem" title="Permalink"></a></h2><p>We change our notation for clarity in this section. Let <span>$L_h$</span> be a error functional, <span>$F_h$</span> be the corresponding nonlinear implicit operator, <span>$\theta$</span> is all the input to this operator and <span>$u_h$</span> is all the output of this node.</p><div>\[\begin{aligned}
    \min_{\theta}&amp;\; L_h(u_h) \\
    \mathrm{s.t.}&amp;\;\; F_h(\theta, u_h) = 0
\end{aligned}\]</div><p>Assume in the forward computation, we solve for <span>$u_h=G_h(\theta)$</span> in <span>$F_h(\theta, u_h)=0$</span>, and then </p><div>\[\tilde L_h(\theta)  = L_h(G_h(\theta))\]</div><p>Applying the implicit function theorem </p><div>\[\begin{aligned}
	&amp; \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }} + {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}}  \frac{\partial G_h(\theta)}{\partial \theta} = 0 \qquad \Rightarrow \\[4pt]
    &amp; \frac{\partial G_h(\theta)}{\partial \theta} =  -\Big( \frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}} \Big)^{ - 1} \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}
\end{aligned}\]</div><p>therefore we have</p><div>\[\begin{aligned}
    \frac{{\partial {{\tilde L}_h}(\theta )}}{{\partial \theta }} 
    &amp;= \frac{\partial {{ L}_h}(u_h )}{\partial u_h}\frac{\partial G_h(\theta)}{\partial \theta} \\
    &amp;= - \frac{{\partial {L_h}({u_h})}}{{\partial {u_h}}} \;
    \Big( {\frac{{\partial {F_h(\theta, u_h)}}}{{\partial {u_h}}}\Big|_{u_h = {G_h}(\theta )}} \Big)^{ - 1} \;
    \frac{{\partial {F_h(\theta, u_h)}}}{{\partial \theta }}\Big|_{u_h = {G_h}(\theta )}
\end{aligned}\]</div><p>This is the desired gradient. For efficiency, the computation strategy is crucial. We can either evaluate from left to right or from right to left. The correct approach is to compute from left to right. A detailed justification of this computational order is beyond the scope of this tutorial. Instead, we simply list the steps for calculating the gradients </p><p>Step 1: Calculate <span>$w$</span> by solving a linear system (never invert the matrix!)</p><div>\[w^T = \underbrace{\frac{{\partial {L_h}({u_h})}}{{\partial {u_h}}\rule[-9pt]{1pt}{0pt}}}_{1\times N} 
        \;\;
        \underbrace{\Big( {\frac{{\partial {F_h}}}{{\partial {u_h}}}\Big|_{u_h = {G_h}(\theta )}} \Big)^{ - 1}}_{N\times N}\]</div><p>Step 2: Calculate the gradient by automatic differentiation </p><div>\[w^T\;\underbrace{\frac{{\partial {F_h}}}{{\partial \theta }}\Big|_{u_h = {G_h}(\theta )}}_{N\times p} = \frac{\partial (w^T\;  {F_h}(\theta, u_h))}{\partial \theta }\Bigg|_{u_h = {G_h}(\theta )}\]</div><p>This step can be done using <a href="../api/#ADCME.independent-Tuple{PyCall.PyObject,Vararg{Any,N} where N}"><code>independent</code></a>, which stops back-propagating the gradients for its argument.  </p><pre><code class="language-julia">l  = L(u)
r  = F(theta, u)
g  = gradients(l, u)
x  = dF&#39;\g
x  = independent(x)
dL = -gradients(sum(r*x), theta)</code></pre><p>Despite the complex nature of this approach, it is quite powerful and efficient in treating implicit operators. To make it more clear, we consider a simpler special case below: the linear implicit operator. </p><h2 id="Special-Case:-Linear-Implicit-Operator"><a class="docs-heading-anchor" href="#Special-Case:-Linear-Implicit-Operator">Special Case: Linear Implicit Operator</a><a id="Special-Case:-Linear-Implicit-Operator-1"></a><a class="docs-heading-anchor-permalink" href="#Special-Case:-Linear-Implicit-Operator" title="Permalink"></a></h2><p>The linear implicit operator can be viewed as a special case of the nonlinear explicit operator. In this case</p><div>\[F(x,y) = x - Ay\]</div><p>and therefore </p><div>\[\frac{\partial J}{\partial x} = \frac{\partial J}{\partial y}A^{-1}\]</div><p>This requires us to solve a linear system with the adjoint of <span>$A$</span>, i.e., </p><div>\[A^T g = \left(\frac{\partial J}{\partial y}\right)^T\]</div><h2 id="Implementation-in-ADCME"><a class="docs-heading-anchor" href="#Implementation-in-ADCME">Implementation in ADCME</a><a id="Implementation-in-ADCME-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-in-ADCME" title="Permalink"></a></h2><p>Let&#39;s see in action how to implement an implicit operator in ADCME. First of all, we can use the <a href="../api/#ADCME.NonlinearConstrainedProblem-Union{Tuple{T}, Tuple{Function,Function,Union{Array{Float64,1}, PyCall.PyObject},Union{PyCall.PyObject, Array{Float64,N} where N}}} where T&lt;:Real"><code>NonlinearConstrainedProblem</code></a> used in <a href="https://kailaix.github.io/ADCME.jl/dev/tutorial/#Functional-Inverse-Problem-1">Functional Inverse Problem</a>. The API is suitable when the residual and the Jacobian matrix can be expressed using ADCME operators (or through custom operators) and a general Newton-Raphson algorithm is satisfactory. However, if the forward solver is performance critical and requires special accleration (such as preconditioning), then building custom operator is a preferable approach. </p><p>This approach is named <strong>physics constrained learning</strong> and has been used to develop <a href="https://github.com/lidongzh/FwiFlow.jl"><code>FwiFlow.jl</code></a>, a package for elastic full waveform inversion for subsurface flow problems. The physical equation is nonlinear, the discretization is implicit, and thus it must be solved using the Newton-Raphson method.</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/diagram.png?raw=true" alt="diagram"/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tu_nn/">« Combining Neural Networks with Numerical Schemes</a><a class="docs-footer-nextpage" href="../tu_customop/">Advanced: Custom Operators »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 21 October 2020 01:37">Wednesday 21 October 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
