<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Study on Optimizers · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li class="is-active"><a class="tocitem" href>Study on Optimizers</a><ul class="internal"><li><a class="tocitem" href="#Test-Case-I"><span>Test Case I</span></a></li><li><a class="tocitem" href="#Test-Case-II"><span>Test Case II</span></a></li><li><a class="tocitem" href="#Test-Case-III"><span>Test Case III</span></a></li></ul></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li><li><a class="tocitem" href="../sqlite3/">Introducing ADCME Database and SQL Integration: an Efficient Approach to Simulation Data Management</a></li><li><a class="tocitem" href="../hessian/">The Mathematical Structure of DNN Hessians</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li><li><a class="tocitem" href="../windows_installation/">Install ADCME on Windows</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Study on Optimizers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Study on Optimizers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/optim.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Study-on-Optimizers"><a class="docs-heading-anchor" href="#Study-on-Optimizers">Study on Optimizers</a><a id="Study-on-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Study-on-Optimizers" title="Permalink"></a></h1><p>When working with BFGS/LBFGS, there are some important aspects of the algorithm, which affect the convergence of the optimizer. </p><ul><li>BFGS approximates the Hessian or the inverse Hessian matrix. LBFGS, instead, stores a limited set of <strong>vectors</strong> and does not explicitly formulate the Hessian matrices. The Hessian matrix solve is approximated using recursions on vector vector production. </li><li>Both BFGS and LBFGS deploy a certain kind of line search strategies. For example, the Hager-Zhang and More-Thuente are two commonly used strategies. These linesearch algorithms require evaluating gradients in each line search step. This means we need to frequently do gradient back-propagation, which may be quite expensive.  We instead employ a backtracking strategy, which requires evaluating forward computation only. </li><li>Initial guess of the line search algorithm. We found that the following initial guess is quite effective for our problems:<p class="math-container">\[\alpha=\min\{100\alpha_0, 10\}\]</p>Here <span>$\alpha_0$</span> is the line search step size for the last step (for the first step, <span>$\alpha=1$</span> is used). Using this choice, we found that in a typical line search step, only 1~3 evaluations of the loss function is needed. </li><li>Stopping criterion in line search algorithms. The algorithm backtracks until a certain set of condition is met. Typically the Wolfe condition, the Armijo curvature rule, or the strong Wolfe conditions are used. Note that these criterion require gradient information, and we want to avoid calculating extra gradients during linesearch. Therefore, we use the following sufficient decrease condition </li></ul><p class="math-container">\[\phi(\alpha) \leq \phi(0) + c_1 \alpha \phi&#39;(0)\]</p><p>Here <span>$c_1=10^{-4}$</span>. </p><p>One interesting observation is that if we apply BFGS/LBFGS directly, we usually cannot make any progress. There are many reasons for this phenomenon:</p><ul><li>The initial guess for the neural network is far away from optimal, and quasi-Newton methods usually do not work well in this regime. </li><li>The approximate Hessian is quite different from the true one. The estimated search direction may deviate from the optimal one too much. </li></ul><p>First-order methods, such as Adam optimizer, usually does not suffer from these difficulties. Therefore, one solution to this problem is via &quot;warm start&quot; by running a first order optimizer (Adam) for a few iterations. Additionally, for BFGS, we can build Hessian approximations while we are running the Adam optimizer. In this way, we can use the historic information as much as possible. </p><p>Our algorithm is as follows (BFGS+Adam+Hessian):</p><ol><li><p>Run the Adam optimizer for the first few iterations, and <strong>build the approximate Hessian matrix at the same time</strong>. That is, in each iteration, we update an approximate Hessian <span>$B$</span> using </p><p><span>$B_{k+1} = \left(I - \frac{s_k y_k^T}{y_k^Ts_k}\right)B_k\left(I - \frac{ y_ks_k^T}{y_k^Ts_k}\right) + \frac{s_ks_k^T}{y_k^Ts_k}$</span></p><p>Here <span>$y_k = \nabla f(x_{k+1}) - \nabla f(x_{k}), s_k = x_{k+1}-x_k, B_0 = I$</span>. Note <span>$B_k$</span> is not used in the Adam optimizer. </p></li><li><p>Run the BFGS optimizer and use the last Hessian matrix <span>$B_k$</span> built in Step 1 as the initial guess. </p></li></ol><p>We compare our algorithm with those without approximated Hessian (BFGS+Adam) or warm start (BFGS). Additionally, we also compare our algorithm with LBFGS counterparts. </p><p>In the test case II and III, a physical field, <span>$f$</span>, is approximated using deep neural networks, <span>$f_\theta$</span>, where <span>$\theta$</span> is the weights and biases. The neural network maps coordinates to a scalar value. <span>$f_\theta$</span> is coupled with a DAE:</p><p class="math-container">\[F(u&#39;, u, Du, D^2u, \ldots ;f_\theta) = 0 \tag{1}\]</p><p>Here <span>$u&#39;$</span> represents the time derivative of <span>$u$</span>, and <span>$Du$</span>, <span>$D^2u$</span>, <span>$\ldots$</span> are first-, second-, <span>$\ldots$</span> order spatial gradients. Typically we can observe <span>$u$</span> on some discrete points <span>$\{\mathbf{x}_k\}_{k\in \mathcal{I}}$</span>. To train the neural network, we consider the following optimization problem </p><p class="math-container">\[\min_\theta \sum_{k\in \mathcal{I}} (u(\mathbf{x}_i) - u_\theta(\mathbf{x}_i))\]</p><p>Here <span>$u_\theta$</span> is the solution from Eq. 1. </p><h2 id="Test-Case-I"><a class="docs-heading-anchor" href="#Test-Case-I">Test Case I</a><a id="Test-Case-I-1"></a><a class="docs-heading-anchor-permalink" href="#Test-Case-I" title="Permalink"></a></h2><p>In the first case, we train a 4 layer network with 20 neurons per layer, and tanh activation functions. The data set is <span>$\{x_i, sin(x_i)\}_{i=1}^{100}$</span>, where <span>$x_i$</span> are randomly generated from <span>$\mathcal{U}(0,1)$</span>. The neural network <span>$f_\theta$</span> is trained by solving the following optimization problem:</p><p class="math-container">\[\min_\theta \sum_{i=1}^{100} (\sin(x_i) - f_\theta(x_i))^2\]</p><p>The neural network is small, and thus we can use BFGS/LBFGS to train. In fact, the following plot shows that BFGS/LBFGS is much more accurate and effective than the commonly used Adam optimizer. Considering the wide range of computational engineering applications, where a small neural network is sufficient, this result implies that BFGS/LBFGS for training neural networks should receive far more attention than what it does nowadays. </p><p>Also, in many applications, the major runtime is doing physical simulations instead of updating neural network parameters or approximate Hessian matrices, these &quot;expensive&quot; BFGS/LBFGS optimization algorithms should be considered a good way to leverage as much history information as possible, so as to reduce the total number of iterations (simulations). </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/Sin/data/sinloss.png?raw=true" alt/></p><h2 id="Test-Case-II"><a class="docs-heading-anchor" href="#Test-Case-II">Test Case II</a><a id="Test-Case-II-1"></a><a class="docs-heading-anchor-permalink" href="#Test-Case-II" title="Permalink"></a></h2><p>In this test case, we consider solving a Poisson&#39;s equation in <a href="https://kailaix.github.io/ADCME.jl/dev/optimizers/">this post</a>. </p><p>The exact <span>$\kappa$</span> and the corresponding solution <span>$u$</span> is shown below</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/Poisson/data/fwd.png?raw=true" alt/></p><p>We run different optimization algorithms and obtain the following loss function profiles. We see that BFGS/LBFGS without Adam warm start terminates early. BFGS in general has much better accuracy than LBFGS. An extra benefit of BFGS+Adam+Hessian compared to BFGS+Adam is that we can achieve much better accuracy. </p><table><tr><th style="text-align: right">Loss Function</th><th style="text-align: right">Zoom-in View</th></tr><tr><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/Poisson/data/loss300.png?raw=true" alt/></td><td style="text-align: right"><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/Poisson/data/loss300_zoom.png?raw=true" alt/></td></tr></table><p>We also show the mean squared error for <span>$\kappa$</span>, which confirms that BFGS+Adam+Hessian achieves the best test error. The error is calculated using the formula </p><p class="math-container">\[\frac{1}{n} (\kappa_{\text{true}}(\textbf{x}_i) - \kappa_\theta(\textbf{x}_i))^2\]</p><p>Here <span>$\textbf{x}_i$</span> is defined on Gauss points. </p><table><tr><th style="text-align: right">Algorithm</th><th style="text-align: right">Adam</th><th style="text-align: right">BFGS+Adam+Hessian</th><th style="text-align: right">BFGS+Adam</th><th style="text-align: right">BFGS</th><th style="text-align: right">LBFGS+Adam</th><th style="text-align: right">LBFGS</th></tr><tr><td style="text-align: right">MSE</td><td style="text-align: right">0.013</td><td style="text-align: right">1.00E-11</td><td style="text-align: right">1.70E-10</td><td style="text-align: right">1.10E+04</td><td style="text-align: right">0.00023</td><td style="text-align: right">97000</td></tr></table><h2 id="Test-Case-III"><a class="docs-heading-anchor" href="#Test-Case-III">Test Case III</a><a id="Test-Case-III-1"></a><a class="docs-heading-anchor-permalink" href="#Test-Case-III" title="Permalink"></a></h2><p>In the last example, we consider the linear elasticity. The problem description can be found <a href="https://kailaix.github.io/AdFem.jl/dev/staticelasticity/">here</a>.</p><p>We fix the random seed for neural network initialization and run different optimization algorithms. The initial guess for the Young&#39;s modulus and the reference one are shown in the following plot </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/LinearElasticity/data/init_le.png?raw=true" alt/></p><p>The corresponding velocity and stress fields are </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/LinearElasticity/data/fwd.png?raw=true" alt/></p><p>We perform the optimization using different algorithms. In the case where Adam is used as warm start, we run Adam optimization for 50 iterations. We run the optimization for at most 500 iterations (so there is at most 500 evaluations of gradients) The loss function is shown below</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/LinearElasticity/data/loss_le.png?raw=true" alt/></p><p>We see that BFGS+Adam+Hessian achieves the smallest loss functions among all algorithms. We also show the MSE for <span>$E$</span>, i.e., <span>$\frac{1}{n} (E_{\text{true}}(\textbf{x}_i) - E_\theta(\textbf{x}_i))^2$</span>, where <span>$\textbf{x}_i$</span> is defined on Gauss points. </p><table><tr><th style="text-align: right">Algorithm</th><th style="text-align: right">Adam</th><th style="text-align: right">BFGS+Adam+Hessian</th><th style="text-align: right">BFGS+Adam</th><th style="text-align: right">BFGS</th><th style="text-align: right">LBFGS+Adam</th><th style="text-align: right">LBFGS</th></tr><tr><td style="text-align: right">MSE</td><td style="text-align: right">0.0033</td><td style="text-align: right">1.90E-07</td><td style="text-align: right">4.00E-06</td><td style="text-align: right">6.20E-06</td><td style="text-align: right">0.0031</td><td style="text-align: right">0.0013</td></tr></table><p>This confirms that BFGS+Adam+Hessian indeed generates a much more accurate result. </p><p>We also compare the results for the BFGS+Adam+Hessian and Adam algorithms:</p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/OR/LinearElasticity/data/compare_le.png?raw=true" alt/></p><p>We see the Adam optimizer achieves reasonable result but is challenging to deliver high accurate estimation within 500 iterations.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimizers/">« Optimizers</a><a class="docs-footer-nextpage" href="../ode/">PDE/ODE Solvers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 6 March 2021 19:09">Saturday 6 March 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
