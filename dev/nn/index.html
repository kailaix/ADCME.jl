<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Neural Networks · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li class="is-active"><a class="tocitem" href>Neural Networks</a><ul class="internal"><li><a class="tocitem" href="#Constructing-a-Neural-Network"><span>Constructing a Neural Network</span></a></li><li><a class="tocitem" href="#Prediction"><span>Prediction</span></a></li><li><a class="tocitem" href="#Save-the-Neural-Network"><span>Save the Neural Network</span></a></li><li><a class="tocitem" href="#Convert-Neural-Network-to-Codes"><span>Convert Neural Network to Codes</span></a></li><li><a class="tocitem" href="#Advance:-Use-Neural-Network-Implementations-from-Python-Script/Modules"><span>Advance: Use Neural Network Implementations from Python Script/Modules</span></a></li></ul></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li><li><a class="tocitem" href="../sqlite3/">Introducing ADCME Database and SQL Integration: an Efficient Approach to Simulation Data Management</a></li><li><a class="tocitem" href="../hessian/">The Mathematical Structure of DNN Hessians</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li><li><a class="tocitem" href="../windows_installation/">Install ADCME on Windows</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Neural Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Neural Networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/nn.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Networks"><a class="docs-heading-anchor" href="#Neural-Networks">Neural Networks</a><a id="Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Networks" title="Permalink"></a></h1><p>A neural network can be viewed as a computational graph where each operator in the computational graph is composed of linear transformation or simple explicit nonlinear mapping (called <em>activation functions</em>). There are essential components of the neural network </p><ol><li>Input: the input to the neural network, which is a real or complex valued vector; the input is often called <em>features</em> in machine learning. To leverage dense linear algebra, features are usually aggregated into a matrix and fed to the neural network. </li><li>Output: the output of the neural network is also a real or complex valued vectors. The vector can be tranformed to categorical values (labels) based on the specific application. </li></ol><p>The common activations functions include ReLU (Rectified linear unit), tanh, leaky ReLU, SELU, ELU, etc. In general, for inverse modeling in scientific computing, tanh usually outperms the others due to its smoothness and boundedness, and forms a solid choice at the first try. </p><p>A common limitation of the neural network is overfitting. The neural network contains plenty of free parameters, which makes the neural network &quot;memorize&quot; the training data easily. Therefore, you may see very a small training error, but have large test errors. Regularization methods have been proposed to alleviate this problem; to name a few, restricting network sizes, imposing weight regulization (Lasso or Ridge), using Dropout and batch normalization, etc. </p><h2 id="Constructing-a-Neural-Network"><a class="docs-heading-anchor" href="#Constructing-a-Neural-Network">Constructing a Neural Network</a><a id="Constructing-a-Neural-Network-1"></a><a class="docs-heading-anchor-permalink" href="#Constructing-a-Neural-Network" title="Permalink"></a></h2><p>ADCME provides a very simple way to specify a fully connected neural network, <a href="../api/#ADCME.fc"><code>fc</code></a> (short for <em>autoencoder</em>)</p><pre><code class="language-julia">x = constant(rand(10,2)) # input
config = [20,20,20,3] # hidden layers
θ = fc_init([2;config]) # getting an initial weight-and-biases vector. 

y1 = fc(x, config)
y2 = fc(x, config, θ)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>When you construct a neural network using <code>fc(x, config)</code> syntax, ADCME will construct the weights and biases automatically for you and label the parameters (the default is <code>default</code>). In some cases, you may have multiple neural networks, and you can label the neural network manually using </p><pre><code class="language-julia">fc(x1, config1, &quot;label1&quot;)
fc(x2, config2, &quot;label2&quot;)
...</code></pre></div></div><p>In scientific computing, sometimes we not only want to evaluate the neural network output, but also the sensitivity. Specifically, if </p><p class="math-container">\[y = NN_{\theta}(x)\]</p><p>We also want to compute <span>$\nabla_x NN_{\theta}(x)$</span>. ADCME provides a function <a href="../api/#ADCME.fcx-Tuple{Union{Array{Float64,2}, PyCall.PyObject},Array{Int64,1},Union{Array{Float64,1}, PyCall.PyObject}}"><code>fcx</code></a> (short for <em>fully-connected</em>)</p><pre><code class="language-julia">y3, dy3 = fcx(x, config, θ)</code></pre><p>Here <code>dy3</code> will be a <span>$10\times 3 \times 2$</span> tensor, where <code>dy3[i,:,:]</code> is the Jacobian matrix of the <span>$i$</span>-th output with respect to the <span>$i$</span>-th input (Note the <span>$i$</span>-th output is independent of <span>$j$</span>-th input, whenever <span>$i\neq j$</span>).</p><h2 id="Prediction"><a class="docs-heading-anchor" href="#Prediction">Prediction</a><a id="Prediction-1"></a><a class="docs-heading-anchor-permalink" href="#Prediction" title="Permalink"></a></h2><p>After training a neural network, we can use the trained neural network for prediction. Here is an example</p><pre><code class="language-julia">using ADCME
x_train = rand(10,2)
x_test = rand(20,2)
y = fc(x_train, [20,20,10])
y_obs = rand(10,10)
loss = sum((y-y_obs)^2)
sess = Session(); init(sess)
BFGS!(sess, loss)
# prediction
run(sess, fc(x_test, [20,20,10]))</code></pre><p>Note that the second <code>fc</code> does not create a new neural network, but instead searches for a neural network with the label <code>default</code> because the default label is <code>default</code>. If you constructed a neural network with label <code>mylabel</code>: <code>fc(x_train, [20,20,10], &quot;mylabel&quot;)</code>, you can predict using </p><pre><code class="language-julia">run(sess, fc(x_test, [20,20,10], &quot;mylabel&quot;))</code></pre><h2 id="Save-the-Neural-Network"><a class="docs-heading-anchor" href="#Save-the-Neural-Network">Save the Neural Network</a><a id="Save-the-Neural-Network-1"></a><a class="docs-heading-anchor-permalink" href="#Save-the-Neural-Network" title="Permalink"></a></h2><p>To save the trained neural network in the Session <code>sess</code>, we can use</p><pre><code class="language-julia">ADCME.save(sess, &quot;filename.mat&quot;)</code></pre><p>This will create a <code>.mat</code> file that contains all the <strong>labeled</strong> weights and biases. If there are other variables besides neural network parameters, these variables will also be saved. </p><p>To load the weights and biases to the current session, create a neural network with the same label and run</p><pre><code class="language-julia">ADCME.load(sess, &quot;filename.mat&quot;)</code></pre><h2 id="Convert-Neural-Network-to-Codes"><a class="docs-heading-anchor" href="#Convert-Neural-Network-to-Codes">Convert Neural Network to Codes</a><a id="Convert-Neural-Network-to-Codes-1"></a><a class="docs-heading-anchor-permalink" href="#Convert-Neural-Network-to-Codes" title="Permalink"></a></h2><p>Sometimes we may also want to convert a fully-connected neural network to pure Julia codes. This can be done via <a href="@ref"><code>fc_to_code</code></a>. </p><p>After saving the neural network to a mat file via <code>ADCME.save</code>, we can call</p><pre><code class="language-julia">ae_to_code(&quot;filename.mat&quot;, &quot;mylabel&quot;)</code></pre><p>If the second argument is missing, the default is <code>default</code>. For example,</p><pre><code class="language-none">julia&gt; ae_to_code(&quot;filename.mat&quot;, &quot;default&quot;)|&gt;println
let aedictdefault = matread(&quot;filename.mat&quot;)
  global nndefault
  function nndefault(net)
    W0 = aedictdefault[&quot;defaultbackslashfully_connectedbackslashweightscolon0&quot;]
    b0 = aedictdefault[&quot;defaultbackslashfully_connectedbackslashbiasescolon0&quot;];
    isa(net, Array) ? (net = net * W0 .+ b0&#39;) : (net = net *W0 + b0)
    isa(net, Array) ? (net = tanh.(net)) : (net=tanh(net))
    #-------------------------------------------------------------------
    W1 = aedictdefault[&quot;defaultbackslashfully_connected_1backslashweightscolon0&quot;]
    b1 = aedictdefault[&quot;defaultbackslashfully_connected_1backslashbiasescolon0&quot;];
    isa(net, Array) ? (net = net * W1 .+ b1&#39;) : (net = net *W1 + b1)
    isa(net, Array) ? (net = tanh.(net)) : (net=tanh(net))
    #-------------------------------------------------------------------
    W2 = aedictdefault[&quot;defaultbackslashfully_connected_2backslashweightscolon0&quot;]
    b2 = aedictdefault[&quot;defaultbackslashfully_connected_2backslashbiasescolon0&quot;];
    isa(net, Array) ? (net = net * W2 .+ b2&#39;) : (net = net *W2 + b2)
    return net
  end
end</code></pre><h2 id="Advance:-Use-Neural-Network-Implementations-from-Python-Script/Modules"><a class="docs-heading-anchor" href="#Advance:-Use-Neural-Network-Implementations-from-Python-Script/Modules">Advance: Use Neural Network Implementations from Python Script/Modules</a><a id="Advance:-Use-Neural-Network-Implementations-from-Python-Script/Modules-1"></a><a class="docs-heading-anchor-permalink" href="#Advance:-Use-Neural-Network-Implementations-from-Python-Script/Modules" title="Permalink"></a></h2><p>If you have a Python implementation of a neural network architecture and want to use that architecture, we do not need to reimplement it in ADCME. Instead, we can use the <code>PyCall.jl</code> package and import the functionalities. For example, if you have a Python package <code>nnpy</code> and it has a function <code>magic_neural_network</code>. We can use the following code to call <code>magic_neural_network</code></p><pre><code class="language-julia">using PyCall
using ADCME

nnpy = pyimport(&quot;nnpy&quot;)

x = constant(rand(100,2))
y = nnpy.magic_neural_network(x)</code></pre><p>Because all the runtime computation are conducted in C++, there is no harm to performance using this mechanism.  </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../julia_customop/">« Julia Custom Operators</a><a class="docs-footer-nextpage" href="../ot/">Optimal Transport »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 4 March 2021 22:36">Thursday 4 March 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
