<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li class="is-active"><a class="tocitem" href>What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a><ul class="internal"><li><a class="tocitem" href="#Computational-Graph-1"><span>Computational Graph</span></a></li><li><a class="tocitem" href="#Automatic-Differentiation-1"><span>Automatic Differentiation</span></a></li><li><a class="tocitem" href="#Comparison-1"><span>Comparison</span></a></li><li><a class="tocitem" href="#A-Mathematical-Description-of-Reverse-mode-Automatic-Differentiation-1"><span>A Mathematical Description of Reverse-mode Automatic Differentiation</span></a></li><li><a class="tocitem" href="#TensorFlow-1"><span>TensorFlow</span></a></li><li><a class="tocitem" href="#Summary-1"><span>Summary</span></a></li></ul></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorial</a></li><li class="is-active"><a href>What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/tu_whatis.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="What-is-ADCME?-Computational-Graph,-Automatic-Differentiation-and-TensorFlow-1"><a class="docs-heading-anchor" href="#What-is-ADCME?-Computational-Graph,-Automatic-Differentiation-and-TensorFlow-1">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a><a class="docs-heading-anchor-permalink" href="#What-is-ADCME?-Computational-Graph,-Automatic-Differentiation-and-TensorFlow-1" title="Permalink"></a></h1><h2 id="Computational-Graph-1"><a class="docs-heading-anchor" href="#Computational-Graph-1">Computational Graph</a><a class="docs-heading-anchor-permalink" href="#Computational-Graph-1" title="Permalink"></a></h2><p>A computational graph is a functional description of the required computation. In the computationall graph, an edge represents a value, such as a scalar, a vector, a matrix or a tensor. A node represents a function whose input arguments are the the incoming edges and output values are are the outcoming edges. Based on the number of input arguments, a function can be nullary, unary, binary, ..., and n-ary; based on the number of output arguments, a function can be single-valued or multiple-valued. </p><p>Computational graphs are directed and acyclic. The acyclicity implies the forward propagation computation is well-defined: we loop over edges in topological order and evaluates the outcoming edges for each node. To make the discussion more concrete, we illustrate the computational graph for </p><div>\[z = \sin(x_1+x_2) + x_2^2 x_3\]</div><p><img src="../assets/fd.jpeg" alt/></p><p>There are in general two programmatic ways to construct computational graphs: static and dynamic declaration. In the static declaration, the computational graph is first constructed symbolically, i.e., no actual numerical arithmetic are executed. Then a bunch of data is fed to the graph for the actual computation. An advantage of static declarations is that they allow for graph optimization such as removing unused branches. Additionally, the dependencies can be analyzed for parallel execution of independent components. Another approach is the dynamic declaration, where the computational graph is constructed on-the-fly as the forward computation is executed. The dynamic declaration interleaves construction and evaluation of the graph, making software development more intuitive. </p><h2 id="Automatic-Differentiation-1"><a class="docs-heading-anchor" href="#Automatic-Differentiation-1">Automatic Differentiation</a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation-1" title="Permalink"></a></h2><p>An important application of computational graphs is automatic differentiation (AD). In general, there are three modes of AD: reverse-mode, forward-mode, and mixed mode. In this tutorial, we focus on the forward-mode and reverse-mode.</p><p>Basically, the forward mode and the reverse mode automatic differenation both use the. chain rule for computing the gradients. They evaluate the gradients of &quot;small&quot; functions analytically (symbolically) and chain all the computed <strong>numerical</strong> gradients via the chain rule</p><div>\[\frac{\partial f\circ g (x)}{\partial x} = \frac{\partial f&#39;\circ g(x)}{\partial g} {\frac{\partial g&#39;(x)}{\partial x}}\]</div><h3 id="Forward-Mode-1"><a class="docs-heading-anchor" href="#Forward-Mode-1">Forward Mode</a><a class="docs-heading-anchor-permalink" href="#Forward-Mode-1" title="Permalink"></a></h3><p>In the forward mode, the gradients are computed in the same order as function evaluation, i.e., <span>${\frac{\partial g&#39;(x)}{\partial x}}$</span> is computed first, and then <span>$\frac{\partial f&#39;\circ g(x)}{\partial g} {\frac{\partial g&#39;(x)}{\partial x}}$</span> as a whole. The idea is the same for a computational graph, except that we need to <strong>aggregate</strong> all the gradients from up-streams first, and then <strong>forward</strong> the gradients to down-stream nodes. Here we show how the gradient </p><div>\[f(x) = \begin{bmatrix}
			x^4\\
			x^2 + \sin(x) \\
			-\sin(x)\end{bmatrix}\]</div><p>is computed.</p><table><tr><th style="text-align: right">Forward-mode AD in the Computational Graph</th><th style="text-align: right">Example</th></tr><tr><td style="text-align: right"><img src="../assets/fad.png" alt/></td><td style="text-align: right"><img src="../assets/forwardmode.png" alt/></td></tr></table><h3 id="Reverse-Mode-1"><a class="docs-heading-anchor" href="#Reverse-Mode-1">Reverse Mode</a><a class="docs-heading-anchor-permalink" href="#Reverse-Mode-1" title="Permalink"></a></h3><p>In contrast, the reverse-mode AD computes the gradient  in the reverse order of forward computation, i.e., <span>$\frac{\partial f&#39;\circ g(x)}{\partial g}$</span> is first evaluated and then <span>$\frac{\partial f&#39;\circ g(x)}{\partial g} {\frac{\partial g&#39;(x)}{\partial x}}$</span> as a whole. In the computational graph, each node first aggregate all the gradients from down-streams  and then back-propagates the gradient to upstream nodes.</p><p>We show how the gradients of <span>$z = \sin(x_1+x_2) + x_2^2 x_3$</span> is evaluated. </p><table><tr><th style="text-align: right">Reverse-mode AD in the Computational Graph</th><th style="text-align: right">Step 1</th><th style="text-align: right">Step 2</th><th style="text-align: right">Step 3</th><th style="text-align: right">Step 4</th></tr><tr><td style="text-align: right"><img src="../assets/rad.png" alt/></td><td style="text-align: right"><img src="../assets/bd1.jpeg" alt/></td><td style="text-align: right"><img src="../assets/bd2.jpeg" alt/></td><td style="text-align: right"><img src="../assets/bd3.jpeg" alt/></td><td style="text-align: right"><img src="../assets/bd4.jpeg" alt/></td></tr></table><h2 id="Comparison-1"><a class="docs-heading-anchor" href="#Comparison-1">Comparison</a><a class="docs-heading-anchor-permalink" href="#Comparison-1" title="Permalink"></a></h2><p>Reverse-mode AD reuses gradients from down-streams. Therefore, this mode is useful for many-to-few mappings. In contrast, forward-mode AD reuses gradients from upstreams. This mechanism makes forward-mode AD suitable for few-to-many mappings. Therefore, for inverse modeling problems where the objective function is usually a scalar, reverse-mode AD is most relevant. For uncertainty quantification or sensitivity analysis, the forward-mode AD is most useful. We summarize the two modes in the following table:</p><p>For a function <span>$f:\mathbf{R}^n \rightarrow \mathbf{R}^m$</span></p><table><tr><th style="text-align: right">Mode</th><th style="text-align: right">Suitable for...</th><th style="text-align: right">Complexity<sup class="footnote-reference"><a id="citeref-OPS" href="#footnote-OPS">[OPS]</a></sup></th><th style="text-align: right">Application</th></tr><tr><td style="text-align: right">Forward</td><td style="text-align: right"><span>$m\gg n$</span></td><td style="text-align: right"><span>$\leq 2.5\;\mathrm{OPS}(f(x))$</span></td><td style="text-align: right">UQ</td></tr><tr><td style="text-align: right">Reverse</td><td style="text-align: right"><span>$m\ll n$</span></td><td style="text-align: right"><span>$\leq 4\;\mathrm{OPS}(f(x))$</span></td><td style="text-align: right">Inverse Modeling</td></tr></table><h2 id="A-Mathematical-Description-of-Reverse-mode-Automatic-Differentiation-1"><a class="docs-heading-anchor" href="#A-Mathematical-Description-of-Reverse-mode-Automatic-Differentiation-1">A Mathematical Description of Reverse-mode Automatic Differentiation</a><a class="docs-heading-anchor-permalink" href="#A-Mathematical-Description-of-Reverse-mode-Automatic-Differentiation-1" title="Permalink"></a></h2><p>Because the reverse-mode automatic differentiation is very important for inverse modeling, we devote this section to a rigorious mathematical description of the reverse-mode automatic differentiation. </p><p>To explain how reverse-mode AD works, let&#39;s consider constructing a computational graph with independent variables </p><div>\[\{x_1, x_2, \ldots, x_n\}\]</div><p>and the forward propagation produces a single output <span>$x_N$</span>, <span>$N&gt;n$</span>. The gradients <span>$\frac{\partial x_N(x_1, x_2, \ldots, x_n)}{\partial x_i}$</span> <span>$i=1$</span>, <span>$2$</span>, <span>$\ldots$</span>, <span>$n$</span> are queried. </p><p>The idea is that this algorithm can be decomposed into a sequence of functions <span>$f_i$</span> (<span>$i=n+1, n+2, \ldots, N$</span>) that can be easily differentiated analytically, such as addition, multiplication, or basic functions like exponential, logarithm and trigonometric functions. Mathematically, we can formulate it as</p><div>\[\begin{aligned}
    x_{n+1} &amp;= f_{n+1}(\mathbf{x}_{\pi({n+1})})\\
    x_{n+2} &amp;= f_{n+2}(\mathbf{x}_{\pi({n+2})})\\
    \ldots\\
    x_{N} &amp;= f_{N}(\mathbf{x}_{\pi({N})})\\
\end{aligned}\]</div><p>where <span>$\mathbf{x} = \{x_i\}_{i=1}^N$</span> and <span>$\pi(i)$</span> are the parents of <span>$x_i$</span>, s.t., <span>$\pi(i) \in \{1,2,\ldots,i-1\}$</span>.</p><p><img src="../assets/cg.jpeg" alt/></p><p>The idea to compute <span>$\partial x_N / \partial x_i$</span> is to start from <span>$i = N$</span>, and establish recurrences to calculate derivatives with respect to <span>$x_i$</span> in terms of derivatives with respect to <span>$x_j$</span>, <span>$j &gt;i$</span>. To define these recurrences rigorously, we need to define different functions that differ by the choice of independent variables.</p><p>The starting point is to define <span>$x_i$</span> considering all previous <span>$x_j$</span>, <span>$j &lt; i$</span>, as independent variables. Then:</p><div>\[x_i(x_1, x_2, \ldots, x_{i-1}) = f_i(\mathbf{x}_{\pi(i)})\]</div><p>Next, we observe that <span>$x_{i-1}$</span> is a function of previous <span>$x_j$</span>, <span>$j &lt; i-1$</span>, and so on; so that we can recursively define <span>$x_i$</span> in terms of fewer independent variables, say in terms of <span>$x_1$</span>, ..., <span>$x_k$</span>, with <span>$k &lt; i-1$</span>. This is done recursively using the following definition:</p><div>\[x_i(x_1, x_2, \ldots, x_j) = x_i(x_1, x_2, \ldots, x_j, f_{j+1}(\mathbf{x}_{\pi(j+1)})), \quad n &lt; j+1 &lt; i\]</div><p>Observe that the function of the left-hand side has <span>$j$</span> arguments, while the function on the right has <span>$j+1$</span> arguments. This equation is used to &quot;reduce&quot; the number of arguments in <span>$x_i$</span>.</p><p>With these definitions, we can define recurrences for our partial derivatives which form the basis of the back-propagation algorithm. The partial derivatives for</p><div>\[x_N(x_1, x_2, \ldots, x_{N-1})\]</div><p>are readily available since we can differentiate</p><div>\[f_N(\mathbf{x}_{\pi(N)})\]</div><p>directly. The problem is therefore to calculate partial derivatives for functions of the type <span>$x_N(x_1, x_2, \ldots, x_i)$</span> with <span>$i&lt;N-1$</span>. This is done using the following recurrence:</p><div>\[\frac{\partial x_N(x_1, x_2, \ldots, x_{i})}{\partial x_i} = \sum_{j\,:\,i\in \pi(j)}
    \frac{\partial x_N(x_1, x_2, \ldots, x_j)}{\partial x_j}
    \frac{\partial x_j(x_1, x_2, \ldots, x_{j-1})}{\partial x_i}\]</div><p>with <span>$n &lt; i&lt; N-1$</span>. Since <span>$i \in \pi(j)$</span>, we have <span>$i &lt; j$</span>. So we are defining derivatives with respect to <span>$x_i$</span> in terms of derivatives with respect to <span>$x_j$</span> with <span>$j &gt; i$</span>. The last term</p><div>\[\frac{\partial x_j(x_1, x_2, \ldots, x_{j-1})}{\partial x_k}\]</div><p>is readily available since:</p><div>\[x_j(x_1, x_2, \ldots, x_{j-1}) = f_j(\mathbf{x}_{\pi(j)})\]</div><p><img src="../assets/cg2.jpeg" alt/></p><p>The computational cost of this recurrence is proportional to the number of edges in the computational graph (excluding the nodes <span>$1$</span> through <span>$n$</span>), assuming that the cost of differentiating <span>$f_k$</span> is <span>$O(1)$</span>. The last step is defining</p><div>\[\frac{\partial x_N(x_1, x_2, \ldots, x_n)}{\partial x_i} = \sum_{j\,:\,i\in \pi(j)}
    \frac{\partial x_N(x_1, x_2, \ldots, x_j)}{\partial x_j}
    \frac{\partial x_j(x_1, x_2, \ldots, x_{j-1})}{\partial x_i}\]</div><p>with <span>$1 \le i \le n$</span>. Since <span>$n &lt; j$</span>, the first term</p><div>\[\frac{\partial x_N(x_1, x_2, \ldots, x_j)}{\partial x_j}\]</div><p>has already been computed in earlier steps of the algorithm. The computational cost is equal to the number of edges connected to one of the nodes in <span>$\{1, \dots, n\}$</span>.</p><p>We can see that the complexity of the back-propagation is bounded by that of the forward step, up to a constant factor. Reverse mode differentiation is very useful in the penalty method, where the loss function is a scalar, and no other constraints are present. </p><p>As a concrete example, we consider the example of evaluating <span>$\frac{dz(x_1,x_2,x_3)}{dx_i}$</span>, where <span>$z = \sin(x_1+x_2) + x_2^2x_3$</span>. The gradients are  backward propagated exactly in the reverse order of the forward propagation. </p><table><tr><th style="text-align: right">Step 1</th><th style="text-align: right">Step 2</th><th style="text-align: right">Step 3</th><th style="text-align: right">Step 4</th></tr><tr><td style="text-align: right"><img src="../assets/bd1.jpeg" alt/></td><td style="text-align: right"><img src="../assets/bd2.jpeg" alt/></td><td style="text-align: right"><img src="../assets/bd3.jpeg" alt/></td><td style="text-align: right"><img src="../assets/bd4.jpeg" alt/></td></tr></table><h2 id="TensorFlow-1"><a class="docs-heading-anchor" href="#TensorFlow-1">TensorFlow</a><a class="docs-heading-anchor-permalink" href="#TensorFlow-1" title="Permalink"></a></h2><p>Google&#39;s TensorFlow provides a convenient way to specify the computational graph statically. TensorFlow  has automatic differentiation features and its performance is optimized for large-scale computing. ADCME is built on TensorFlow by overloading numerical operators and augmenting TensorFlow with essential scientific computing functionalities. We contrast the TensorFlow implementation with the ADCME implementation of computing the objective function and its gradient in the following example.</p><div>\[y(x) = \|(AA^T+xI)^{-1}b-c\|^2, \; z = y&#39;(x)\]</div><p>where <span>$A\in \mathbb{R}^{n\times n}$</span> is a random matrix, <span>$x,b,c$</span> are scalars, and <span>$n=10$</span>.</p><p><strong>TensorFlow Implementation</strong></p><pre><code class="language-python">import tensorflow as tf
import numpy as np 
A = tf.constant(np.random.rand(10,10), dtype=tf.float64)
x = tf.constant(1.0, dtype=tf.float64)
b = tf.constant(np.random.rand(10), dtype=tf.float64)
c = tf.constant(np.random.rand(10), dtype=tf.float64)
B = tf.matmul(A, tf.transpose(A)) + x * tf.constant(np.identity(10))
y = tf.reduce_sum((tf.squeeze(tf.matrix_solve(B, tf.reshape(b, (-1,1))))-c)**2)
z = tf.gradients(y, x)[0]
sess = tf.Session()
sess.run([y, z])</code></pre><p><strong>Julia Implementation</strong></p><pre><code class="language-julia">using ADCME, LinearAlgebra
A = constant(rand(10,10))
x = constant(1.0)
b = rand(10)
c = rand(10)
y = sum(((A*A&#39;+x*diagm(0=&gt;ones(10)))\b - c)^2)
z = gradients(y, x)
sess = Session()
run(sess, [y,z])</code></pre><h2 id="Summary-1"><a class="docs-heading-anchor" href="#Summary-1">Summary</a><a class="docs-heading-anchor-permalink" href="#Summary-1" title="Permalink"></a></h2><p>The computational graph and automatic differentiation are the core concepts underlying ADCME. TensorFlow works as the workhorse for optimion and execution of the computational graph in a high performance environment. </p><p>To construct a computational graph for a Julia program, ADCME overloads most numerical operators like <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code> and matrix multiplication in Julia by the corresponding TensorFlow operators. Therefore, you will find many similar workflows and concepts as TensorFlow, such as <code>constant</code>, <code>Variable</code>, <code>session</code>, etc. However, not all operators relevant to scientific computing in Julia have its counterparts in TensorFlow. To that end, custom kernels are implemented to supplement TensorFlow, such as sparse linear algebra related functions. </p><p>ADCME aims at providing a easy-to-use, flexible,  and high performance interface to do data processing, implement numerical schemes, and conduct mathematical optimization. It is built not only for academic interest but also for real-life large-scale simulations. </p><p>Like TensorFlow, ADCME works in sessions, in which each session consumes a computational graph. Usually the workflow is split into three steps:</p><ol><li><p>Define independent variables. <code>constant</code> for tensors that do not require gradients and <code>Variable</code> for those requiring gradients. </p><pre><code class="language-julia">a = constant(0.0)</code></pre></li><li><p>Construct the computational graph by defining the computation</p><pre><code class="language-julia">L = (a-1)^2</code></pre></li><li><p>Create a session and run the computational graph</p><pre><code class="language-julia">sess = Session()
run(sess, L)</code></pre></li></ol><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-OPS"><a class="tag is-link" href="#citeref-OPS">OPS</a>See &quot;Margossian CC. A review of automatic differentiation and its efficient implementation. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 2019 Jul;9(4):e1305.&quot;.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../tutorial/">« Overview</a><a class="docs-footer-nextpage" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 8 May 2020 23:18">Friday 8 May 2020</span>. Using Julia version 1.4.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
