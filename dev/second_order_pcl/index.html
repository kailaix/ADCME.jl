<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Second Order Physics Constrained Learning · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li><li><a class="tocitem" href="../sqlite3/">Introducing ADCME Database and SQL Integration: an Efficient Approach to Simulation Data Management</a></li><li><a class="tocitem" href="../hessian/">The Mathematical Structure of DNN Hessians</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li><li><a class="tocitem" href="../reinforcement_learning/">Reinforcement Learning Basics: Q-learning and SARSA</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li><li><a class="tocitem" href="../windows_installation/">Install ADCME on Windows</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Second Order Physics Constrained Learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Second Order Physics Constrained Learning</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/second_order_pcl.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Second-Order-Physics-Constrained-Learning"><a class="docs-heading-anchor" href="#Second-Order-Physics-Constrained-Learning">Second Order Physics Constrained Learning</a><a id="Second-Order-Physics-Constrained-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Second-Order-Physics-Constrained-Learning" title="Permalink"></a></h1><p>In this note, we describe the second order physics constrained learning (PCL) for efficient calculating Hessians using computational graphs. To begin with, let <span>$x\in\mathbb{R}^d$</span> be the coordinates and consider a chain of operations </p><p class="math-container">\[l(x) = F_n\circ F_{n-1}\circ \cdots \circ F_1(x) \tag{1}\]</p><p>Here <span>$l$</span> is a scalar function, which means <span>$F_n$</span> is a scalar function. It is not hard to see that we can express any computational graph with a scalar output using Eq. 1. For convenience, given a fixed <span>$k\in\{1,2,\ldots, n\}$</span>, we define <span>$\Phi$</span>, <span>$F$</span> as follows</p><p class="math-container">\[l(x) = \underbrace{F_n\circ F_{n-1}\circ \cdots \circ F_k}_{\Phi} \circ \underbrace{F_{k-1}\cdots \circ F_1}_{F}(x)\]</p><p>We omit <span>$k$</span> in <span>$\Phi$</span>, <span>$F$</span> for clarity. </p><h2 id="Calculating-the-Hessian-in-TensorFlow"><a class="docs-heading-anchor" href="#Calculating-the-Hessian-in-TensorFlow">Calculating the Hessian in TensorFlow</a><a id="Calculating-the-Hessian-in-TensorFlow-1"></a><a class="docs-heading-anchor-permalink" href="#Calculating-the-Hessian-in-TensorFlow" title="Permalink"></a></h2><p>TensorFlow provides <a href="https://www.tensorflow.org/api_docs/python/tf/hessians"><code>tf.hessians</code></a> to calculate hessian functions. ADCME exposes this function via <a href="@ref"><code>hessians</code></a>. The idea is to first construct a computational graph for the gradients <span>$\nabla_x l(x)$</span>, then for <strong>each component</strong> of <span>$\nabla_x l(x)$</span>, <span>$\nabla_{x_i} l(x)$</span>, we construct a gradient back-propagation computational graph </p><p class="math-container">\[\nabla_x \nabla_{x_i} l(x)\]</p><p>This gives us a row/column in the Hessian matrix. The following shows the main code from TensorFlow:</p><pre><code class="language-python">_, hessian = control_flow_ops.while_loop(
        lambda j, _: j &lt; n,
        lambda j, result: (j + 1,
                           result.write(j, gradients(gradient[j], x)[0])),
        loop_vars
    )</code></pre><p>We see it&#39;s essentially a loop over each component in <code>gradient</code></p><p>Albeit straight-forward, this approach suffers from three drawbacks:</p><ol><li>The algorithm does not leverage the symmetry of the Hessian matrix. The Hessian structure can be exploited for efficient computations. </li><li>The algorithm can be quite expensive. Firstly, it requires a gradient back-propagation over each component of <span>$\nabla_x l(x)$</span>. Although TensorFlow can concurrently evaluates these Hessian rows/columns concurrently, the dimension of <span>$x$</span> can be very large and therefore the computations cannot be fully parallelized. Secondly, each back-propagation of <span>$\nabla_{x_i} l(x)$</span> requires both forward computation <span>$l(x)$</span> and gradient back-propagation for <span>$\nabla_{x} l(x)$</span>, we need to carefully arrange the computations and storages so that these intermediate results can be reused. Otherwise, redundant computations lead to extra costs. </li><li>The most demanding requirements of the algorithm is that we need to implement–-for every operator–-the &quot;gradients of gradients&quot;. Although simple for some operators (there are already existing implementations for some operators in TensorFlow!), this can be very hard for sophisticated operators, e.g., implicit operators. </li></ol><h2 id="Second-Order-Physics-Constrained-Learning-2"><a class="docs-heading-anchor" href="#Second-Order-Physics-Constrained-Learning-2">Second Order Physics Constrained Learning</a><a class="docs-heading-anchor-permalink" href="#Second-Order-Physics-Constrained-Learning-2" title="Permalink"></a></h2><p>Here we consider the second order physics constrained learning. The main idea is to apply the implicit function theorem to </p><p class="math-container">\[l = \Phi(F(x)) \tag{2}\]</p><p>twice. First, we introduce some notation:</p><p class="math-container">\[\Phi_k(y) = \frac{\partial \Phi(y)}{\partial y_k}, \quad \Phi_{kl}(y) = \frac{\partial^2 \Phi(y)}{\partial y_k \partial y_l}\]</p><p class="math-container">\[F_{k,l}(x) = \frac{\partial F_k(x)}{\partial x_l}, \quad F_{k,lr}(x) = \frac{\partial^2 F_k(x)}{\partial x_l\partial x_r}\]</p><p>We take the derivative with respect to <span>$x_i$</span> on both sides of Eq. 2, and get</p><p class="math-container">\[\frac{\partial l}{\partial x_i} = \Phi_k F_{k,i} \tag{3}\]</p><p>We take the derivative with respect to <span>$x_j$</span> on both sides of Eq. 3, and get </p><p class="math-container">\[\frac{\partial^2 l}{\partial x_i\partial x_j} = \Phi_{kr} F_{k,i}F_{r,j} + \Phi_k F_{k, ij} \tag{4}\]</p><p>Here we have used the Einstein notation. Let <span>$\bar\Phi = \nabla \Phi$</span> but we treat <span>$x$</span> as a independent variable of <span>$\bar\Phi$</span>, then we can rewrite Eq. 4 to </p><p class="math-container">\[\nabla_x^2 l = (\nabla_x F) \nabla^2_x\Phi (\nabla_x F)^T + \nabla_x^2 (\bar\Phi^T F)\tag{5}\]</p><p>Note the values of <span>$\bar\Phi$</span> is already available in the gradient back-propagation. </p><h2 id="Algorithm"><a class="docs-heading-anchor" href="#Algorithm">Algorithm</a><a id="Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithm" title="Permalink"></a></h2><p>Based on Eq. 5, we have the following algorithm for calculating the Hessian</p><ol><li>Initialize <span>$H = 0$</span></li><li>for <span>$k = n-1, n-2,\ldots, 1$</span><ul><li>Calculate <span>$J = \nabla F_k$</span> and extract <span>$\bar \Phi_{k+1}$</span> from the gradient back-propagation tape.</li><li>Calculate <span>$Z = \nabla^2 (\bar\Phi_{k+1}^T F_k)$</span></li><li><p class="math-container">\[H \gets JHJ^T + Z\]</p></li></ul></li></ol><p>This algorithm only requires one backward pass and constructs the Hessian iteratively. Additionally, we can leverage the symmetry of the Hessian when we do the calculations in the second step. This algorithm also doesn&#39;t require looping over each components of the gradient. </p><p>However, the challenge here is that we need to calculate <span>$\nabla F_k$</span> and <span>$Z = \nabla^2 (\bar\Phi_{k+1}^T F_k)$</span>. Developing a complete support of such calculations for all operators  can be a time-consuming task. But due to the benefit brought by the trust region method, we deem it to be a rewarding investment. </p><h2 id="Example:-Developing-Second-Order-PCL-for-a-Sparse-Linear-Solver"><a class="docs-heading-anchor" href="#Example:-Developing-Second-Order-PCL-for-a-Sparse-Linear-Solver">Example: Developing Second Order PCL for a Sparse Linear Solver</a><a id="Example:-Developing-Second-Order-PCL-for-a-Sparse-Linear-Solver-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Developing-Second-Order-PCL-for-a-Sparse-Linear-Solver" title="Permalink"></a></h2><p>Here we consider an application of second order PCL for a sparse solver. We focus on the operator that takes the sparse entries of a matrix <span>$A\in\mathbb{R}^{n\times n}$</span> as input, and outputs <span>$u$</span></p><p class="math-container">\[Au = f\]</p><p>Let <span>$A = [a_{ij}]$</span>, and some of <span>$a_{ij}$</span> are zero. According to 2nd order PCL, we need to calculate <span>$\frac{\partial u_k}{\partial a_{ij}}$</span> and <span>$\frac{\partial^2 (y^T u)}{\partial a_{ij} \partial a_{rs}}$</span>. </p><p>We consider a multi-index <span>$l$</span> and <span>$r$</span>. We take the gradient with respect to <span>$a_l$</span> on both sides of </p><p class="math-container">\[a_{i1}u_1 + a_{i2}u_2 + \ldots + a_{in}u_n = f_i\]</p><p>which leads to </p><p class="math-container">\[a_{i1}^lu_1 + a_{i2}^lu_2 + \ldots + a_{in}^lu_n + a_{i1}u^l_1 + a_{i2}u^l_2 + \ldots + a_{in}u^l_n = 0\tag{6}\]</p><p>Here the superscript indicates the derivative. </p><p>Eq. 6 leads to </p><p class="math-container">\[u^l = -A^{-1}A^l u \tag{7}\]</p><p>Note at most one entry in <span>$A^l$</span> is nonzero, and therefore at most one entry in <span>$A^l u$</span> is nonzero. Thus to calculate Eq. 7, we can calculate the inverse <span>$A^{-1}$</span> first, and then <span>$u^l$</span> can be obtained cheaply by taking a column from <span>$A^{-1}$</span>. The complexity will be <span>$\mathcal{O}(n^3)$</span>–-the cost of inverting <span>$A^{-1}$</span>.</p><p>Now take the derivative with respect to <span>$a_r$</span> on both sides of Eq. 6, we have</p><p class="math-container">\[\begin{aligned}
a_{i1}^lu_1^r + a_{i2}^lu_2^r + \ldots + a_{in}^lu_n^r + \\ a_{i1}^ru^l_1 + a_{i2}^ru^l_2 + \ldots + a_{in}^ru^l_n +\\ a_{i1}u^{rl}_1 + a_{i2}u^{rl}_2 + \ldots + a_{in}u^{rl}_n = 0
\end{aligned}\]</p><p>which leads to </p><p class="math-container">\[Au^{rl} = -A^l u^r - A^r u^l\]</p><p>Therefore, </p><p class="math-container">\[(y^Tu)^{rl} = - y^TA^{-1}(A^l u^r + A^r u^l)\]</p><p>We can calculate <span>$z^T = y^TA^{-1}$</span> first with a cost <span>$\mathcal{O}(n^2)$</span>. Because <span>$u^r$</span>, <span>$u^l$</span> has already been calculated and <span>$A^l$</span>, <span>$A^r$</span> has at most one nonzero entry, <span>$A^l u^r + A^r u^l$</span> has at most two nonzero entries. The calculation <span>$z^T(A^l u^r + A^r u^l)$</span> can be done in <span>$\mathcal{O}(1)$</span> and therefore the total cost is <span>$\mathcal{O}(d^2)$</span>, where <span>$d$</span> is the number of nonzero entries. </p><p>Upon obtaining <span>$\frac{\partial u_k}{\partial a_{ij}}$</span> and <span>$\frac{\partial^2 (y^T u)}{\partial a_{ij} \partial a_{rs}}$</span>, we can apply the recursive the formula to &quot;back-propagate&quot; the Hessian matrix. </p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 16 March 2021 09:26">Tuesday 16 March 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
