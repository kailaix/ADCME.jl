<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Understand the Multi-threading Model · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li><a class="tocitem" href="../mpi/">Distributed Scientific Machine Learning using MPI</a></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li class="is-active"><a class="tocitem" href>Understand the Multi-threading Model</a><ul class="internal"><li><a class="tocitem" href="#Processes,-Threads,-and-Cores"><span>Processes, Threads, and Cores</span></a></li><li><a class="tocitem" href="#Inter-and-Intra-Parallelism"><span>Inter and Intra Parallelism</span></a></li><li><a class="tocitem" href="#ThreadPools"><span>ThreadPools</span></a></li><li><a class="tocitem" href="#How-to-Use-the-Intra-Thread-Pool"><span>How to Use the Intra Thread Pool</span></a></li><li><a class="tocitem" href="#Runtime-Optimizations"><span>Runtime Optimizations</span></a></li></ul></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Understand the Multi-threading Model</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Understand the Multi-threading Model</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/multithreading.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Understand-the-Multi-threading-Model"><a class="docs-heading-anchor" href="#Understand-the-Multi-threading-Model">Understand the Multi-threading Model</a><a id="Understand-the-Multi-threading-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Understand-the-Multi-threading-Model" title="Permalink"></a></h1><p>Multi-threading is a very important technique for accelerating simulations in ADCME. Through this section, we look into the multi-threading models of ADCME&#39;s backend, TensorFlow. Let us start with some basic concepts related to CPUs.</p><h2 id="Processes,-Threads,-and-Cores"><a class="docs-heading-anchor" href="#Processes,-Threads,-and-Cores">Processes, Threads, and Cores</a><a id="Processes,-Threads,-and-Cores-1"></a><a class="docs-heading-anchor-permalink" href="#Processes,-Threads,-and-Cores" title="Permalink"></a></h2><p>We often hear about processes and threads when talking about multi-threading. In a word, a process is a program in execution. A process may invoke multiple threads. A thread can be viewed as a scheduler for executing each line of codes in order. It tells the CPU to perform an instruction. </p><p>The biggest difference is that different processes do not share memory with each other. But different threads <strong>within the same process</strong> has the same memory space. </p><p>Up till now, processes and threads are <strong>logical</strong> concepts, which are not bound to the physical devices. Cores are physical concepts. Nowadays, each CPU contains multiple cores. Cores contain workers that actually perform computation, and these workers are called ALUs (arithmetic logic units). To use these ALUs, we need some schedulers that tell the CPU/cores to perform certain tasks. This is done by <strong>hardware threads</strong>. Typically when we want to do some computation, we need to load data first and then perform calculations on ALUs. The data loading process is also taken care by the hardware threads. Therefore, it is possible that the ALUs are waiting for input data. One clever idea in computer science is to use pipelining: overlapping data loading of the current instruction and computation of the last instruction. This means we need more schedulers, i.e., hardware threads, to take care of data loading and computing simultaneously. Therefore, modern CPUs usually have multiple hardware threads for one core. For example, Intel CPUs have the so-called <strong>hyperthreading technology</strong>, i.e., each core has two physical threads. </p><p>Now we understand four concepts: processes, (logical) threads, cores, hardware threads. So what is the relationship between threads and hardware threads? Actually this is straight-forward: logical threads are mapped to hardware threads. For example, if there are 4 logical threads and 4 hardware threads, the OS may map each logical thread to one distinct hardware thread. If there are more than 4 logical threads, some logical threads may be mapped to one. That says, these logical threads will not enjoy truly parallelism.</p><p>Now let&#39;s consider how ADCME works: for one CPU, ADCME always runs only one process. But to gain maximum efficiency, ADCME will create multiple threads to leverage any parallelism we have in hardware resources and computational models. </p><h2 id="Inter-and-Intra-Parallelism"><a class="docs-heading-anchor" href="#Inter-and-Intra-Parallelism">Inter and Intra Parallelism</a><a id="Inter-and-Intra-Parallelism-1"></a><a class="docs-heading-anchor-permalink" href="#Inter-and-Intra-Parallelism" title="Permalink"></a></h2><p>There are two types of parallelism in ADCME execution: <strong>inter</strong> and <strong>intra</strong>. </p><p>Consider a computational graph, there may be multiple independent operators and therefore we can execute them in parallel. This type of parallelism is called inter-parallelism. For example, </p><pre><code class="language-julia">using ADCME 

a1 = constant(rand(10))
a2 = constant(rand(10))
a3 = a1 * a2 
a4 = a1 + a2 
a5 = a3 + a4</code></pre><p>In the above code, <code>a3 = a1 * a2</code> and <code>a4 = a1 + a2</code> are independent and can be executed in parallel. </p><p>Another type of parallelism is intra-parallel, that is, the computation within each operator can be computed in parallel. For example, in the example above, we can compute the first 5 entries and last 5 entries in <code>a4 = a1 + a2</code> in parallel. </p><p>These types of parallelism can be achieved using multi-threading. In the next section, we explain how this is implemented in TensorFlow.</p><h2 id="ThreadPools"><a class="docs-heading-anchor" href="#ThreadPools">ThreadPools</a><a id="ThreadPools-1"></a><a class="docs-heading-anchor-permalink" href="#ThreadPools" title="Permalink"></a></h2><p>The backend of ADCME, TensorFlow, uses two threadpools for multithreading. One thread pool is for inter-parallelism, and the other is for intra-parallelism. They can be set by the users.</p><p>One common mistake is that users think in terms of hardware threads instead of logical threads. For example, if we have 8 hardware threads in total, one might want to allocate 4 threads for inter-parallelism and 4 threads for intra-parallelism. That&#39;s not true. When we talk about allocating threads for intra and inter thread pools, we are always talking about logical threads. So we can have a threadpool containing 100 threads for both inter and intra thread pools even if we only have 8 hardware threads in total. And in the runtime, inter and intra thread pools may use the same hardware threads for scheduling tasks. </p><p>The following figure is an illustration of the two thread pools of ADCME. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/threadpool.png?raw=true" alt/></p><h2 id="How-to-Use-the-Intra-Thread-Pool"><a class="docs-heading-anchor" href="#How-to-Use-the-Intra-Thread-Pool">How to Use the Intra Thread Pool</a><a id="How-to-Use-the-Intra-Thread-Pool-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-Use-the-Intra-Thread-Pool" title="Permalink"></a></h2><p>In practice, when we implement custom operators, we may want to use the intra thread pool. <a href="https://github.com/kailaix/ADCME.jl/tree/master/docs/src/assets/Codes/ThreadPools">Here</a> gives an example how to use thread pools. </p><pre><code class="language-c">#include &lt;thread&gt;
#include &lt;chrono&gt;
#include &lt;condition_variable&gt;
#include &lt;atomic&gt;

void print_thread(std::atomic_int &amp;cnt, std::condition_variable &amp;cv){
  std::this_thread::sleep_for(std::chrono::milliseconds(1000));
  printf(&quot;My thread ID is %d\n&quot;, std::this_thread::get_id());
  cnt++;
  if (cnt==7) cv.notify_one();
}

void threadpool_print(OpKernelContext* context){
  thread::ThreadPool * const tp = context-&gt;device()-&gt;tensorflow_cpu_worker_threads()-&gt;workers;
  std::atomic_int cnt = 0;
  std::condition_variable cv;
  std::mutex mu;

  printf(&quot;Number of intra-parallel thread = %d\n&quot;, tp-&gt;NumThreads());
  printf(&quot;Maximum Parallelism = %d\n&quot;, port::MaxParallelism());

  for (int i = 0; i &lt; 7; i++)
    tp-&gt;Schedule([&amp;cnt, &amp;cv](){print_thread(cnt, cv);});
  
  {
    std::unique_lock&lt;std::mutex&gt; lck(mu);
    cv.wait(lck, [&amp;cnt](){return cnt==7;});
  }
  printf(&quot;Op finished\n&quot;);
}</code></pre><p>Basically, we can asynchronously launch jobs using the thread pools. Additionally, we are responsible for synchronization. Here we have used condition variables for synchronization. </p><p>Typically our CPU operators are synchronous and do not need the thread pools. But it does not hard to have an intra thread pool. </p><h2 id="Runtime-Optimizations"><a class="docs-heading-anchor" href="#Runtime-Optimizations">Runtime Optimizations</a><a id="Runtime-Optimizations-1"></a><a class="docs-heading-anchor-permalink" href="#Runtime-Optimizations" title="Permalink"></a></h2><p>If you are using Intel CPUs, we may have some runtime optimization configurations. See this <a href="https://software.intel.com/content/www/us/en/develop/articles/guide-to-tensorflow-runtime-optimizations-for-cpu.html">link</a> for details. Here, we show the effects of some optimizations. </p><p>We already understand <code>intra_op_parallelism_threads</code> and <code>inter_op_parallelism_threads</code>; now let us consider some other options. We consider computing <span>$\sin$</span> function using the following formula </p><p class="math-container">\[\sin x \approx x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!}\]</p><p>The implementation can be found <a href=".">here</a>.</p><h3 id="Configure-OpenMP"><a class="docs-heading-anchor" href="#Configure-OpenMP">Configure OpenMP</a><a id="Configure-OpenMP-1"></a><a class="docs-heading-anchor-permalink" href="#Configure-OpenMP" title="Permalink"></a></h3><p>To set the number of OMP threads, we can configure the <code>OMP_NUM_THREADS</code> environment variable. One caveat is that the variable must be set before loading ADCME. For example </p><pre><code class="language-julia">ENV[&quot;OMP_NUM_THREADS&quot;] = 5
using ADCME</code></pre><p>Running the <code>omp_thread.jl</code>, we have the following output</p><pre><code class="language-none">There are 5 OpenMP threads
4 is computing...
0 is computing...
4 is computing...
1 is computing...
1 is computing...
0 is computing...
3 is computing...
3 is computing...
2 is computing...
2 is computing...</code></pre><p>We see that there are 5 threads running. </p><h3 id="Configure-Number-of-Devices"><a class="docs-heading-anchor" href="#Configure-Number-of-Devices">Configure Number of Devices</a><a id="Configure-Number-of-Devices-1"></a><a class="docs-heading-anchor-permalink" href="#Configure-Number-of-Devices" title="Permalink"></a></h3><p><a href="../api/#ADCME.Session-Tuple"><code>Session</code></a> accepts keywords <code>CPU</code>, which limits the number of CPUs we can use. Note, <code>CPU</code> corresponds to the number of CPU devices, not cores or threads. For example, if we run <code>num_device.jl</code> with (default is using all CPUs)</p><pre><code class="language-julia">sess = Session(CPU=1); init(sess)</code></pre><p>We will see </p><pre><code class="language-none">There are 144 OpenMP threads</code></pre><p>This is because we have 144 cores in our machine. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mpi_benchmark/">« MPI Benchmarks</a><a class="docs-footer-nextpage" href="../rbf/">Radial Basis Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 18 January 2021 23:53">Monday 18 January 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
