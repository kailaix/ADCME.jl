<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distributed Scientific Machine Learning using MPI · ADCME</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="ADCME logo"/></a><div class="docs-package-name"><span class="docs-autofit">ADCME</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Overview</a></li><li><span class="tocitem">Tutorial</span><ul><li><a class="tocitem" href="../tutorial/">Overview</a></li><li><a class="tocitem" href="../videos_and_slides/">Video Lectures and Slides</a></li><li><a class="tocitem" href="../tu_whatis/">What is ADCME? Computational Graph, Automatic Differentiation &amp; TensorFlow</a></li><li><a class="tocitem" href="../tu_basic/">ADCME Basics: Tensor, Type, Operator, Session &amp; Kernel</a></li><li><a class="tocitem" href="../tu_optimization/">PDE Constrained Optimization</a></li><li><a class="tocitem" href="../tu_sparse/">Sparse Linear Algebra</a></li><li><a class="tocitem" href="../tu_fd/">Numerical Scheme in ADCME: Finite Difference Example</a></li><li><a class="tocitem" href="../tu_fem/">Numerical Scheme in ADCME: Finite Element Example</a></li><li><a class="tocitem" href="../tu_inv/">Inverse Modeling with ADCME</a></li><li><a class="tocitem" href="../tu_recipe/">Inverse Modeling Recipe</a></li><li><a class="tocitem" href="../tu_nn/">Combining Neural Networks with Numerical Schemes</a></li><li><a class="tocitem" href="../tu_implicit/">Advanced: Automatic Differentiation for Implicit Operators</a></li><li><a class="tocitem" href="../tu_customop/">Advanced: Custom Operators</a></li><li><a class="tocitem" href="../tu_debug/">Advanced: Debugging and Profiling</a></li><li><a class="tocitem" href="../exercise/">Exercise: Inverse Modeling with ADCME</a></li></ul></li><li><span class="tocitem">Resources</span><ul><li><a class="tocitem" href="../newton_raphson/">Newton Raphson</a></li><li><a class="tocitem" href="../parallel/">Parallel Computing</a></li><li><a class="tocitem" href="../optimizers/">Optimizers</a></li><li><a class="tocitem" href="../optim/">Study on Optimizers</a></li><li><a class="tocitem" href="../ode/">PDE/ODE Solvers</a></li><li><a class="tocitem" href="../global/">Shared Memory Across Kernels</a></li><li><a class="tocitem" href="../julia_customop/">Julia Custom Operators</a></li><li><a class="tocitem" href="../nn/">Neural Networks</a></li><li><a class="tocitem" href="../ot/">Optimal Transport</a></li><li><a class="tocitem" href="../resource_manager/">Resource Manager</a></li><li><a class="tocitem" href="../alphascheme/">Generalized α Scheme</a></li><li><a class="tocitem" href="../factorization/">Direct Methods for Sparse Matrices</a></li><li><a class="tocitem" href="../customopt/">Custom Optimizer</a></li><li><a class="tocitem" href="../options/">Global Options</a></li><li><a class="tocitem" href="../mcmc/">Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a></li><li class="is-active"><a class="tocitem" href>Distributed Scientific Machine Learning using MPI</a><ul class="internal"><li><a class="tocitem" href="#MPI-Support-in-ADCME"><span>MPI Support in ADCME</span></a></li><li><a class="tocitem" href="#Limitations"><span>Limitations</span></a></li><li><a class="tocitem" href="#Implementing-Custom-Operators-using-MPI"><span>Implementing Custom Operators using MPI</span></a></li><li><a class="tocitem" href="#Hybrid-Programming"><span>Hybrid Programming</span></a></li><li><a class="tocitem" href="#Optimization"><span>Optimization</span></a></li><li><a class="tocitem" href="#Solving-the-Heat-Equation"><span>Solving the Heat Equation</span></a></li></ul></li><li><a class="tocitem" href="../mpi_benchmark/">MPI Benchmarks</a></li><li><a class="tocitem" href="../multithreading/">Understand the Multi-threading Model</a></li><li><a class="tocitem" href="../rbf/">Radial Basis Functions</a></li><li><a class="tocitem" href="../topopt/">Topological Optimization</a></li><li><a class="tocitem" href="../quadrature/">Numerical Integration</a></li><li><a class="tocitem" href="../sqlite3/">Introducing ADCME Database and SQL Integration: an Efficient Approach to Simulation Data Management</a></li><li><a class="tocitem" href="../hessian/">The Mathematical Structure of DNN Hessians</a></li></ul></li><li><span class="tocitem">Physics Informed Machine Learning</span><ul><li><a class="tocitem" href="../fdtd/">Finite-difference Time-domain for Electromagnetics and Seismic Inversion</a></li></ul></li><li><span class="tocitem">Deep Learning Schemes</span><ul><li><a class="tocitem" href="../vae/">Variational Autoencoder</a></li><li><a class="tocitem" href="../flow/">Normalizing Flows</a></li><li><a class="tocitem" href="../convnet/">Convolutional Neural Network</a></li><li><a class="tocitem" href="../bnn/">Bayesian Neural Networks</a></li><li><a class="tocitem" href="../reinforcement_learning/">Reinforcement Learning Basics: Q-learning and SARSA</a></li></ul></li><li><span class="tocitem">Developer Guide</span><ul><li><a class="tocitem" href="../designpattern/">Design Pattern</a></li><li><a class="tocitem" href="../toolchain/">Built-in Toolchain for Third-party Libraries</a></li><li><a class="tocitem" href="../installmpi/">Configure MPI for Distributed Computing</a></li><li><a class="tocitem" href="../windows_installation/">Install ADCME on Windows</a></li></ul></li><li><span class="tocitem">Applications</span><ul><li><a class="tocitem" href="../apps/">Overview</a></li><li><a class="tocitem" href="../apps_ana/">Adversarial Numerical Analysis</a></li><li><a class="tocitem" href="../apps_levy/">Calibrating Multivariate Lévy Processes with Neural Networks</a></li><li><a class="tocitem" href="../apps_constitutive_law/">Learning Constitutive Relations from Indirect Observations Using Deep Neural Networks</a></li><li><a class="tocitem" href="../apps_ad/">Intelligent Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_adseismic/">General Seismic Inversion using Automatic Differentiation</a></li><li><a class="tocitem" href="../apps_nnfem/">Symmetric Positive Definite Neural Networks (SPD-NN)</a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Resources</a></li><li class="is-active"><a href>Distributed Scientific Machine Learning using MPI</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distributed Scientific Machine Learning using MPI</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kailaix/ADCME.jl/blob/master/docs/src/mpi.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Distributed-Scientific-Machine-Learning-using-MPI"><a class="docs-heading-anchor" href="#Distributed-Scientific-Machine-Learning-using-MPI">Distributed Scientific Machine Learning using MPI</a><a id="Distributed-Scientific-Machine-Learning-using-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Scientific-Machine-Learning-using-MPI" title="Permalink"></a></h1><p>Many large-scale scientific computing involves parallel computing. Among many parallel computing models, the MPI  is one of the most popular models. In this section, we describe how ADCME can work with MPI for solving inverse modeling. Specifically, we describe how gradients can be back-propagated via MPI function calls.  </p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Message Passing Interface (MPI) is an interface for parallel computing based on message passing models. In the message passing model, a master process assigns work to workers by passing them a message that describes the work. The message may be data or meta information (e.g., operations to perform). A consensus was reached around 1992 and the MPI standard was born. MPI is a definition of interface, and the implementations are left to hardware venders. </p></div></div><h2 id="MPI-Support-in-ADCME"><a class="docs-heading-anchor" href="#MPI-Support-in-ADCME">MPI Support in ADCME</a><a id="MPI-Support-in-ADCME-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Support-in-ADCME" title="Permalink"></a></h2><p>The ADCME solution to distributed computing for scientific machine learning is to provide a set of &quot;data communication&quot; nodes in the computational graph. Each machine (MPI processor) runs an identical computational graph. The computational nodes are executed independently on each processor, and the data communication nodes need to synchronize among different processors. </p><p>These data communication nodes are implemented using MPI APIs. They are not necessarily blocking operations, but because ADCME respects the data dependency of computation, they act like blocking operations and the child operators are executed only when data communication is finished. For example, in the following example,</p><pre><code class="language-julia">b = mpi_op(a)
c = custom_op(b)</code></pre><p>even though <code>mpi_op</code> and <code>custom_op</code> can overlap, ADCME still sequentially execute these two operations. </p><p>This blocking behavior simplifies the synchronization logic as well as the implementation of gradient back-propagation while harming little performance. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi1.PNG?raw=true" alt/></p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi2.PNG?raw=true" alt/></p><p>ADCME provides a set of commonly used MPI operators. See <a href="https://kailaix.github.io/ADCME.jl/dev/api/#MPI">MPI Operators</a>. Basically, they are</p><ul><li><a href="../api/#ADCME.mpi_init-Tuple{}"><code>mpi_init</code></a>, <a href="../api/#ADCME.mpi_finalize-Tuple{}"><code>mpi_finalize</code></a>: Initialize and finalize MPI session. </li><li><a href="../api/#ADCME.mpi_rank-Tuple{}"><code>mpi_rank</code></a>, <a href="../api/#ADCME.mpi_size-Tuple{}"><code>mpi_size</code></a>: Get the MPI rank and size.</li><li><a href="../api/#ADCME.mpi_sum"><code>mpi_sum</code></a>, <a href="../api/#ADCME.mpi_bcast"><code>mpi_bcast</code></a>: Sum and broadcast tensors in different processors. </li><li><a href="../api/#ADCME.mpi_send"><code>mpi_send</code></a>, <a href="../api/#ADCME.mpi_recv"><code>mpi_recv</code></a>: Send and receive operators. </li></ul><p>The above two set of operators support automatic differentiation. They were implemented with MPI adjoint methods, which have existed in academia for decades. </p><p><a href="../installmpi/">This section</a> shows how to configure MPI for distributed computing in ADCME. </p><h2 id="Limitations"><a class="docs-heading-anchor" href="#Limitations">Limitations</a><a id="Limitations-1"></a><a class="docs-heading-anchor-permalink" href="#Limitations" title="Permalink"></a></h2><p>Despite that the provided <code>mpi_*</code> operations meet most needs,  some sophisticated data communication operations may not be easily expressed using these APIs. For example, when solving the Poisson&#39;s equation on a uniform grid, we may decompose the domain into many squares, and two adjacent squares exchange data in each iteration. A sequence of <code>mpi_send</code>, <code>mpi_recv</code> will likely cause deadlock. </p><p>Just like when it is difficult to use automatic differentiation to implement a forward computation and its gradient back-propagation, we resort to custom operators, it is the same case for MPI. We can design a specialized custom operator for data communication. To resolve the deadlock problem, we found the asynchronous sending, followed by asynchronous receiving, and then followed by waiting, a very general and convenient way to implement custom operators. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpiadjoint.png?raw=true" alt/></p><h2 id="Implementing-Custom-Operators-using-MPI"><a class="docs-heading-anchor" href="#Implementing-Custom-Operators-using-MPI">Implementing Custom Operators using MPI</a><a id="Implementing-Custom-Operators-using-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Implementing-Custom-Operators-using-MPI" title="Permalink"></a></h2><p>We can also make custom operators with MPI. Let us consider computing</p><p class="math-container">\[f(\theta) = \sum_{i=1}^n f_i(\theta)\]</p><p>Each <span>$f_i$</span> is a very expensive function so it makes sense to use MPI to split the jobs on different processors. To simplify the problem, we consider </p><p class="math-container">\[f(\theta) = f_1(\theta) + f_2(\theta) + f_3(\theta) + f_4(\theta)\]</p><p>where <span>$f_i(\theta) = \theta^{i-1}$</span>. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi.png?raw=true" alt/></p><p>Using the ADCME MPI API, we have the following code (<code>test_simple.jl</code>)</p><pre><code class="language-julia">using ADCME

mpi_init()
θ = placeholder(ones(1))
fθ = mpi_bcast(θ)
l = fθ^mpi_rank()
L = sum(mpi_sum(l))
g = gradients(L, θ)
sess = Session(); init(sess)
L_ = run(sess, L, θ=&gt;ones(1)*2.0)
g_ = run(sess, g, θ=&gt;ones(1)*2.0)

if mpi_rank()==0
    @info  L_, g_ 
end
mpi_finalize()</code></pre><p>We run the program with 4 processors</p><pre><code class="language-bash">mpirun -n 4 julia test_simple.jl</code></pre><p>We have the results:</p><pre><code class="language-bash">[ Info: (15.0, [17.0])</code></pre><h2 id="Hybrid-Programming"><a class="docs-heading-anchor" href="#Hybrid-Programming">Hybrid Programming</a><a id="Hybrid-Programming-1"></a><a class="docs-heading-anchor-permalink" href="#Hybrid-Programming" title="Permalink"></a></h2><p>Each MPI processor can communicate data between processes, which do not share memory. Within each process, ADCME also allows for multi-threaded parallelism with a shared-memory model. For example, we can use OpenMP to accelerate matrix vector production. We can also use a threadpool per process to manage more complex and dynamic parallel tasks. However, the hybrid model brings challenges to communicate data using MPI. When we post MPI calls from different threads within the same process, we need to prevent data races and match the corresponding broadcast and collective operators. For example, without any guarantee on the ordering of concurrent MPI calls, we might incorrectly matched a send operator with a gather operator. </p><p>In ADCME, we adopt the dependency injection technique: we explicitly serialize the MPI calls by adding ghost dependencies. For example, in the following computational graph, originally, Operator 2 and Operator 3 are independent. In a concurrent computing environment, Rank 0 may execute Operator 2 first and then Operator 3, while Rank 1 executes Operator 3 first and then Operator 2. Then there is a mismatch of the MPI call (race condition): Operator 2 in Rank 0 coacts with Operator 3 in Rank 1, and Operator 3 in Rank 0 coacts with Operator 2 in Rank 1. </p><p>To resolve the data race issue, we can explicitly make Operator 3 depend on Operator 2. In this way, we can ensure that <strong>the MPI calls</strong> Operator 1, 2, and 3 are executed in order. Note this technique sacrifices some concurrency (Operator 2 and Operator 3 cannot be executed concurrently), but the concurrency of most non-MPI operators is still preserved. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpi/dependency_injection.PNG?raw=true" alt/></p><h2 id="Optimization"><a class="docs-heading-anchor" href="#Optimization">Optimization</a><a id="Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization" title="Permalink"></a></h2><p>For solving inverse problems using distributed computing, an MPI-capable optimizer is required. The ADCME solution to distributed optimization is that the master machine holds, distributes and updates the optimizable variables. The gradients are calculated in the same device where the corresponding forward computation is done. Therefore, for a given serial optimizer, we can refactor it to a distributed one by letting worker nodes wait for instructions from the master node to compute either the objective function or the gradient.</p><p>This idea is implemented in the <a href="https://github.com/kailaix/ADOPT.jl"><code>ADOPT.jl</code></a> package, a customized version of <a href="https://github.com/JuliaNLSolvers/Optim.jl"><code>Optim.jl</code></a>. </p><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/mpiopt.png?raw=true" alt/></p><p>In the following, we try to solve </p><p class="math-container">\[1+\theta +\theta^2+\theta^3 = 2\]</p><p>using MPI-enabled LBFGS optimizer. </p><pre><code class="language-julia">using ADCME
using ADOPT
mpi_init()
θ = placeholder(ones(1))
fθ = mpi_bcast(θ)
l = fθ^mpi_rank()
L = (sum(mpi_sum(l)) - 2.0)^2
sess = Session(); init(sess)

f = x-&gt;run(sess, L, θ=&gt;x)
g! = (G, x)-&gt;(G[:] = run(sess, g, θ=&gt;x))

options = Options()
if mpi_rank()==0
    options.show_trace = true 
end
mpi_optimize(f, g!, ones(1), ADOPT.LBFGS(), options)
if mpi_rank()==0
    @info  result.minimizer, result.minimum
end

mpi_finalize()</code></pre><p>The expected output is </p><pre><code class="language-none">Iter     Function value   Gradient norm
     0     4.000000e+00     2.400000e+01
 * time: 0.00012421607971191406
     1     6.660012e-01     7.040518e+00
 * time: 1.128843069076538
     2     7.050686e-02     1.322515e+00
 * time: 1.210536003112793
     3     2.254820e-03     2.744374e-01
 * time: 1.2910940647125244
     4     4.319834e-07     3.908046e-03
 * time: 1.3442070484161377
     5     2.894433e-16     1.011994e-07
 * time: 1.3975300788879395
     6     0.000000e+00     0.000000e+00
 * time: 1.4507441520690918
[ Info: ([0.5436890126920764], 0.0)</code></pre><h3 id="Reduce-Sum"><a class="docs-heading-anchor" href="#Reduce-Sum">Reduce Sum</a><a id="Reduce-Sum-1"></a><a class="docs-heading-anchor-permalink" href="#Reduce-Sum" title="Permalink"></a></h3><pre><code class="language-julia">using ADCME

mpi_init()
r = mpi_rank()
a = constant(Float64.(Array(1:10) * r))
b = mpi_sum(a)

L = sum(b)
g = gradients(L, a)
sess = Session(); init(sess)
v, G = run(sess, [b,g])
mpi_finalize()</code></pre><h3 id="Broadcast"><a class="docs-heading-anchor" href="#Broadcast">Broadcast</a><a id="Broadcast-1"></a><a class="docs-heading-anchor-permalink" href="#Broadcast" title="Permalink"></a></h3><pre><code class="language-julia">using ADCME

mpi_init()
r = mpi_rank()
a = constant(ones(10) * r)
b = mpi_bcast(a, 3)
L = sum(b^2)
L = mpi_sum(L)
g = gradients(L, a)

sess = Session(); init(sess)
v, G = run(sess, [b, G])
mpi_finalize()</code></pre><h3 id="Send-and-Receive"><a class="docs-heading-anchor" href="#Send-and-Receive">Send and Receive</a><a id="Send-and-Receive-1"></a><a class="docs-heading-anchor-permalink" href="#Send-and-Receive" title="Permalink"></a></h3><pre><code class="language-julia"># mpiexec.exe -n 4 julia .\mpisum.jl
using ADCME

mpi_init()
r = mpi_rank()
a = constant(ones(10) * r)
a = mpi_sendrecv(a, 0, 2)

L = sum(a^2)
g = gradients(L, a)

sess = Session(); init(sess)
v, G = run(sess, [a,g])
mpi_finalize()</code></pre><p><a href="../api/#ADCME.mpi_sendrecv"><code>mpi_sendrecv</code></a> is a lightweight wrapper for <a href="../api/#ADCME.mpi_send"><code>mpi_send</code></a> followed by <a href="../api/#ADCME.mpi_recv"><code>mpi_recv</code></a>. Equivalently, we have</p><pre><code class="language-julia">if r==2
    global a
    a = mpi_send(a, 0)
end
if r==0
    global a
    a = mpi_recv(a,2)
end</code></pre><h2 id="Solving-the-Heat-Equation"><a class="docs-heading-anchor" href="#Solving-the-Heat-Equation">Solving the Heat Equation</a><a id="Solving-the-Heat-Equation-1"></a><a class="docs-heading-anchor-permalink" href="#Solving-the-Heat-Equation" title="Permalink"></a></h2><p>In this section, we consider solving the Poisson equation </p><p class="math-container">\[\frac{\partial u(x,y)}{\partial t} =\kappa(x,y) \Delta u(x,y) \quad (x,y) \in [0,1]^2\]</p><p>We discretize the above PDE with an explicit finite difference scheme</p><p class="math-container">\[\frac{u_{ij}^{n+1} - u^n_{ij}}{\Delta t} = \kappa_{ij} \frac{u_{i+1,j}^n + u_{i,j+1}^n + u_{i,j-1}^n + u_{i-1,j}^n - 4u_{ij}^n}{h^2} \tag{1}\]</p><p>To mitigate the computational and memory requirement, we use MPI APIs to implement a domain decomposition solver for the heat equation. The mesh is divided into <span>$N\times M$</span> rectangle patches. We implemented two operation:</p><ol><li><p><code>heat_op</code>, which updates <span>$u_{ij}^{n+1}$</span> using Equation 1 for a specific patch, with state variables <span>$u_{ij}^n$</span> in the current rectangle patch and on the boundary (from adjacent patches). </p></li><li><p><code>data_exchange</code>, which is a data communication operator that sends the boundary data to adjacent patches and receives boundary data from other patches. </p></li></ol><p><img src="https://github.com/ADCMEMarket/ADCMEImages/blob/master/ADCME/dataexchange.png?raw=true" alt/></p><p>Then the time marching scheme can be implemented with the following code:</p><pre><code class="language-julia">function heat_update_u(u, kv, f)
    r = mpi_rank()
    I = div(r, M)
    J = r%M

    up_ = constant(zeros(m))
    down_ = constant(zeros(m))
    left_ = constant(zeros(n))
    right_ = constant(zeros(n))


    up = constant(zeros(m))
    down = constant(zeros(m))
    left = constant(zeros(n))
    right = constant(zeros(n))

    (I&gt;0) &amp;&amp; (up = u[1,:])
    (I&lt;N-1) &amp;&amp; (down = u[end,:])
    (J&gt;0) &amp;&amp; (left = u[:,1])
    (J&lt;M-1) &amp;&amp; (right = u[:,end])

    left_, right_, up_, down_ = data_exchange(left, right, up, down)

    u = heat(u, kv, up_, down_, left_, right_, f, h, Δt)
end</code></pre><p>An MPI-capable heat equation time integrator can be implemented with </p><pre><code class="language-julia">function heat_solver(u0, kv, f, NT=10)
    f = constant(f)
    function condition(i, u_arr)
        i&lt;=NT
    end
    function body(i, u_arr)
        u = read(u_arr, i)
        u_new = heat_update_u(u, kv, f[i])
        # op = tf.print(r, i)
        # u_new = bind(u_new, op)
        i+1, write(u_arr, i+1, u_new)
    end
    i = constant(1, dtype =Int32)
    u_arr = TensorArray(NT+1)
    u_arr = write(u_arr, 1, u0)
    _, u = while_loop(condition, body, [i, u_arr])
    reshape(stack(u), (NT+1, n, m))
end</code></pre><p>For example, we can implement the heat solver with diffusivity coefficient <span>$K_0$</span> and initial condition <span>$u_0$</span> with the following code:</p><pre><code class="language-julia">K = placeholder(K0)
a_ = mpi_bcast(K)
sol = heat_solver(u0, K_, F, NT)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../mcmc/">« Uncertainty Quantification of Neural Networks in Physics Informed Learning using MCMC</a><a class="docs-footer-nextpage" href="../mpi_benchmark/">MPI Benchmarks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 17 March 2021 05:58">Wednesday 17 March 2021</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
